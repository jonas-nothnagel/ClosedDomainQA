{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Experiment for QA Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'feedparser'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-ef8b272a12d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mfeedparser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'..'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'src'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'feedparser'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath(os.path.join('..', 'src')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import haystack and FARM utils\n",
    "\n",
    "from haystack.file_converter.txt import TextConverter\n",
    "from haystack.file_converter.pdf import PDFToTextConverter\n",
    "from haystack.file_converter.docx import DocxToTextConverter\n",
    "\n",
    "from haystack.preprocessor.utils import convert_files_to_dicts, fetch_archive_from_http\n",
    "from haystack.preprocessor.preprocessor import PreProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize ElasticSearch document storage\n",
    "#from haystack.document_store.elasticsearch import ElasticsearchDocumentStore\n",
    "#document_store = ElasticsearchDocumentStore(host=\"localhost\", username=\"\", password=\"\", index=\"document\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data\n",
    "df = pd.read_csv('../data/raw/arxiv_results.csv')\n",
    "\n",
    "raw_data = pickle.load(open(\"../data/raw/feedparser_results.pickle\", 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df[['title', 'summary']].rename(columns={'title':'name','summary':'text'}).to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/jonas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "03/30/2021 20:03:48 - WARNING - haystack.preprocessor.preprocessor -   A sentence found with word count higher than the split length.\n",
      "03/30/2021 20:03:49 - WARNING - haystack.preprocessor.preprocessor -   A sentence found with word count higher than the split length.\n",
      "03/30/2021 20:03:49 - WARNING - haystack.preprocessor.preprocessor -   A sentence found with word count higher than the split length.\n",
      "03/30/2021 20:03:49 - WARNING - haystack.preprocessor.preprocessor -   A sentence found with word count higher than the split length.\n",
      "03/30/2021 20:03:49 - WARNING - haystack.preprocessor.preprocessor -   A sentence found with word count higher than the split length.\n",
      "03/30/2021 20:03:49 - WARNING - haystack.preprocessor.preprocessor -   A sentence found with word count higher than the split length.\n",
      "03/30/2021 20:03:49 - WARNING - haystack.preprocessor.preprocessor -   A sentence found with word count higher than the split length.\n",
      "03/30/2021 20:03:49 - WARNING - haystack.preprocessor.preprocessor -   A sentence found with word count higher than the split length.\n",
      "03/30/2021 20:03:49 - WARNING - haystack.preprocessor.preprocessor -   A sentence found with word count higher than the split length.\n",
      "03/30/2021 20:03:49 - WARNING - haystack.preprocessor.preprocessor -   A sentence found with word count higher than the split length.\n",
      "03/30/2021 20:03:50 - WARNING - haystack.preprocessor.preprocessor -   A sentence found with word count higher than the split length.\n",
      "03/30/2021 20:03:50 - WARNING - haystack.preprocessor.preprocessor -   A sentence found with word count higher than the split length.\n",
      "03/30/2021 20:03:51 - WARNING - haystack.preprocessor.preprocessor -   A sentence found with word count higher than the split length.\n",
      "03/30/2021 20:03:51 - WARNING - haystack.preprocessor.preprocessor -   A sentence found with word count higher than the split length.\n",
      "03/30/2021 20:03:51 - WARNING - haystack.preprocessor.preprocessor -   A sentence found with word count higher than the split length.\n",
      "03/30/2021 20:03:51 - WARNING - haystack.preprocessor.preprocessor -   A sentence found with word count higher than the split length.\n",
      "03/30/2021 20:03:51 - WARNING - haystack.preprocessor.preprocessor -   A sentence found with word count higher than the split length.\n",
      "03/30/2021 20:03:52 - WARNING - haystack.preprocessor.preprocessor -   A sentence found with word count higher than the split length.\n",
      "03/30/2021 20:03:53 - WARNING - haystack.preprocessor.preprocessor -   A sentence found with word count higher than the split length.\n",
      "03/30/2021 20:03:53 - WARNING - haystack.preprocessor.preprocessor -   A sentence found with word count higher than the split length.\n",
      "03/30/2021 20:03:53 - WARNING - haystack.preprocessor.preprocessor -   A sentence found with word count higher than the split length.\n",
      "03/30/2021 20:03:53 - WARNING - haystack.preprocessor.preprocessor -   A sentence found with word count higher than the split length.\n"
     ]
    }
   ],
   "source": [
    "preprocessor = PreProcessor(\n",
    "    clean_empty_lines=True,\n",
    "    clean_whitespace=True,\n",
    "    clean_header_footer=False,\n",
    "    split_by=\"word\",\n",
    "    split_length=100,\n",
    "    split_respect_sentence_boundary=True\n",
    ")\n",
    "nested_docs = [preprocessor.process(d) for d in test]\n",
    "docs = [d for x in nested_docs for d in x]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'Taking Ethics, Fairness, and Bias Seriously in Machine Learning for\\n  Disaster Risk Management',\n",
       "  'text': 'This paper highlights an important, if under-examined, set of questions about\\nthe deployment of machine learning technologies in the field of disaster risk\\nmanagement (DRM). While emerging tools show promising capacity to support\\nscientific efforts to better understand and mitigate the threats posed by\\ndisasters and climate change, our field must undertake a much more careful\\nassessment of the potential negative impacts that machine learning technologies\\nmay create. We also argue that attention to these issues in the context of\\nmachine learning affords the opportunity to have discussions about potential\\nethics, bias, and fairness concerns within disaster data more broadly.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Taking Ethics, Fairness, and Bias Seriously in Machine Learning for\\n  Disaster Risk Management',\n",
       "  'text': 'In what\\nfollows, we first describe some of the uses and potential benefits of\\nmachine-learning technology in disaster risk management. We then draw on\\nresearch from other fields to speculate about potential negative impacts. Finally, we outline a research agenda for how our disaster risk management can\\nbegin to take these issues seriously and ensure that deployments of\\nmachine-learning tools are conducted in a responsible and beneficial manner.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'A Survey on the Role of Wireless Sensor Networks and IoT in Disaster\\n  Management',\n",
       "  'text': 'Extreme events and disasters resulting from climate change or other\\necological factors are difficult to predict and manage. Current limitations of\\nstate-of-the-art approaches to disaster prediction and management could be\\naddressed by adopting new unorthodox risk assessment and management strategies. The next generation Internet of Things (IoT), Wireless Sensor Networks (WSNs),\\n5G wireless communication, and big data analytics technologies are the key\\nenablers for future effective disaster management infrastructures. In this\\nchapter, we commissioned a survey on emerging wireless communication\\ntechnologies with potential for enhancing disaster prediction, monitoring, and\\nmanagement systems.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'A Survey on the Role of Wireless Sensor Networks and IoT in Disaster\\n  Management',\n",
       "  'text': 'Challenges, opportunities, and future research trends are\\nhighlighted to provide some insight on the potential future work for\\nresearchers in this field.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'A Natural Disasters Index',\n",
       "  'text': 'Natural disasters, such as tornadoes, floods, and wildfire pose risks to life\\nand property, requiring the intervention of insurance corporations. One of the\\nmost visible consequences of changing climate is an increase in the intensity\\nand frequency of extreme weather events. The relative strengths of these\\ndisasters are far beyond the habitual seasonal maxima, often resulting in\\nsubsequent increases in property losses. Thus, insurance policies should be\\nmodified to endure increasingly volatile catastrophic weather events. We\\npropose a Natural Disasters Index (NDI) for the property losses caused by\\nnatural disasters in the United States based on the \"Storm Data\" published by\\nthe National Oceanic and Atmospheric Administration.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'A Natural Disasters Index',\n",
       "  'text': 'The proposed NDI is an\\nattempt to construct a financial instrument for hedging the intrinsic risk. The\\nNDI is intended to forecast the degree of future risk that could forewarn the\\ninsurers and corporations allowing them to transfer insurance risk to capital\\nmarket investors. This index could also be modified to other regions and\\ncountries.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Sistem Pengambilan Keputusan Penanganan Bencana Alam Gempa Bumi Di\\n  Indonesia',\n",
       "  'text': \"After Aceh's quake many earthquakes have struck Indonesia alternately and\\neven other disasters have been a threat for every citizen in this country. Actually an everyday occurrence on earth and more than 3 million earthquakes\\noccur every year, about 8,000 a day, or one every 11 seconds in Indonesia there\\nare 5 to 30 quakes prediction everyday. Government's responsibility to protect\\nthe citizen has been done by making National body of disaster management. Preparing, saving and distribution logistic become National body of disaster\\nmanagement's responsibility to build information management. Many law's\\nproducts have been produced as a government's responsibility to give secure\\nlife for the citizen.\",\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Sistem Pengambilan Keputusan Penanganan Bencana Alam Gempa Bumi Di\\n  Indonesia',\n",
       "  'text': 'We can not prevent them totally, we have to learn to live\\nwith them and need to be prepared all the time, need to learn how to mitigate\\nrisk of losses in such events by managing crisis and emergencies correctly. After disaster happens respond must be rapidly and at an optimal level to save\\nlives and help to victims. DSS is information technology environment which can\\nbe used to help human in order to learn from past earthquake, record it, learn\\nand plan for future mitigation and hope will reduce the disaster risk in the\\nfuture.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Sistem Pengambilan Keputusan Penanganan Bencana Alam Gempa Bumi Di\\n  Indonesia',\n",
       "  'text': 'Using web technology for DSS will give value added where not only make\\na strategic decision for the decision maker, but for others who need national\\nearthquake information like citizen, scholars, researches and people around the\\nworld.',\n",
       "  'meta': {'_split_id': 2}},\n",
       " {'name': 'CNN-Based Semantic Change Detection in Satellite Imagery',\n",
       "  'text': 'Timely disaster risk management requires accurate road maps and prompt damage\\nassessment. Currently, this is done by volunteers manually marking satellite\\nimagery of affected areas but this process is slow and often error-prone. Segmentation algorithms can be applied to satellite images to detect road\\nnetworks. However, existing methods are unsuitable for disaster-struck areas as\\nthey make assumptions about the road network topology which may no longer be\\nvalid in these scenarios. Herein, we propose a CNN-based framework for\\nidentifying accessible roads in post-disaster imagery by detecting changes from\\npre-disaster imagery. Graph theory is combined with the CNN output for\\ndetecting semantic changes in road networks with OpenStreetMap data.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'CNN-Based Semantic Change Detection in Satellite Imagery',\n",
       "  'text': 'Our\\nresults are validated with data of a tsunami-affected region in Palu, Indonesia\\nacquired from DigitalGlobe.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Social Media Data Analysis and Feedback for Advanced Disaster Risk\\n  Management',\n",
       "  'text': 'Social media are more than just a one-way communication channel. Data can be\\ncollected, analyzed and contextualized to support disaster risk management. However, disaster management agencies typically use such added-value\\ninformation to support only their own decisions. A feedback loop between\\ncontextualized information and data suppliers would result in various\\nadvantages. First, it could facilitate the near real-time communication of\\nearly warnings derived from social media, linked to other sources of\\ninformation. Second, it could support the staff of aid organizations during\\nresponse operations. Based on the example of Hurricanes Harvey and Irma we show\\nhow filtered, geolocated Tweets can be used for rapid damage assessment.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Social Media Data Analysis and Feedback for Advanced Disaster Risk\\n  Management',\n",
       "  'text': 'We\\nclaim that the next generation of big data analyses will have to generate\\nactionable information resulting from the application of advanced analytical\\ntechniques. These applications could include the provision of social\\nmedia-based training data for algorithms designed to forecast actual cyclone\\nimpacts or new socio-economic validation metrics for seasonal climate\\nforecasts.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Deep Learning-based Aerial Image Segmentation with Open Data for\\n  Disaster Impact Assessment',\n",
       "  'text': 'Satellite images are an extremely valuable resource in the aftermath of\\nnatural disasters such as hurricanes and tsunamis where they can be used for\\nrisk assessment and disaster management. In order to provide timely and\\nactionable information for disaster response, in this paper a framework\\nutilising segmentation neural networks is proposed to identify impacted areas\\nand accessible roads in post-disaster scenarios. The effectiveness of\\npretraining with ImageNet on the task of aerial image segmentation has been\\nanalysed and performances of popular segmentation models compared. Experimental\\nresults show that pretraining on ImageNet usually improves the segmentation\\nperformance for a number of models.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Deep Learning-based Aerial Image Segmentation with Open Data for\\n  Disaster Impact Assessment',\n",
       "  'text': 'Open data available from OpenStreetMap\\n(OSM) is used for training, forgoing the need for time-consuming manual\\nannotation. The method also makes use of graph theory to update road network\\ndata available from OSM and to detect the changes caused by a natural disaster. Extensive experiments on data from the 2018 tsunami that struck Palu, Indonesia\\nshow the effectiveness of the proposed framework. ENetSeparable, with 30% fewer\\nparameters compared to ENet, achieved comparable segmentation results to that\\nof the state-of-the-art networks.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Smartphone sensing platform for emergency management',\n",
       "  'text': 'The increasingly sophisticated sensors supported by modern smartphones open\\nup novel research opportunities, such as mobile phone sensing. One of the most\\nchallenging of these research areas is context-aware and activity recognition. The SmartRescue project takes advantage of smartphone sensing, processing and\\ncommunication capabilities to monitor hazards and track people in a disaster. The goal is to help crisis managers and members of the public in early hazard\\ndetection, prediction, and in devising risk-minimizing evacuation plans when\\ndisaster strikes. In this paper we suggest a novel smartphone-based\\ncommunication framework. It uses specific machine learning techniques that\\nintelligently process sensor readings into useful information for the crisis\\nresponders.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Smartphone sensing platform for emergency management',\n",
       "  'text': 'Core to the framework is a content-based publish-subscribe\\nmechanism that allows flexible sharing of sensor data and computation results. We also evaluate a preliminary implementation of the platform, involving a\\nsmartphone app that reads and shares mobile phone sensor data for activity\\nrecognition.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Bayesian inference for CoVaR',\n",
       "  'text': \"Recent financial disasters emphasised the need to investigate the consequence\\nassociated with the tail co-movements among institutions; episodes of contagion\\nare frequently observed and increase the probability of large losses affecting\\nmarket participants' risk capital. Commonly used risk management tools fail to\\naccount for potential spillover effects among institutions because they provide\\nindividual risk assessment. We contribute to analyse the interdependence\\neffects of extreme events providing an estimation tool for evaluating the\\nConditional Value-at-Risk (CoVaR) defined as the Value-at-Risk of an\\ninstitution conditioned on another institution being under distress. In\\nparticular, our approach relies on Bayesian quantile regression framework.\",\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Bayesian inference for CoVaR',\n",
       "  'text': \"We\\npropose a Markov chain Monte Carlo algorithm exploiting the Asymmetric Laplace\\ndistribution and its representation as a location-scale mixture of Normals. Moreover, since risk measures are usually evaluated on time series data and\\nreturns typically change over time, we extend the CoVaR model to account for\\nthe dynamics of the tail behaviour. Application on U.S. companies belonging to\\ndifferent sectors of the Standard and Poor's Composite Index (S&P500) is\\nconsidered to evaluate the marginal contribution to the overall systemic risk\\nof each individual institution\",\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Risk Aversion to Parameter Uncertainty in Markov Decision Processes with\\n  an Application to Slow-Onset Disaster Relief',\n",
       "  'text': 'In classical Markov Decision Processes (MDPs), action costs and transition\\nprobabilities are assumed to be known, although an accurate estimation of these\\nparameters is often not possible in practice. This study addresses MDPs under\\ncost and transition probability uncertainty and aims to provide a mathematical\\nframework to obtain policies minimizing the risk of high long-term losses due\\nto not knowing the true system parameters. To this end, we utilize the risk\\nmeasure value-at-risk associated with the expected performance of an MDP model\\nwith respect to parameter uncertainty.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Risk Aversion to Parameter Uncertainty in Markov Decision Processes with\\n  an Application to Slow-Onset Disaster Relief',\n",
       "  'text': 'We provide mixed-integer linear and\\nnonlinear programming formulations and heuristic algorithms for such\\nrisk-averse models of MDPs under a finite distribution of the uncertain\\nparameters. Our proposed models and solution methods are illustrated on an\\ninventory management problem for humanitarian relief operations during a\\nslow-onset disaster. The results demonstrate the potential of our risk-averse\\nmodeling approach for reducing the risk of highly undesirable outcomes in\\nuncertain/risky environments.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Public-Private Partnership in the Management of Natural Disasters: A\\n  Review',\n",
       "  'text': 'Natural hazards can considerably impact the overall society of a country. As\\nsome degree of public sector involvement is always necessary to deal with the\\nconsequences of natural disasters, central governments have increasingly\\ninvested in proactive risk management planning. In order to empower and involve\\nthe whole society, some countries have established public-private partnerships,\\nmainly with the insurance industry, with satisfactorily outcomes. Although they\\nhave proven necessary and most often effective, the public-private initiatives\\nhave often incurred high debts or have failed to achieved the desired risk\\nreduction objectives. We review the role of these partnerships in the\\nmanagement of natural risks, with particular attention to the insurance sector.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Public-Private Partnership in the Management of Natural Disasters: A\\n  Review',\n",
       "  'text': \"Among other country-specific issues, poor risk knowledge and weak governance\\nhave widely challenged the initiatives during the recent years, while the\\nfuture is threatened by the uncertainty of climate change and unsustainable\\ndevelopment. In order to strengthen the country's resilience, a greater\\ninvolvement of all segments of the community, especially the weakest layers, is\\nneeded and the management of natural risks should be included in a sustainable\\ndevelopment plan.\",\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Modeling Weather-induced Home Insurance Risks with Support Vector\\n  Machine Regression',\n",
       "  'text': 'Insurance industry is one of the most vulnerable sectors to climate change. Assessment of future number of claims and incurred losses is critical for\\ndisaster preparedness and risk management. In this project, we study the effect\\nof precipitation on a joint dynamics of weather-induced home insurance claims\\nand losses. We discuss utility and limitations of such machine learning\\nprocedures as Support Vector Machines and Artificial Neural Networks, in\\nforecasting future claim dynamics and evaluating associated uncertainties. We\\nillustrate our approach by application to attribution analysis and forecasting\\nof weather-induced home insurance claims in a middle-sized city in the Canadian\\nPrairies.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': \"Non-profit Organizations' Need to Address Security for Effective\\n  Government Contacting\",\n",
       "  'text': 'The need for information security within small to mid-size companies is\\nincreasing. The risks of information security breach, data loss, and disaster\\nare growing. The impact of IT outages and issues on the company are\\nunacceptable to any size business and their clients. There are many ways to\\naddress the security for IT departments. The need to address risks of attacks\\nas well as disasters is important to the IT security policies and procedures.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': \"Non-profit Organizations' Need to Address Security for Effective\\n  Government Contacting\",\n",
       "  'text': 'The IT departments of small to medium companies have to address these security\\nconcerns within their budgets and other limited resources.Security planning,\\ndesign, and employee training that is needed requires input and agreement from\\nall levels of the company and management. This paper will discuss security\\nneeds and methods to implement them into a corporate infrastructure.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Disaster Resilience and Asset Prices',\n",
       "  'text': \"This paper investigates whether security markets price the effect of social\\ndistancing on firms' operations. We document that firms that are more resilient\\nto social distancing significantly outperformed those with lower resilience\\nduring the COVID-19 outbreak, even after controlling for the standard risk\\nfactors. Similar cross-sectional return differentials already emerged before\\nthe COVID-19 crisis: the 2014-19 cumulative return differential between more\\nand less resilient firms is of similar size as during the outbreak, suggesting\\ngrowing awareness of pandemic risk well in advance of its materialization.\",\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Disaster Resilience and Asset Prices',\n",
       "  'text': \"Finally, we use stock option prices to infer the market's return expectations\\nafter the onset of the pandemic: even at a two-year horizon, stocks of more\\npandemic-resilient firms are expected to yield significantly lower returns than\\nless resilient ones, reflecting their lower exposure to disaster risk. Hence,\\ngoing forward, markets appear to price exposure to a new risk factor, namely,\\npandemic risk.\",\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Social Media Information Sharing for Natural Disaster Response',\n",
       "  'text': 'Social media has become an essential channel for posting disaster-related\\ninformation, which provide governments and relief agencies real-time data for\\nbetter disaster management. However, research in this field has not received\\nsufficient attention and extracting useful information is still challenging. This paper aims to improve disaster relief efficiency via mining and analyzing\\nsocial media data like public attitudes towards disaster response and public\\ndemands for targeted relief supplies during different types of disasters. We\\nfocus on different natural disasters based on properties such as types,\\ndurations, and damages, which contains a total of 41,993 tweets.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Social Media Information Sharing for Natural Disaster Response',\n",
       "  'text': \"In this paper,\\npublic perception is assessed qualitatively by manually classified tweets,\\nwhich contain information like the demand for targeted relief supplies,\\nsatisfactions of disaster response, and public fear. Public attitudes to\\nnatural disasters are studied via a quantitative analysis using eight machine\\nlearning models. To better provide decision-makers with the appropriate model,\\nthe comparison of machine learning models based on computational time and\\nprediction accuracy is conducted. The change of public opinion during different\\nnatural disasters and the evolution of people's behavior of using social media\\nfor disaster relief in the face of the identical type of natural disasters as\\nTwitter continues to evolve are studied.\",\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Social Media Information Sharing for Natural Disaster Response',\n",
       "  'text': 'The results in this paper demonstrate\\nthe feasibility and validation of the proposed research approach and provide\\nrelief agencies with insights into better disaster management.',\n",
       "  'meta': {'_split_id': 2}},\n",
       " {'name': 'Doubly Optimal Secure Multicasting: Hierarchical Hybrid Communication\\n  Network : Disaster Relief',\n",
       "  'text': 'Recently, the world has witnessed the increasing occurrence of disasters,\\nsome of natural origin and others caused by man. The intensity of the\\nphenomenon that cause such disasters, the frequency in which they occur, the\\nnumber of people affected and the material damage caused by them have been\\ngrowing substantially. Disasters are defined as natural, technological, and\\nhuman-initiated events that disrupt the normal functioning of the economy and\\nsociety on a large scale. Areas where disasters have occurred bring many\\ndangers to rescue teams and the communication network infrastructure is usually\\ndestroyed. To manage these hazards, different wireless technologies can be\\nlaunched in the area of disaster.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Doubly Optimal Secure Multicasting: Hierarchical Hybrid Communication\\n  Network : Disaster Relief',\n",
       "  'text': 'This paper discusses the innovative wireless\\ntechnologies for Disaster Management. Specifically, issues related to the\\ndesign of Hierarchical Hybrid Communication Network (arising in the\\ncommunication network for disaster relief) are discussed.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Securing Virtualized Datacenters',\n",
       "  'text': 'Virtualization is a very popular solution to many problems in datacenter\\nmanagement. It offers increased utilization of existing system resources\\nthrough effective consolidation, negating the need for more servers and\\nadditional rack space. Furthermore, it offers essential capabilities in terms\\nof disaster recovery and potential savings on energy and maintenance costs. However, these benefits may be tempered by the increased complexities of\\nsecuring virtual infrastructure. Do the benefits of virtualization outweigh the\\nrisks? In this study, the authors evaluated the functionalities of the basic\\ncomponents of virtual datacenters, identified the major risks to the data\\ninfrastructure, and present here several solutions for overcoming potential\\nthreats to virtual infrastructure.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Research Agenda in Intelligent Infrastructure to Enhance Disaster\\n  Management, Community Resilience and Public Safety',\n",
       "  'text': 'Modern societies can be understood as the intersection of four interdependent\\nsystems: (1) the natural environment of geography, climate and weather; (2) the\\nbuilt environment of cities, engineered systems, and physical infrastructure;\\n(3) the social environment of human populations, communities and socio-economic\\nactivities; and (4) an information ecosystem that overlays the other three\\ndomains and provides the means for understanding, interacting with, and\\nmanaging the relationships between the natural, built, and human environments. As the nation and its communities become more connected, networked and\\ntechnologically sophisticated, new challenges and opportunities arise that\\ndemand a rethinking of current approaches to public safety and emergency\\nmanagement.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Research Agenda in Intelligent Infrastructure to Enhance Disaster\\n  Management, Community Resilience and Public Safety',\n",
       "  'text': 'Addressing the current and future challenges requires an equally\\nsophisticated program of research, technology development, and strategic\\nplanning. The design and integration of intelligent infrastructure-including\\nembedded sensors, the Internet of Things (IoT), advanced wireless information\\ntechnologies, real-time data capture and analysis, and machine-learning-based\\ndecision support-holds the potential to greatly enhance public safety,\\nemergency management, disaster recovery, and overall community resilience,\\nwhile addressing new and emerging threats to public safety and security. Ultimately, the objective of this program of research and development is to\\nsave lives, reduce risk and disaster impacts, permit efficient use of material\\nand social resources, and protect quality of life and economic stability across\\nentire regions.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'A cognitive radio ad hoc networks based disaster management scheme with\\n  efficient spectrum management, collaboration and interoperability',\n",
       "  'text': 'In this work, a disaster management scheme based on cognitive radio ad hoc\\nnetwork (CRAHN) has been presented. Disaster management has been a big problem\\nfor mankind for years. However, still not much research work has been presented\\non this problem. Technology has been employed in past few years to address this\\nproblem. Cognitive radio ad hoc network presents a viable solution for disaster\\nmanagement. It can be deployed rapidly without infrastructure and it solves the\\nspectrum scarcity and congestion issues that arise during disaster. This paper\\npresents a novel solution for disaster management. It provides a multi-layer\\nperceptron (MLP) based disaster detection scheme based on WSN.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'A cognitive radio ad hoc networks based disaster management scheme with\\n  efficient spectrum management, collaboration and interoperability',\n",
       "  'text': 'To solve the\\nspectrum scarcity problem, a MLP based spectrum management scheme has been\\nproposed. In order to ensure collaboration among rescue workers during\\ndisaster, a novel service discovery scheme has been proposed. To ensure\\ninteroperability during communication, XML format has been recommended. A\\nreal-time GUI has been proposed to provide shared situation awareness to rescue\\nworkers and enabling better decision making. The proposed approach has been\\nimplemented in NS-2 simulator. The results show accurate disaster detection,\\nefficient spectrum usage, and interoperability and collaboration among nodes\\nwith reduced latency.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Towards a disaster response system based on cognitive radio ad hoc\\n  networks',\n",
       "  'text': 'This paper presents an approach towards disaster management based on\\ncognitive radio ad hoc network. Despite the growing interests on cognitive\\nradio ad hoc networks, not much work has been reported on using them for\\ndisaster management. This paper discusses opportunities for disaster management\\nbased on cognitive radio ad hoc networks. In this direction, the paper presents\\na novel technique for disaster detection based on Artificial Neural Network\\n(ANN). The ANN is trained using backward propagation algorithm. An ANN-based\\nspectrum sensing scheme is also presented. Finally, a service discovery scheme\\nis presented for coordination during the time of disaster.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Towards a disaster response system based on cognitive radio ad hoc\\n  networks',\n",
       "  'text': 'The simulation of\\nproposed approach has been performed in NS-2 simulator. The proposed approach\\nshows very low false negative alarm rate using the proposed disaster detection\\nsystem. The spectrum switching time of spectrum sensing scheme is also analyzed\\nalong with an analysis of latency of proposed service discovery scheme',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'A semiparametric spatiotemporal Bayesian model for the bulk and extremes\\n  of the Fosberg Fire Weather Index',\n",
       "  'text': 'Large wildfires pose a major environmental concern, and precise maps of fire\\nrisk can improve disaster relief planning. Fosberg Fire Weather Index (FFWI) is\\noften used to measure wildfire risk; FFWI exhibits non-Gaussian marginal\\ndistributions as well as strong spatiotemporal extremal dependence and thus,\\nmodeling FFWI using geostatistical models like Gaussian processes is\\nquestionable. Extreme value theory (EVT)-driven models like max-stable\\nprocesses are theoretically appealing but are computationally demanding and\\napplicable only for threshold exceedances or block maxima. Disaster management\\npolicies often consider moderate-to-extreme quantiles of climate parameters and\\nhence, joint modeling of the bulk and the tail of the data is required.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'A semiparametric spatiotemporal Bayesian model for the bulk and extremes\\n  of the Fosberg Fire Weather Index',\n",
       "  'text': 'In this\\npaper, we consider a Dirichlet process mixture of spatial skew-t processes that\\ncan flexibly model the bulk as well as the tail. The proposed model has\\nnonstationary mean and covariance structure, and also nonzero spatiotemporal\\nextremal dependence. A simulation study demonstrates that the proposed model\\nhas better spatial prediction performance compared to some competing models. We\\ndevelop spatial maps of FFWI medians and extremes, and discuss the wildfire\\nrisk throughout the Santa Ana region of California.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Architecture of Environmental Risk Modelling: for a faster and more\\n  robust response to natural disasters',\n",
       "  'text': 'Demands on the disaster response capacity of the European Union are likely to\\nincrease, as the impacts of disasters continue to grow both in size and\\nfrequency. This has resulted in intensive research on issues concerning\\nspatially-explicit information and modelling and their multiple sources of\\nuncertainty. Geospatial support is one of the forms of assistance frequently\\nrequired by emergency response centres along with hazard forecast and event\\nmanagement assessment. Robust modelling of natural hazards requires dynamic\\nsimulations under an array of multiple inputs from different sources. Uncertainty is associated with meteorological forecast and calibration of the\\nmodel parameters.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Architecture of Environmental Risk Modelling: for a faster and more\\n  robust response to natural disasters',\n",
       "  'text': 'Software uncertainty also derives from the data\\ntransformation models (D-TM) needed for predicting hazard behaviour and its\\nconsequences. On the other hand, social contributions have recently been\\nrecognized as valuable in raw-data collection and mapping efforts traditionally\\ndominated by professional organizations. Here an architecture overview is\\nproposed for adaptive and robust modelling of natural hazards, following the\\nSemantic Array Programming paradigm to also include the distributed array of\\nsocial contributors called Citizen Sensor in a semantically-enhanced strategy\\nfor D-TM modelling.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Architecture of Environmental Risk Modelling: for a faster and more\\n  robust response to natural disasters',\n",
       "  'text': 'The modelling architecture proposes a multicriteria\\napproach for assessing the array of potential impacts with qualitative rapid\\nassessment methods based on a Partial Open Loop Feedback Control (POLFC) schema\\nand complementing more traditional and accurate a-posteriori assessment. We\\ndiscuss the computational aspect of environmental risk modelling using\\narray-based parallel paradigms on High Performance Computing (HPC) platforms,\\nin order for the implications of urgency to be introduced into the systems\\n(Urgent-HPC).',\n",
       "  'meta': {'_split_id': 2}},\n",
       " {'name': 'Large Variance and Fat Tail of Damage by Natural Disaster',\n",
       "  'text': 'In order to account for large variance and fat tail of damage by natural\\ndisaster, we study a simple model by combining distributions of disaster and\\npopulation/property with their spatial correlation. We assume fat-tailed or\\npower-law distributions for disaster and population/property exposed to the\\ndisaster, and a constant vulnerability for exposed population/property. Our\\nmodel suggests that the fat tail property of damage can be determined either by\\nthat of disaster or by those of population/property depending on which tail is\\nfatter. It is also found that the spatial correlations of population/property\\ncan enhance or reduce the variance of damage depending on how fat the tails of\\npopulation/property are.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Large Variance and Fat Tail of Damage by Natural Disaster',\n",
       "  'text': 'In case of tornadoes in the United States, we show\\nthat the damage does have fat tail property. Our results support that the\\nstandard cost-benefit analysis would not be reliable for social investment in\\nvulnerability reduction and disaster prevention.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Crowdsourced bi-directional disaster reporting and alerting on\\n  smartphones in Lao PDR',\n",
       "  'text': 'Natural disasters are a large threat for people especially in developing\\ncountries such as Laos. ICT-based disaster management systems aim at supporting\\ndisaster warning and response efforts. However, the ability to directly\\ncommunicate in both directions between local and administrative level is often\\nnot supported, and a tight integration into administrative workflows is\\nmissing. In this paper, we present the smartphone-based disaster and reporting\\nsystem Mobile4D. It allows for bi-directional communication while being fully\\ninvolved in administrative processes. We present the system setup and discuss\\nintegration into administrative structures in Lao PDR.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Switching-GAS Copula Models With Application to Systemic Risk',\n",
       "  'text': 'Recent financial disasters have emphasised the need to accurately predict\\nextreme financial losses and their consequences for the institutions belonging\\nto a given financial market. The ability of econometric models to predict\\nextreme events strongly relies on their flexibility to account for the highly\\nnonlinear and asymmetric dependence observed in financial returns. We develop a\\nnew class of flexible Copula models where the evolution of the dependence\\nparameters follow a Markov-Switching Generalised Autoregressive Score (SGASC)\\ndynamics. Maximum Likelihood estimation is consistently performed using the\\nInference Functions for Margins (IFM) approach and a version of the\\nExpectation-Maximisation (EM) algorithm specifically tailored to this class of\\nmodels.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Switching-GAS Copula Models With Application to Systemic Risk',\n",
       "  'text': 'The SGASC models are then used to estimate the Conditional\\nValue-at-Risk (CoVaR), which is defined as the VaR of a given asset conditional\\non another asset (or portfolio) being in financial distress, and the\\nConditional Expected Shortfall (CoES). Our empirical investigation shows that\\nthe proposed SGASC models are able to explain and predict the systemic risk\\ncontribution of several European countries. Moreover, we also find that the\\nSGASC models outperform competitors using several CoVaR backtesting procedures.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'A multi-agent system approach in evaluating human spatio-temporal\\n  vulnerability to seismic risk using social attachment',\n",
       "  'text': 'Social attachment theory states that individuals seek the proximity of\\nattachment figures (e.g. family members, friends, colleagues, familiar places\\nor objects) when faced with threat. During disasters, this means that family\\nmembers may seek each other before evacuating, gather personal property before\\nheading to familiar exits and places, or follow groups/crowds, etc. This\\nhard-wired human tendency should be considered in the assessment of risk and\\nthe creation of disaster management plans. Doing so may result in more\\nrealistic evacuation procedures and may minimise the number of casualties and\\ninjuries.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'A multi-agent system approach in evaluating human spatio-temporal\\n  vulnerability to seismic risk using social attachment',\n",
       "  'text': 'In this context, a dynamic spatio-temporal analysis of seismic risk\\nis presented using SOLACE, a multi-agent model of pedestrian behaviour based on\\nsocial attachment theory implemented using the Belief-Desire-Intention\\napproach. The model focuses on the influence of human, social, physical and\\ntemporal factors on successful evacuation. Human factors considered include\\nperception and mobility defined by age. Social factors are defined by\\nattachment bonds, social groups, population distribution, and cultural norms. Physical factors refer to the location of the epicentre of the earthquake,\\nspatial distribution/layout and attributes of environmental objects such as\\nbuildings, roads, barriers (cars), placement of safe areas, evacuation routes,\\nand the resulting debris/damage from the earthquake.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'A multi-agent system approach in evaluating human spatio-temporal\\n  vulnerability to seismic risk using social attachment',\n",
       "  'text': 'Experiments tested the\\ninfluence of time of the day, presence of disabled persons and earthquake\\nintensity. Initial results show that factors that influence arrivals in safe\\nareas include (a) human factors (age, disability, speed), (b) pre-evacuation\\nbehaviours, (c) perception distance (social attachment, time of day), (d)\\nsocial interaction during evacuation, and (e) physical and spatial aspects,\\nsuch as limitations imposed by debris (damage), and the distance to safe areas. To validate the results, scenarios will be designed with stakeholders, who will\\nalso take part in the definition of a serious game.',\n",
       "  'meta': {'_split_id': 2}},\n",
       " {'name': 'A multi-agent system approach in evaluating human spatio-temporal\\n  vulnerability to seismic risk using social attachment',\n",
       "  'text': 'The recommendation of this\\nresearch is that both social and physical aspects should be considered when\\ndefining vulnerability in the analysis of risk.',\n",
       "  'meta': {'_split_id': 3}},\n",
       " {'name': 'Disaster Monitoring using Unmanned Aerial Vehicles and Deep Learning',\n",
       "  'text': 'Monitoring of disasters is crucial for mitigating their effects on the\\nenvironment and human population, and can be facilitated by the use of unmanned\\naerial vehicles (UAV), equipped with camera sensors that produce aerial photos\\nof the areas of interest. A modern technique for recognition of events based on\\naerial photos is deep learning. In this paper, we present the state of the art\\nwork related to the use of deep learning techniques for disaster\\nidentification. We demonstrate the potential of this technique in identifying\\ndisasters with high accuracy, by means of a relatively simple deep learning\\nmodel.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Disaster Monitoring using Unmanned Aerial Vehicles and Deep Learning',\n",
       "  'text': 'Based on a dataset of 544 images (containing disaster images such as\\nfires, earthquakes, collapsed buildings, tsunami and flooding, as well as\\nnon-disaster scenes), our results show an accuracy of 91% achieved, indicating\\nthat deep learning, combined with UAV equipped with camera sensors, have the\\npotential to predict disasters with high accuracy.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Precision-Aware application execution for Energy-optimization in HPC\\n  node system',\n",
       "  'text': 'Power consumption is a critical consideration in high performance computing\\nsystems and it is becoming the limiting factor to build and operate Petascale\\nand Exascale systems. When studying the power consumption of existing systems\\nrunning HPC workloads, we find that power, energy and performance are closely\\nrelated which leads to the possibility to optimize energy consumption without\\nsacrificing (much or at all) the performance. In this paper, we propose a HPC\\nsystem running with a GNU/Linux OS and a Real Time Resource Manager (RTRM) that\\nis aware and monitors the healthy of the platform. On the system, an\\napplication for disaster management runs.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Precision-Aware application execution for Energy-optimization in HPC\\n  node system',\n",
       "  'text': 'The application can run with\\ndifferent QoS depending on the situation. We defined two main situations. Normal execution, when there is no risk of a disaster, even though we still\\nhave to run the system to look ahead in the near future if the situation\\nchanges suddenly. In the second scenario, the possibilities for a disaster are\\nvery high. Then the allocation of more resources for improving the precision\\nand the human decision has to be taken into account.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Precision-Aware application execution for Energy-optimization in HPC\\n  node system',\n",
       "  'text': 'The paper shows that at\\ndesign time, it is possible to describe different optimal points that are going\\nto be used at runtime by the RTOS with the application. This environment helps\\nto the system that must run 24/7 in saving energy with the trade-off of losing\\nprecision. The paper shows a model execution which can improve the precision of\\nresults by 65% in average by increasing the number of iterations from 1e3 to\\n1e4. This also produces one order of magnitude longer execution time which\\nleads to the need to use a multi-node solution.',\n",
       "  'meta': {'_split_id': 2}},\n",
       " {'name': 'Precision-Aware application execution for Energy-optimization in HPC\\n  node system',\n",
       "  'text': 'The optimal trade-off between\\nprecision vs. execution time is computed by the RTOS with the time overhead\\nless than 10% against a native execution.',\n",
       "  'meta': {'_split_id': 3}},\n",
       " {'name': 'IoT-based Emergency Evacuation Systems',\n",
       "  'text': 'Fires, earthquakes, floods, hurricanes, overcrowding, or and even pandemic\\nviruses endanger human lives. Hence, designing infrastructures to handle\\npossible emergencies has become an ever-increasing need. The safe evacuation of\\noccupants from the building takes precedence when dealing with the necessary\\nmitigation and disaster risk management. This thesis deals with designing an\\nIoT system to provide safe and quick evacuation suggestions. The IoT-based\\nevacuation system provides optimal evacuation paths that can be continuously\\nupdated based on run-time sensory data, so evacuation guidelines can be\\nadjusted according to visitors occupants that evolve over time.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'IoT-based Emergency Evacuation Systems',\n",
       "  'text': 'This thesis\\nmakes the following main contributions: i) Addressing an up to date state of\\nthe art class for IoT architectural styles and patterns; ii) Proposing a set of\\nself-adaptive IoT patterns and assessing their specific quality attributes\\n(fault-tolerance, energy consumption, and performance); iii) Designing an IoT\\ninfrastructure and testing its performance in both real-time and design-time\\napplications; iv) Developing a network flow algorithm that facilitates\\nminimizing the time necessary to evacuate people from a scene of a disaster; v)\\nModeling various social agents and their interactions during an emergency to\\nimprove the IoT system accordingly; vi) Evaluating the system by using\\nempirical and real case studies.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'A Twitter Tale of Three Hurricanes: Harvey, Irma, and Maria',\n",
       "  'text': 'People increasingly use microblogging platforms such as Twitter during\\nnatural disasters and emergencies. Research studies have revealed the\\nusefulness of the data available on Twitter for several disaster response\\ntasks. However, making sense of social media data is a challenging task due to\\nseveral reasons such as limitations of available tools to analyze high-volume\\nand high-velocity data streams. This work presents an extensive\\nmultidimensional analysis of textual and multimedia content from millions of\\ntweets shared on Twitter during the three disaster events.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'A Twitter Tale of Three Hurricanes: Harvey, Irma, and Maria',\n",
       "  'text': 'Specifically, we\\nemploy various Artificial Intelligence techniques from Natural Language\\nProcessing and Computer Vision fields, which exploit different machine learning\\nalgorithms to process the data generated during the disaster events. Our study\\nreveals the distributions of various types of useful information that can\\ninform crisis managers and responders as well as facilitate the development of\\nfuture automated systems for disaster management.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'NARMADA: Need and Available Resource Managing Assistant for Disasters\\n  and Adversities',\n",
       "  'text': 'Although a lot of research has been done on utilising Online Social Media\\nduring disasters, there exists no system for a specific task that is critical\\nin a post-disaster scenario -- identifying resource-needs and\\nresource-availabilities in the disaster-affected region, coupled with their\\nsubsequent matching. To this end, we present NARMADA, a semi-automated platform\\nwhich leverages the crowd-sourced information from social media posts for\\nassisting post-disaster relief coordination efforts. The system employs Natural\\nLanguage Processing and Information Retrieval techniques for identifying\\nresource-needs and resource-availabilities from microblogs, extracting\\nresources from the posts, and also matching the needs to suitable\\navailabilities.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'NARMADA: Need and Available Resource Managing Assistant for Disasters\\n  and Adversities',\n",
       "  'text': 'The system is thus capable of facilitating the judicious\\nmanagement of resources during post-disaster relief operations.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Simulation Pipeline for Traffic Evacuation in Urban Areas and Emergency\\n  Traffic Management Policy Improvements through Case Studies',\n",
       "  'text': 'Traffic evacuation plays a critical role in saving lives in devastating\\ndisasters such as hurricanes, wildfires, floods, earthquakes, etc. An ability\\nto evaluate evacuation plans in advance for these rare events, including\\nidentifying traffic flow bottlenecks, improving traffic management policies,\\nand understanding the robustness of the traffic management policy are critical\\nfor emergency management. Given the rareness of such events and the\\ncorresponding lack of real data, traffic simulation provides a flexible and\\nversatile approach for such scenarios, and furthermore allows dynamic\\ninteraction with the simulated evacuation.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Simulation Pipeline for Traffic Evacuation in Urban Areas and Emergency\\n  Traffic Management Policy Improvements through Case Studies',\n",
       "  'text': 'In this paper, we build a traffic\\nsimulation pipeline to explore the above problems, covering many aspects of\\nevacuation, including map creation, demand generation, vehicle behavior,\\nbottleneck identification, traffic management policy improvement, and results\\nanalysis. We apply the pipeline to two case studies in California. The first is\\nParadise, which was destroyed by a large wildfire in 2018 and experienced\\ncatastrophic traffic jams during the evacuation. The second is Mill Valley,\\nwhich has high risk of wildfire and potential traffic issues since the city is\\nsituated in a narrow valley.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Risk-limiting Load Restoration for Resilience Enhancement with\\n  Intermittent Energy Resources',\n",
       "  'text': 'Microgrids are resources that can be used to restore critical loads after a\\nnatural disaster, enhancing resilience of a distribution network. To deal with\\nthe stochastic nature of intermittent energy resources, such as wind turbines\\n(WTs) and photovoltaics (PVs), many methods rely on forecast information. However, some microgrids may not be equipped with power forecasting tools. To\\nfill this gap, a risk-limiting strategy based on measurements is proposed. Gaussian mixture model (GMM) is used to represent a prior joint probability\\ndensity function (PDF) of power outputs of WTs and PVs over multiple periods.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Risk-limiting Load Restoration for Resilience Enhancement with\\n  Intermittent Energy Resources',\n",
       "  'text': 'As time rolls forward, the distribution of WT/PV generation is updated based\\nthe latest measurement data in a recursive manner. The updated distribution is\\nused as an input for the risk-limiting load restoration problem, enabling an\\nequivalent transformation of the original chance constrained problem into a\\nmixed integer linear programming (MILP). Simulation cases on a distribution\\nsystem with three microgrids demonstrate the effectiveness of the proposed\\nmethod. Results also indicate that networked microgrids have better uncertainty\\nmanagement capabilities than stand-alone microgrids.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Tense, aspect and mood based event extraction for situation analysis and\\n  crisis management',\n",
       "  'text': 'Nowadays event extraction systems mainly deal with a relatively small amount\\nof information about temporal and modal qualifications of situations, primarily\\nprocessing assertive sentences in the past tense. However, systems with a wider\\ncoverage of tense, aspect and mood can provide better analyses and can be used\\nin a wider range of text analysis applications. This thesis develops such a\\nsystem for Turkish language.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Tense, aspect and mood based event extraction for situation analysis and\\n  crisis management',\n",
       "  'text': 'This is accomplished by extending Open Source\\nInformation Mining and Analysis (OPTIMA) research group\\'s event extraction\\nsoftware, by implementing appropriate extensions in the semantic representation\\nformat, by adding a partial grammar which improves the TAM (Tense, Aspect and\\nMood) marker, adverb analysis and matching functions of ExPRESS, and by\\nconstructing an appropriate lexicon in the standard of CORLEONE. These\\nextensions are based on iv the theory of anchoring relations (Tem\\\\\"urc\\\\\"u,\\n2007, 2011) which is a crosslinguistically applicable semantic framework for\\nanalyzing tense, aspect and mood related categories.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Tense, aspect and mood based event extraction for situation analysis and\\n  crisis management',\n",
       "  'text': 'The result is a system\\nwhich can, in addition to extracting basic event structures, classify sentences\\ngiven in news reports according to their temporal, modal and\\nvolitional/illocutionary values. Although the focus is on news reports of\\nnatural disasters, disease outbreaks and man-made disasters in Turkish\\nlanguage, the approach can be adapted to other languages, domains and genres. This event extraction and classification system, with further developments, can\\nprovide a basis for automated browsing systems for preventing environmental and\\nhumanitarian risk.',\n",
       "  'meta': {'_split_id': 2}},\n",
       " {'name': 'Floods impact dynamics quantified from big data sources',\n",
       "  'text': 'Natural disasters affect hundreds of millions of people worldwide every year. Early warning, humanitarian response and recovery mechanisms can be improved by\\nusing big data sources. Measuring the different dimensions of the impact of\\nnatural disasters is critical for designing policies and building up\\nresilience. Detailed quantification of the movement and behaviours of affected\\npopulations requires the use of high granularity data that entails privacy\\nrisks. Leveraging all this data is costly and has to be done ensuring privacy\\nand security of large amounts of data. Proxies based on social media and data\\naggregates would streamline this process by providing evidences and narrowing\\nrequirements.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Floods impact dynamics quantified from big data sources',\n",
       "  'text': 'We propose a framework that integrates environmental data, social\\nmedia, remote sensing, digital topography and mobile phone data to understand\\ndifferent types of floods and how data can provide insights useful for managing\\nhumanitarian action and recovery plans. Thus, data is dynamically requested\\nupon data-based indicators forming a multi-granularity and multi-access data\\npipeline. We present a composed study of three cases to show potential\\nvariability in the natures of floodings,as well as the impact and applicability\\nof data sources. Critical heterogeneity of the available data in the different\\ncases has to be addressed in order to design systematic approaches based on\\ndata.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Floods impact dynamics quantified from big data sources',\n",
       "  'text': 'The proposed framework establishes the foundation to relate the physical\\nand socio-economical impacts of floods.',\n",
       "  'meta': {'_split_id': 2}},\n",
       " {'name': 'Multi-stage models for the failure of complex systems, cascading\\n  disasters, and the onset of disease',\n",
       "  'text': 'Complex systems can fail through different routes, often progressing through\\na series of (rate-limiting) steps and modified by environmental exposures. The\\nonset of disease, cancer in particular, is no different. Multi-stage models\\nprovide a simple but very general mathematical framework for studying the\\nfailure of complex systems, or equivalently, the onset of disease. They include\\nthe Armitage Doll multi-stage cancer model as a particular case, and have\\npotential to provide new insights into how failures and disease, arise and\\nprogress. A method described by E.T.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Multi-stage models for the failure of complex systems, cascading\\n  disasters, and the onset of disease',\n",
       "  'text': 'Jaynes is developed to provide an\\nanalytical solution for a large class of these models, and highlights\\nconnections between the convolution of Laplace transforms, sums of random\\nvariables, and Schwinger/Feynman parameterisations. Examples include: exact\\nsolutions to the Armitage-Doll model, the sum of Gamma-distributed variables\\nwith integer-valued shape parameters, a clonal-growth cancer model, and a model\\nfor cascading disasters. Applications and limitations of the approach are\\ndiscussed in the context of recent cancer research. The model is sufficiently\\ngeneral to be used in many contexts, such as engineering, project management,\\ndisease progression, and disaster risk for example, allowing the estimation of\\nfailure rates in complex systems and projects.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Multi-stage models for the failure of complex systems, cascading\\n  disasters, and the onset of disease',\n",
       "  'text': 'The intended result is a\\nmathematical toolkit for applying multi-stage models to the study of failure\\nrates in complex systems and to the onset of disease, cancer in particular.',\n",
       "  'meta': {'_split_id': 2}},\n",
       " {'name': 'Weathering Adaptation: Grid Infrastructure Planning in a Changing\\n  Climate',\n",
       "  'text': 'Decisions related to electric power systems planning and operations rely on\\nassumptions and insights informed by historic weather data and records of past\\nperformance. Evolving climate trends are, however, changing the energy use\\npatterns and operating conditions of grid assets, thus altering the nature and\\nseverity of risks the system faces. Because grid assets remain in operation for\\ndecades, planning for evolving risks will require incorporating climate\\nprojections into grid infrastructure planning processes. The current work\\ntraces a pathway for climate-aware decision-making in the electricity sector. We evaluate the suitability of using existing climate models and data for\\nelectricity planning and discuss their limitations.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Weathering Adaptation: Grid Infrastructure Planning in a Changing\\n  Climate',\n",
       "  'text': 'We review the interactions\\nbetween grid infrastructure and climate by synthesizing what is known about how\\nchanging environmental operating conditions would impact infrastructure\\nutilization, constraints, and performance. We contextualize our findings by\\npresenting a case study of California, examining if and where climate data can\\nbe integrated into infrastructure planning processes. The core contribution of\\nthe work is a series of nine recommendations detailing advancements in climate\\nprojections, grid modeling architecture, and disaster preparedness that would\\nbe needed to ensure that infrastructure planning decisions are robust to\\nuncertainty and risks associated with evolving climate conditions.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Social media usage patterns during natural hazards',\n",
       "  'text': 'Natural hazards are becoming increasingly expensive as climate change and\\ndevelopment are exposing communities to greater risks. Preparation and recovery\\nare critical for climate change resilience, and social media are being used\\nmore and more to communicate before, during, and after disasters. While there\\nis a growing body of research aimed at understanding how people use social\\nmedia surrounding disaster events, most existing work has focused on a single\\ndisaster case study. In the present study, we analyze five of the costliest\\ndisasters in the last decade in the United States (Hurricanes Irene and Sandy,\\ntwo sets of tornado outbreaks, and flooding in Louisiana) through the lens of\\nTwitter.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Social media usage patterns during natural hazards',\n",
       "  'text': 'In particular, we explore the frequency of both generic and specific\\nfood-security related terms, and quantify the relationship between network size\\nand Twitter activity during disasters. We find differences in tweet volume for\\nkeywords depending on disaster type, with people using Twitter more frequently\\nin preparation for Hurricanes, and for real-time or recovery information for\\ntornado and flooding events. Further, we find that people share a host of\\ngeneral disaster and specific preparation and recovery terms during these\\nevents.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Social media usage patterns during natural hazards',\n",
       "  'text': 'Finally, we find that among all account types, individuals with\\n\"average\" sized networks are most likely to share information during these\\ndisasters, and in most cases, do so more frequently than normal. This suggests\\nthat around disasters, an ideal form of social contagion is being engaged in\\nwhich average people rather than outsized influentials are key to\\ncommunication. These results provide important context for the type of disaster\\ninformation and target audiences that may be most useful for disaster\\ncommunication during varying extreme events.',\n",
       "  'meta': {'_split_id': 2}},\n",
       " {'name': 'Deep Bayesian U-Nets for Efficient, Robust and Reliable Post-Disaster\\n  Damage Localization',\n",
       "  'text': 'Post-disaster inspections are critical to emergency management after\\nearthquakes. The availability of data on the condition of civil infrastructure\\nimmediately after an earthquake is of great importance for emergency\\nmanagement. Stakeholders require this information to take effective actions and\\nto better recover from the disaster. The data-driven SHM has shown great\\npromises to achieve this goal in near real-time. There have been several\\nproposals to automate the inspection process from different sources of input\\nusing deep learning. The existing models in the literature only provide a final\\nprediction output, while the risks of utilizing such models for safety-critical\\nassessments should not be ignored.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Deep Bayesian U-Nets for Efficient, Robust and Reliable Post-Disaster\\n  Damage Localization',\n",
       "  'text': 'This paper is dedicated to developing deep\\nBayesian U-Nets where the uncertainty of predictions is a second output of the\\nmodel, which is made possible through Monte Carlo dropout sampling in test\\ntime. Based on a grid-like data structure, the concept of semantic damage\\nsegmentation (SDS) is revisited. Compared to image segmentation, it is shown\\nthat a much higher level of precision is necessary for damage diagnosis. To\\nvalidate and test the proposed framework, a benchmark dataset, 10,800 nonlinear\\nresponse history analyses on a 10-story-10-bay 2D reinforced concrete moment\\nframe, is utilized.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Deep Bayesian U-Nets for Efficient, Robust and Reliable Post-Disaster\\n  Damage Localization',\n",
       "  'text': \"Compared to the benchmark SDS model, Bayesian models\\nexhibit superior robustness with enhanced global and mean class accuracies. Finally, the model's uncertainty output is studied by monitoring the softmax\\nclass variance of different predictions. It is shown that class variance\\ncorrelates well with locations where the model makes mistakes. This output can\\nbe used in combination with the prediction results to increase the reliability\\nof this data-driven framework in structural inspections.\",\n",
       "  'meta': {'_split_id': 2}},\n",
       " {'name': 'Developing an Efficient DMCIS with Next-Generation Wireless Networks',\n",
       "  'text': 'The impact of extreme events across the globe is extraordinary which\\ncontinues to handicap the advancement of the struggling developing societies\\nand threatens most of the industrialized countries in the globe. Various fields\\nof Information and Communication Technology have widely been used for efficient\\ndisaster management; but only to a limited extent though, there is a tremendous\\npotential for increasing efficiency and effectiveness in coping with disasters\\nwith the utilization of emerging wireless network technologies. Early warning,\\nresponse to the particular situation and proper recovery are among the main\\nfocuses of an efficient disaster management system today.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Developing an Efficient DMCIS with Next-Generation Wireless Networks',\n",
       "  'text': 'Considering these\\naspects, in this paper we propose a framework for developing an efficient\\nDisaster Management Communications and Information System (DMCIS) which is\\nbasically benefited by the exploitation of the emerging wireless network\\ntechnologies combined with other networking and data processing technologies.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'A model-free characterization of recurrences in stationary time series',\n",
       "  'text': 'Study of recurrences in earthquakes, climate, financial time-series, etc. is\\ncrucial to better forecast disasters and limit their consequences. However,\\nalmost all the previous phenomenological studies involved only a long-ranged\\nautocorrelation function, or disregarded the multi-scaling properties induced\\nby potential higher order dependencies. Consequently, they missed the facts\\nthat non-linear dependences do impact both the statistics and dynamics of\\nrecurrence times, and that scaling arguments for the unconditional distribution\\nmay not be applicable. We argue that copulas is the correct model-free\\nframework to study non-linear dependencies in time series and related concepts\\nlike recurrences.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'A model-free characterization of recurrences in stationary time series',\n",
       "  'text': 'Fitting and/or simulating the intertemporal distribution of\\nrecurrence intervals is very much system specific, and cannot actually benefit\\nfrom universal features, in contrast to the previous claims. This has important\\nimplications in epilepsy prognosis and financial risk management applications.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Decentralized, Robust and Efficient Services for an Autonomous and\\n  Real-time Urban Crisis Management',\n",
       "  'text': 'The globalization of trade and the organization of work are currently causing\\na large migratory flow towards the cities. This growth of cities requires new\\nurban planning where digital tools take a preponderant place to capture data\\nand understand and decide in face of changes. These tools however hardly resist\\nto natural disasters, terrorism, accidents, etc.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Decentralized, Robust and Efficient Services for an Autonomous and\\n  Real-time Urban Crisis Management',\n",
       "  'text': 'Based on the expertise of the\\nCITI laboratory of INSA Lyon and SC3 of the Industrial University of Santander,\\nwe propose to create the ALERT project - Autonomous Liable Emergency service in\\nReal Time - with decentralized, reliable and efficient services, physically\\nclose to the citizens, taking decisions locally, in a relevant manner without\\nrisk of disconnection with a central authority. These information gathering and\\ndecision-making will involve the population with participatory and social\\napproaches.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Mitigating Docker Security Issues',\n",
       "  'text': 'It is very easy to run applications in Docker. Docker offers an ecosystem\\nthat offers a platform for application packaging, distributing and managing\\nwithin containers. However, Docker platform is yet not matured. Presently,\\nDocker is less secured as compare to virtual machines (VM) and most of the\\nother cloud technologies. The key of reason of Docker inadequate security\\nprotocols is containers sharing of Linux kernel, which can lead to risk of\\nprivileged escalations. This research is going to outline some major security\\nvulnerabilities at Docker and counter solutions to neutralize such attacks. There are variety of security attacks like insider and outsider.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Mitigating Docker Security Issues',\n",
       "  'text': 'This research\\nwill outline both types of attacks and their mitigations strategies. Taking\\nsome precautionary measures can save from huge disasters. This research will\\nalso present Docker secure deployment guidelines. These guidelines will suggest\\ndifferent configurations to deploy Docker containers in a more secure way.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Assessing Disaster Impacts on Highways Using Social Media: Case Study of\\n  Hurricane Harvey',\n",
       "  'text': 'During and after disasters, highways provide vital routes for emergency\\nservices, relief efforts, and evacuation activities. Thus, a timely and\\nreliable assessment of disaster impacts on highways is critical for\\ndecision-makers to quickly and effectively perform relief and recovery efforts. Recently, social media has increasingly been used in disaster management for\\nobtaining a rapid, public-centric assessment of disaster impacts due to its\\nnear real-time, social and informational characteristics. Although promising,\\nthe employment of social media for assessing disaster impacts on highways is\\nstill limited due to the inability of extracting accurate highway-related data\\nfrom social media.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Assessing Disaster Impacts on Highways Using Social Media: Case Study of\\n  Hurricane Harvey',\n",
       "  'text': 'To overcome this limitation, a systematic approach is\\nproposed to identify highway-related data from social media for assessing\\ndisaster impacts on highways, and a case study of Hurricane Harvey in Houston,\\nTX is employed for the demonstration. The approach is constructed through three\\nsteps: (1) building data sources for social media and highways of interest in\\nHouston, respectively; (2) adapting the social media data to each highway\\nthrough a developed mapping algorithm; (3) assessing disaster impacts through\\nanalyzing social media activities in terms of their intensity, geographic, and\\ntopic distributions. Results show that the proposed approach is capable of\\ncapturing the temporal patterns of disaster impacts on highways.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Assessing Disaster Impacts on Highways Using Social Media: Case Study of\\n  Hurricane Harvey',\n",
       "  'text': 'Official news\\nand reports are employed to validate the assessed impacts.',\n",
       "  'meta': {'_split_id': 2}},\n",
       " {'name': 'Learning from Multimodal and Multitemporal Earth Observation Data for\\n  Building Damage Mapping',\n",
       "  'text': \"Earth observation technologies, such as optical imaging and synthetic\\naperture radar (SAR), provide excellent means to monitor ever-growing urban\\nenvironments continuously. Notably, in the case of large-scale disasters (e.g.,\\ntsunamis and earthquakes), in which a response is highly time-critical, images\\nfrom both data modalities can complement each other to accurately convey the\\nfull damage condition in the disaster's aftermath. However, due to several\\nfactors, such as weather and satellite coverage, it is often uncertain which\\ndata modality will be the first available for rapid disaster response efforts. Hence, novel methodologies that can utilize all accessible EO datasets are\\nessential for disaster management.\",\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Learning from Multimodal and Multitemporal Earth Observation Data for\\n  Building Damage Mapping',\n",
       "  'text': 'In this study, we have developed a global\\nmultisensor and multitemporal dataset for building damage mapping. We included\\nbuilding damage characteristics from three disaster types, namely, earthquakes,\\ntsunamis, and typhoons, and considered three building damage categories. The\\nglobal dataset contains high-resolution optical imagery and\\nhigh-to-moderate-resolution multiband SAR data acquired before and after each\\ndisaster. Using this comprehensive dataset, we analyzed five data modality\\nscenarios for damage mapping: single-mode (optical and SAR datasets),\\ncross-modal (pre-disaster optical and post-disaster SAR datasets), and mode\\nfusion scenarios. We defined a damage mapping framework for the semantic\\nsegmentation of damaged buildings based on a deep convolutional neural network\\nalgorithm.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Learning from Multimodal and Multitemporal Earth Observation Data for\\n  Building Damage Mapping',\n",
       "  'text': 'We compare our approach to another state-of-the-art baseline model\\nfor damage mapping. The results indicated that our dataset, together with a\\ndeep learning network, enabled acceptable predictions for all the data modality\\nscenarios.',\n",
       "  'meta': {'_split_id': 2}},\n",
       " {'name': 'Disaster-Resilient Control Plane Design and Mapping in Software-Defined\\n  Networks',\n",
       "  'text': 'Communication networks, such as core optical networks, heavily depend on\\ntheir physical infrastructure, and hence they are vulnerable to man-made\\ndisasters, such as Electromagnetic Pulse (EMP) or Weapons of Mass Destruction\\n(WMD) attacks, as well as to natural disasters. Large-scale disasters may cause\\nhuge data loss and connectivity disruption in these networks. As our dependence\\non network services increases, the need for novel survivability methods to\\nmitigate the effects of disasters on communication networks becomes a major\\nconcern. Software-Defined Networking (SDN), by centralizing control logic and\\nseparating it from physical equipment, facilitates network programmability and\\nopens up new ways to design disaster-resilient networks.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Disaster-Resilient Control Plane Design and Mapping in Software-Defined\\n  Networks',\n",
       "  'text': 'On the other hand, to\\nfully exploit the potential of SDN, along with data-plane survivability, we\\nalso need to design the control plane to be resilient enough to survive network\\nfailures caused by disasters. Several distributed SDN controller architectures\\nhave been proposed to mitigate the risks of overload and failure, but they are\\noptimized for limited faults without addressing the extent of large-scale\\ndisaster failures. For disaster resiliency of the control plane, we propose to\\ndesign it as a virtual network, which can be solved using Virtual Network\\nMapping techniques.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Disaster-Resilient Control Plane Design and Mapping in Software-Defined\\n  Networks',\n",
       "  'text': 'We select appropriate mapping of the controllers over the\\nphysical network such that the connectivity among the controllers\\n(controller-to-controller) and between the switches to the controllers\\n(switch-to-controllers) is not compromised by physical infrastructure failures\\ncaused by disasters. We formally model this disaster-aware control-plane design\\nand mapping problem, and demonstrate a significant reduction in the disruption\\nof controller-to-controller and switch-to-controller communication channels\\nusing our approach.',\n",
       "  'meta': {'_split_id': 2}},\n",
       " {'name': 'A review on application of data mining techniques to combat natural\\n  disasters',\n",
       "  'text': 'Thousands of human lives are lost every year around the globe, apart from\\nsignificant damage on property, animal life, etc., due to natural disasters\\n(e.g., earthquake, flood, tsunami, hurricane and other storms, landslides,\\ncloudburst, heat wave, forest fire). In this paper, we focus on reviewing the\\napplication of data mining and analytical techniques designed so far for (i)\\nprediction, (ii) detection, and (iii) development of appropriate disaster\\nmanagement strategy based on the collected data from disasters. A detailed\\ndescription of availability of data from geological observatories\\n(seismological, hydrological), satellites, remote sensing and newer sources\\nlike social networking sites as twitter is presented.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'A review on application of data mining techniques to combat natural\\n  disasters',\n",
       "  'text': 'An extensive and in-depth\\nliterature study on current techniques for disaster prediction, detection and\\nmanagement has been done and the results are summarized according to various\\ntypes of disasters. Finally a framework for building a disaster management\\ndatabase for India hosted on open source Big Data platform like Hadoop in a\\nphased manner has been proposed. The study has special focus on India which\\nranks among top five counties in terms of absolute number of the loss of human\\nlife.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Mining Social Media to Inform Peatland Fire and Haze Disaster Management',\n",
       "  'text': 'Peatland fires and haze events are disasters with national, regional and\\ninternational implications. The phenomena lead to direct damage to local\\nassets, as well as broader economic and environmental losses. Satellite imagery\\nis still the main and often the only available source of information for\\ndisaster management. In this article, we test the potential of social media to\\nassist disaster management. To this end, we compare insights from two datasets:\\nfire hotspots detected via NASA satellite imagery and almost all GPS-stamped\\ntweets from Sumatra Island, Indonesia, posted during 2014.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Mining Social Media to Inform Peatland Fire and Haze Disaster Management',\n",
       "  'text': \"Sumatra Island is\\nchosen as it regularly experiences a significant number of haze events, which\\naffect citizens in Indonesia as well as in nearby countries including Malaysia\\nand Singapore. We analyse temporal correlations between the datasets and their\\ngeo-spatial interdependence. Furthermore, we show how Twitter data reveals\\nchanges in users' behavior during severe haze events. Overall, we demonstrate\\nthat social media is a valuable source of complementary and supplementary\\ninformation for haze disaster management. Based on our methodology and\\nfindings, an analytics tool to improve peatland fire and haze disaster\\nmanagement by the Indonesian authorities is under development.\",\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Satellite imagery analysis for operational damage assessment in\\n  Emergency situations',\n",
       "  'text': 'When major disaster occurs the questions are raised how to estimate the\\ndamage in time to support the decision making process and relief efforts by\\nlocal authorities or humanitarian teams. In this paper we consider the use of\\nMachine Learning and Computer Vision on remote sensing imagery to improve time\\nefficiency of assessment of damaged buildings in disaster affected area. We\\npropose a general workflow that can be useful in various disaster management\\napplications, and demonstrate the use of the proposed workflow for the\\nassessment of the damage caused by the wildfires in California in 2017.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Optimal Clearing Payments in a Financial Contagion Model',\n",
       "  'text': 'Modern financial networks are characterized by complex structures of mutual\\nobligations. Such interconnections may propagate and amplificate individual\\ndefaults, leading in some cases to financial disaster. For this reason,\\nmathematical models for the study and control of systemic risk (the risk of\\nsevere instabilities on the system as a whole, due to default of single\\nentities) have attracted considerable research attention in recent years. One\\nimportant line of research is concerned with mechanisms of clearing, that is,\\nthe mechanism by which mutual debts are repaid, in the regular regime, or in a\\ndefault regime.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Optimal Clearing Payments in a Financial Contagion Model',\n",
       "  'text': \"One of the first models of a clearing mechanism was proposed by\\nEisenberg and Noe and is based on the three rules: limited liability, the\\npriority of debt claims over the shareholders' interests, and the equal\\npriority of debts (pro-rata rule). These three principles naturally lead to the\\nconcept of clearing vector (the vector of the entities' total payments). In\\nthis paper, we propose a necessary and sufficient condition for the uniqueness\\nof clearing vector applicable to an arbitrary topology of the financial\\nnetwork.\",\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Optimal Clearing Payments in a Financial Contagion Model',\n",
       "  'text': 'Further, we show that the overall system loss can be reduced if one\\nrelaxes the pro-rata rule and replaces the clearing vector by a matrix of\\nclearing payments. This approach shifts the focus from the individual interest\\nto the system, or social, interest, in order to control and contain the adverse\\neffects of cascaded failures.',\n",
       "  'meta': {'_split_id': 2}},\n",
       " {'name': 'Near-optimal planning using approximate dynamic programming to enhance\\n  post-hazard community resilience management',\n",
       "  'text': 'The lack of a comprehensive decision-making approach at the community level\\nis an important problem that warrants immediate attention. Network-level\\ndecision-making algorithms need to solve large-scale optimization problems that\\npose computational challenges. The complexity of the optimization problems\\nincreases when various sources of uncertainty are considered. This research\\nintroduces a sequential discrete optimization approach, as a decision-making\\nframework at the community level for recovery management. The proposed\\nmathematical approach leverages approximate dynamic programming along with\\nheuristics for the determination of recovery actions. Our methodology overcomes\\nthe curse of dimensionality and manages multi-state, large-scale infrastructure\\nsystems following disasters.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Near-optimal planning using approximate dynamic programming to enhance\\n  post-hazard community resilience management',\n",
       "  'text': 'We also provide computational results showing that\\nour methodology not only incorporates recovery policies of responsible public\\nand private entities within the community but also substantially enhances the\\nperformance of their underlying strategies with limited resources. The\\nmethodology can be implemented efficiently to identify near-optimal recovery\\ndecisions following a severe earthquake based on multiple objectives for an\\nelectrical power network of a testbed community coarsely modeled after Gilroy,\\nCalifornia, United States. The proposed optimization method supports\\nrisk-informed community decision makers within chaotic post-hazard\\ncircumstances.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Computer-Suported Risk Identification for the Holistic Management of\\n  Risks',\n",
       "  'text': 'Risk is part of the fabric of every business; surprisingly, there is little\\nwork on establishing best practices for systematic, repeatable risk\\nidentification, arguably the first step of any risk management process. In this\\npaper, we present a proposal that constitutes a more holistic risk management\\napproach, a methodology for computer-supported risk identification is proposed\\nthat may lead to more consistent (objective, repeatable) risk analysis.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Early Warning of Human Crowds Based on Query Data from Baidu Map:\\n  Analysis Based on Shanghai Stampede',\n",
       "  'text': \"Without sufficient preparation and on-site management, the mass scale\\nunexpected huge human crowd is a serious threat to public safety. A recent\\nimpressive tragedy is the 2014 Shanghai Stampede, where 36 people were killed\\nand 49 were injured in celebration of the New Year's Eve on December 31th 2014\\nin the Shanghai Bund. Due to the innately stochastic and complicated individual\\nmovement, it is not easy to predict collective gatherings, which potentially\\nleads to crowd events. In this paper, with leveraging the big data generated on\\nBaidu map, we propose a novel approach to early warning such potential crowd\\ndisasters, which has profound public benefits.\",\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Early Warning of Human Crowds Based on Query Data from Baidu Map:\\n  Analysis Based on Shanghai Stampede',\n",
       "  'text': \"An insightful observation is\\nthat, with the prevalence and convenience of mobile map service, users usually\\nsearch on the Baidu map to plan a routine. Therefore, aggregating users' query\\ndata on Baidu map can obtain priori and indication information for estimating\\nfuture human population in a specific area ahead of time. Our careful analysis\\nand deep investigation on the Baidu map data on various events also\\ndemonstrates a strong correlation pattern between the number of map query and\\nthe number of positioning users in an area.\",\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Early Warning of Human Crowds Based on Query Data from Baidu Map:\\n  Analysis Based on Shanghai Stampede',\n",
       "  'text': 'Based on such observation, we\\npropose a decision method utilizing query data on Baidu map to invoke warnings\\nfor potential crowd events about 1-3 hours in advance. Then we also construct a\\nmachine learning model with heterogeneous data (such as query data and mobile\\npositioning data) to quantitatively measure the risk of the potential crowd\\ndisasters. We evaluate the effectiveness of our methods on the data of Baidu\\nmap.',\n",
       "  'meta': {'_split_id': 2}},\n",
       " {'name': '\"Breaking\" Disasters: Predicting and Characterizing the Global News\\n  Value of Natural and Man-made Disasters',\n",
       "  'text': \"Due to their often unexpected nature, natural and man-made disasters are\\ndifficult to monitor and detect for journalists and disaster management\\nresponse teams. Journalists are increasingly relying on signals from social\\nmedia to detect such stories in their early stage of development. Twitter,\\nwhich features a vast network of local news outlets, is a major source of early\\nsignal for disaster detection. Journalists who work for global desks often\\nfollow these sources via Twitter's lists, but have to comb through thousands of\\nsmall-scale or low-impact stories to find events that may be globally relevant. These are events that have a large scope, high impact, or potential\\ngeo-political relevance.\",\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': '\"Breaking\" Disasters: Predicting and Characterizing the Global News\\n  Value of Natural and Man-made Disasters',\n",
       "  'text': 'We propose a model for automatically identifying\\nevents from local news sources that may break on a global scale within the next\\n24 hours. The results are promising and can be used in a predictive setting to\\nhelp journalists manage their sources more effectively, or in a descriptive\\nmanner to analyze media coverage of disasters. Through the feature evaluation\\nprocess, we also address the question: \"what makes a disaster event newsworthy\\non a global scale?\" As part of our data collection process, we have created a\\nlist of local sources of disaster/accident news on Twitter, which we have made\\npublicly available.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Cost-Efficient Data Backup for Data Center Networks against\\n  ε-Time Early Warning Disaster',\n",
       "  'text': 'Data backup in data center networks (DCNs) is critical to minimize the data\\nloss under disaster. This paper considers the cost-efficient data backup for\\nDCNs against a disaster with $\\\\varepsilon$ early warning time. Given\\ngeo-distributed DCNs and such a $\\\\varepsilon$-time early warning disaster, we\\ninvestigate the issue of how to back up the data in DCN nodes under risk to\\nother safe DCN nodes within the $\\\\varepsilon$ early warning time constraint,\\nwhich is significant because it is an emergency data protection scheme against\\na predictable disaster and also help DCN operators to build a complete backup\\nscheme, i.e., regular backup and emergency backup.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Cost-Efficient Data Backup for Data Center Networks against\\n  ε-Time Early Warning Disaster',\n",
       "  'text': 'Specifically, an Integer\\nLinear Program (ILP)-based theoretical framework is proposed to identify the\\noptimal selections of backup DCN nodes and data transmission paths, such that\\nthe overall data backup cost is minimized. Extensive numerical results are also\\nprovided to illustrate the proposed framework for DCN data backup.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Unveiling Spatial Patterns of Disaster Impacts and Recovery Using Credit\\n  Card Transaction Variances',\n",
       "  'text': 'The objective of this study is to examine spatial patterns of impacts and\\nrecovery of communities based on variances in credit card transactions. Such\\nvariances could capture the collective effects of household impacts, disrupted\\naccesses, and business closures, and thus provide an integrative measure for\\nexamining disaster impacts and community recovery in disasters. Existing\\nstudies depend mainly on survey and sociodemographic data for disaster impacts\\nand recovery effort evaluations, although such data has limitations, including\\nlarge data collection efforts and delayed timeliness results.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Unveiling Spatial Patterns of Disaster Impacts and Recovery Using Credit\\n  Card Transaction Variances',\n",
       "  'text': 'In addition,\\nthere are very few studies have concentrated on spatial patterns and\\ndisparities of disaster impacts and short-term recovery of communities,\\nalthough such investigation can enhance situational awareness during disasters\\nand support the identification of disparate spatial patterns of disaster\\nimpacts and recovery in the impacted regions. This study examines credit card\\ntransaction data Harris County (Texas, USA) during Hurricane Harvey in 2017 to\\nexplore spatial patterns of disaster impacts and recovery during from the\\nperspective of community residents and businesses at ZIP code and county\\nscales, respectively, and to further investigate their spatial disparities\\nacross ZIP codes.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Unveiling Spatial Patterns of Disaster Impacts and Recovery Using Credit\\n  Card Transaction Variances',\n",
       "  'text': 'The results indicate that individuals in ZIP codes with\\npopulations of higher income experienced more severe disaster impact and\\nrecovered more quickly than those located in lower-income ZIP codes for most\\nbusiness sectors. Our findings not only enhance the understanding of spatial\\npatterns and disparities in disaster impacts and recovery for better community\\nresilience assessment, but also could benefit emergency managers, city\\nplanners, and public officials in harnessing population activity data, using\\ncredit card transactions as a proxy for activity, to improve situational\\nawareness and resource allocation.',\n",
       "  'meta': {'_split_id': 2}},\n",
       " {'name': 'Collective Dynamics of Hierarchical Networks',\n",
       "  'text': 'In an increasingly complex, mobile and interconnected world, we face growing\\nthreats of disasters, whether by chance or deliberately. Disruption of\\ncoordinated response and recovery efforts due to organizational, technical,\\nprocedural, random or deliberate attack could result in the risk of massive\\nloss of life. This requires urgent action to explore the development of optimal\\ninformation-sharing environments for promoting collective disaster response and\\npreparedness using multijurisdictional hierarchical networks.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Collective Dynamics of Hierarchical Networks',\n",
       "  'text': 'Innovative\\napproaches to information flow modeling and analysis for dealing with\\nchallenges of coordinating across multi layered agency structures as well as\\ndevelopment of early warnings through social systems using social media\\nanalytics may be pivotal to timely responses to dealing with large scale\\ndisasters where response strategies need to be viewed as a shared\\nresponsibility. How do facilitate the development of collective disaster\\nresponse in a multijurisdictional setting? How do we develop and test the level\\nand effectiveness of shared multijurisdictional hierarchical networks for\\nimproved preparedness and response? What is the role of multi layered training\\nand exercises in building the shared learning space for collective disaster\\npreparedness and response?',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Collective Dynamics of Hierarchical Networks',\n",
       "  'text': 'The aim of this is therefore to determine factors\\nthat may be responsible for affecting disaster response.',\n",
       "  'meta': {'_split_id': 2}},\n",
       " {'name': 'Resilience-based performance modeling and decision optimization for\\n  transportation network',\n",
       "  'text': \"This research presented a novel resilience-based framework to support\\nresilience planning regarding pre-disaster mitigation and post-disaster\\nrecovery. First, the author proposes a new performance metric for transportation\\nnetwork, weighted number of independent pathways (WIPW), integrating the\\nnetwork topology, redundancy level, traffic patterns, structural reliability of\\nnetwork components, and functionality of the network during community's\\npost-disaster recovery in a systematical way. To the best of our knowledge,\\nWIPW is the only performance metric that permits risk mitigation alternatives\\nfor improving transportation network resilience to be compared on a common\\nbasis. Based on the WIPW, a decision methodology of prioritizing transportation\\nnetwork retrofit projects is developed.\",\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Resilience-based performance modeling and decision optimization for\\n  transportation network',\n",
       "  'text': 'Second, our studies extend from pre-disaster mitigation to post-hazard\\nrecovery, in which this research presents two metrics to evaluate the\\nrestoration over the horizon after disasters . That is, total recovery time and\\nthe skew of the recovery trajectory. Both metrics are involved in the\\nmulti-objective stochastic optimization problem of restoration scheduling. The\\nmetrics provided a new dimension to evaluate the relative efficiency of\\nalternative network recovery strategies. The author then develops a restoration\\nscheduling methodology for network post-disaster recovery that minimizes the\\noverall network recovery time and optimizes the recovery trajectory, which\\nultimately will reduce economic losses due to network service disruption.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Modeling s-t Path Availability to Support Disaster Vulnerability\\n  Assessment of Network Infrastructure',\n",
       "  'text': 'The maintenance of system flow is critical for effective network operation. Any type of disruption to network facilities (arcs/nodes) potentially risks\\nloss of service, leaving users without access to important resources. It is\\ntherefore an important goal of planners to assess infrastructures for\\nvulnerabilities, identifying those vital nodes/arcs whose debilitation would\\ncompromise the most source-sink (s-t) interaction or system flow. Due to the\\nbudgetary limitations of disaster management agencies, protection/fortification\\nand planning for the recovery of these vital infrastructure facilities is a\\nlogical and efficient proactive approach to reducing worst-case risk of service\\ndisruption.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Modeling s-t Path Availability to Support Disaster Vulnerability\\n  Assessment of Network Infrastructure',\n",
       "  'text': 'Given damage to a network, evaluating the potential for flow\\nbetween s-t pairs requires assessing the availability of an operational s-t\\npath. Recent models proposed for identifying infrastructure vital to system\\nflow have relied on enumeration of all s-t paths to support this task. This\\npaper proposes an alternative model constraint structure that does not require\\ncomplete enumeration of s-t paths, providing computational benefits over\\nexisting models. To illustrate the model, an application to a practical\\ninfrastructure planning problem is presented.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'ExGAN: Adversarial Generation of Extreme Samples',\n",
       "  'text': 'Mitigating the risk arising from extreme events is a fundamental goal with\\nmany applications, such as the modelling of natural disasters, financial\\ncrashes, epidemics, and many others. To manage this risk, a vital step is to be\\nable to understand or generate a wide range of extreme scenarios. Existing\\napproaches based on Generative Adversarial Networks (GANs) excel at generating\\nrealistic samples, but seek to generate typical samples, rather than extreme\\nsamples. Hence, in this work, we propose ExGAN, a GAN-based approach to\\ngenerate realistic and extreme samples.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'ExGAN: Adversarial Generation of Extreme Samples',\n",
       "  'text': 'To model the extremes of the training\\ndistribution in a principled way, our work draws from Extreme Value Theory\\n(EVT), a probabilistic approach for modelling the extreme tails of\\ndistributions. For practical utility, our framework allows the user to specify\\nboth the desired extremeness measure, as well as the desired extremeness\\nprobability they wish to sample at. Experiments on real US Precipitation data\\nshow that our method generates realistic samples, based on visual inspection\\nand quantitative measures, in an efficient manner.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'ExGAN: Adversarial Generation of Extreme Samples',\n",
       "  'text': 'Moreover, generating\\nincreasingly extreme examples using ExGAN can be done in constant time (with\\nrespect to the extremeness probability $\\\\tau$), as opposed to the\\n$\\\\mathcal{O}(\\\\frac{1}{\\\\tau})$ time required by the baseline approach.',\n",
       "  'meta': {'_split_id': 2}},\n",
       " {'name': 'The role of the Model Validation function to manage and mitigate model\\n  risk',\n",
       "  'text': 'This paper describes the current taxonomy of model risk, ways for its\\nmitigation and management and the importance of the model validation function\\nin collaboration with other departments to design and implement them.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Forecasting vegetation condition for drought early warning systems in\\n  pastoral communities in Kenya',\n",
       "  'text': 'Droughts are a recurring hazard in sub-Saharan Africa, that can wreak huge\\nsocioeconomic costs.Acting early based on alerts provided by early warning\\nsystems (EWS) can potentially provide substantial mitigation, reducing the\\nfinancial and human cost. However, existing EWS tend only to monitor current,\\nrather than forecast future, environmental and socioeconomic indicators of\\ndrought, and hence are not always sufficiently timely to be effective in\\npractice. Here we present a novel method for forecasting satellite-based\\nindicators of vegetation condition. Specifically, we focused on the 3-month\\nVegetation Condition Index (VCI3M) over pastoral livelihood zones in Kenya,\\nwhich is the indicator used by the Kenyan National Drought Management\\nAuthority(NDMA).',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Forecasting vegetation condition for drought early warning systems in\\n  pastoral communities in Kenya',\n",
       "  'text': 'Using data from MODIS and Landsat, we apply linear\\nautoregression and Gaussian process modeling methods and demonstrate high\\nforecasting skill several weeks ahead. As a benchmark we predicted the drought\\nalert marker used by NDMA (VCI3M<35). Both of our models were able to predict\\nthis alert marker four weeks ahead with a hit rate of around 89% and a false\\nalarm rate of around 4%, or 81% and 6% respectively six weeks ahead. The\\nmethods developed here can thus identify a deteriorating vegetation condition\\nwell and sufficiently in advance to help disaster risk managers act early to\\nsupport vulnerable communities and limit the impact of a drought hazard.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Twitter Speaks: A Case of National Disaster Situational Awareness',\n",
       "  'text': \"In recent years, we have been faced with a series of natural disasters\\ncausing a tremendous amount of financial, environmental, and human losses. The\\nunpredictable nature of natural disasters' behavior makes it hard to have a\\ncomprehensive situational awareness (SA) to support disaster management. Using\\nopinion surveys is a traditional approach to analyze public concerns during\\nnatural disasters; however, this approach is limited, expensive, and\\ntime-consuming. Luckily the advent of social media has provided scholars with\\nan alternative means of analyzing public concerns. Social media enable users\\n(people) to freely communicate their opinions and disperse information\\nregarding current events including natural disasters.\",\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Twitter Speaks: A Case of National Disaster Situational Awareness',\n",
       "  'text': 'This research emphasizes\\nthe value of social media analysis and proposes an analytical framework:\\nTwitter Situational Awareness (TwiSA). This framework uses text mining methods\\nincluding sentiment analysis and topic modeling to create a better SA for\\ndisaster preparedness, response, and recovery. TwiSA has also effectively\\ndeployed on a large number of tweets and tracks the negative concerns of people\\nduring the 2015 South Carolina flood.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Internet of Things (IoT) and Cloud Computing Enabled Disaster Management',\n",
       "  'text': 'Disaster management demands a near real-time information dissemina-tion so\\nthat the emergency services can be provided to the right people at the right\\ntime. Recent advances in information and communication technologies enable\\ncollection of real-time information from various sources. For example, sensors\\ndeployed in the fields collect data about the environment. Similarly, social\\nnetworks like Twitter and Facebook can help to collect data from people in the\\ndisaster zone. On one hand, inadequate situation awareness in disasters has\\nbeen identified as one of the primary factors in human errors with grave\\nconsequences such as loss of lives and destruction of critical infrastructure.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Internet of Things (IoT) and Cloud Computing Enabled Disaster Management',\n",
       "  'text': 'On the other hand, the growing ubiquity of social media and mobile devices, and\\npervasive nature of the Internet-of-Things means that there are more sources of\\noutbound traffic, which ultimately results in the creation of a data deluge,\\nbeginning shortly after the onset of disaster events, leading to the problem of\\ninformation tsunami. In addition, security and privacy has crucial role to\\novercome the misuse of the system for either intrusions into data or overcome\\nthe misuse of the information that was meant for a specified purpose. .... In\\nthis chapter, we provide such a situation aware application to support disaster\\nmanagement data lifecycle, i.e.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Internet of Things (IoT) and Cloud Computing Enabled Disaster Management',\n",
       "  'text': 'from data ingestion and processing to alert\\ndissemination. We utilize cloud computing, Internet of Things and social\\ncomputing technologies to achieve a scalable, effi-cient, and usable\\nsituation-aware application called Cloud4BigData.',\n",
       "  'meta': {'_split_id': 2}},\n",
       " {'name': 'BUDAMAF: Data Management in Cloud Federations',\n",
       "  'text': 'Data management has always been a multi-domain problem even in the simplest\\ncases. It involves, quality of service, security, resource management, cost\\nmanagement, incident identification, disaster avoidance and/or recovery, as\\nwell as many other concerns. In our case, this situation gets ever more\\ncomplicated because of the divergent nature of a cloud federation like BASMATI. In this federation, the BASMATI Unified Data Management Framework (BUDaMaF),\\ntries to create an automated uniform way of managing all the data transactions,\\nas well as the data stores themselves, in a polyglot multi-cloud, consisting of\\na plethora of different machines and data store systems.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Performance Improvements in Heterogeneous Wireless Networks for First\\n  Responders',\n",
       "  'text': 'Efficient communications are crucial for disaster response and recovery. However, most current public safety land mobile radio (LMR) networks only\\nprovide narrowband voice service with limited support of low-speed data\\nservices. In this paper, we study to enhance the interoperability of LMR with\\ncommercial wireless cellular networks, by which a wide variety of benefits can\\nbe offered to disaster responders, including new multimedia services, increased\\ndata rates and low cost devices. Our approach is based on Session Initial\\nProtocol (SIP) and a joint radio resource management framework.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Performance Improvements in Heterogeneous Wireless Networks for First\\n  Responders',\n",
       "  'text': 'In addition, an\\noptimal radio resource management scheme is proposed to maximize the overall\\nradio resource utilization and at the same time guarantee service availability\\nand continuity quality of service (QoS) for disaster responders. The\\neffectiveness of the proposed approach is illustrated by numerical examples.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Historical risk measures on stock market indices and energy markets',\n",
       "  'text': 'In this paper we look at the efficacy of different risk measures on energy\\nmarkets and across several different stock market indices. We use both the\\nValue at Risk and the Tail Conditional Expectation on each of these data sets. We also consider several different durations and levels for historical risk\\nmeasures. Through our results we make some recommendations for a robust risk\\nmanagement strategy that involves historical risk measures.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Myopic Policies for Non-Preemptive Scheduling of Jobs with Decaying\\n  Value',\n",
       "  'text': 'In many scheduling applications, minimizing delays is of high importance. One\\nadverse effect of such delays is that the reward for completion of a job may\\ndecay over time. Indeed in healthcare settings, delays in access to care can\\nresult in worse outcomes, such as an increase in mortality risk. Motivated by\\nmanaging hospital operations in disaster scenarios, as well as other\\napplications in perishable inventory control and information services, we\\nconsider non-preemptive scheduling of jobs whose internal value decays over\\ntime.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Myopic Policies for Non-Preemptive Scheduling of Jobs with Decaying\\n  Value',\n",
       "  'text': 'Because solving for the optimal scheduling policy is computationally\\nintractable, we focus our attention on the performance of three intuitive\\nheuristics: (1) a policy which maximizes the expected immediate reward, (2) a\\npolicy which maximizes the expected immediate reward rate, and (3) a policy\\nwhich prioritizes jobs with imminent deadlines. We provide performance\\nguarantees for all three policies and show that many of these performance\\nbounds are tight. In addition, we provide numerical experiments and simulations\\nto compare how the policies perform in a variety of scenarios.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Myopic Policies for Non-Preemptive Scheduling of Jobs with Decaying\\n  Value',\n",
       "  'text': 'Our theoretical\\nand numerical results allow us to establish rules-of-thumb for applying these\\nheuristics in a variety of situations, including patient scheduling scenarios.',\n",
       "  'meta': {'_split_id': 2}},\n",
       " {'name': 'A two-stage stochastic approach for the asset protection problem during\\n  escaped wildfires with uncertain timing of a wind change',\n",
       "  'text': \"Wildfires are natural disasters capable of damaging economies and\\ncommunities. When wildfires become uncontrollable, Incident Manager Teams\\n(IMT's) dispatch response vehicles to key assets to undertake protective tasks\\nand so mitigate the risk to these assets. In developing a deployment plan under\\nsevere time pressure, IMT's need to consider the special requirements of each\\nasset, the resources (vehicles and their teams), as well as uncertainties\\nassociated with the wildfire. A common situation that arises in southern\\nAustralian wildfires is a wind change. There is a reliable forecast of a wind\\nchange, but some uncertainty around the timing of that change.\",\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'A two-stage stochastic approach for the asset protection problem during\\n  escaped wildfires with uncertain timing of a wind change',\n",
       "  'text': \"To assist IMT's\\nto deal with this situation we develop a two-stage stochastic model to\\nintegrate such an uncertainty with the complexities of asset protection\\noperations. This is the first time a mathematical model is proposed which\\nconsiders uncertainty in the timing of a scenario change. The model is\\nimplemented for a case study that uses the context of the 2009 Black Saturday\\nbushfires in Victoria. A new set of benchmark instances is generated using\\nrealistic wildfire attributes to test the computational tractability of our\\nmodel and the results compared to a dynamic rerouting approach.\",\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'A two-stage stochastic approach for the asset protection problem during\\n  escaped wildfires with uncertain timing of a wind change',\n",
       "  'text': 'The\\ncomputations reveal that, compared with dynamic rerouting, the new model can\\ngenerate better deployment plans. The model can achieve solutions in\\noperational time for realistic-sized problems, although for larger problems the\\nsub-optimal rerouting algorithm would still need to be deployed.',\n",
       "  'meta': {'_split_id': 2}},\n",
       " {'name': 'Aerial Imagery Pile burn detection using Deep Learning: the FLAME\\n  dataset',\n",
       "  'text': 'Wildfires are one of the costliest and deadliest natural disasters in the US,\\ncausing damage to millions of hectares of forest resources and threatening the\\nlives of people and animals. Of particular importance are risks to firefighters\\nand operational forces, which highlights the need for leveraging technology to\\nminimize danger to people and property. FLAME (Fire Luminosity Airborne-based\\nMachine learning Evaluation) offers a dataset of aerial images of fires along\\nwith methods for fire detection and segmentation which can help firefighters\\nand researchers to develop optimal fire management strategies.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Aerial Imagery Pile burn detection using Deep Learning: the FLAME\\n  dataset',\n",
       "  'text': 'This paper\\nprovides a fire image dataset collected by drones during a prescribed burning\\npiled detritus in an Arizona pine forest. The dataset includes video recordings\\nand thermal heatmaps captured by infrared cameras. The captured videos and\\nimages are annotated and labeled frame-wise to help researchers easily apply\\ntheir fire detection and modeling algorithms. The paper also highlights\\nsolutions to two machine learning problems: (1) Binary classification of video\\nframes based on the presence [and absence] of fire flames. An Artificial Neural\\nNetwork (ANN) method is developed that achieved a 76% classification accuracy. (2) Fire detection using segmentation methods to precisely determine fire\\nborders.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Aerial Imagery Pile burn detection using Deep Learning: the FLAME\\n  dataset',\n",
       "  'text': 'A deep learning method is designed based on the U-Net up-sampling and\\ndown-sampling approach to extract a fire mask from the video frames. Our FLAME\\nmethod approached a precision of 92% and a recall of 84%. Future research will\\nexpand the technique for free burning broadcast fire using thermal images.',\n",
       "  'meta': {'_split_id': 2}},\n",
       " {'name': 'Design For Change: Information-Centric Architecture to Support Agile\\n  Disaster Response',\n",
       "  'text': 'This paper presents a case for the adoption of an information-centric\\narchitecture for a global disaster management system. Drawing from a case study\\nof the 2010/2011 Queensland floods, we describe the challenges in providing\\nevery participant with relevant and actionable information. We use various\\nexamples to argue for a more flexible information dissemination framework which\\nis designed from the ground up to minimise the effort needed to fix the\\nunexpected and unavoidable information acquisition, quality, and dissemination\\nchallenges posed by any real disaster.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Report for the Edinburgh Tram Inquiry',\n",
       "  'text': \"This report reviews the Edinburgh tram project's risk management. Projects\\nfrequently overrun their cost and timelines and fall short on intended\\nbenefits. Cost, schedule, and benefit risk of projects need to be carefully\\nconsidered to avoid this. The report describes and evaluates risk assessment\\nand management for the Edinburgh tram. The report was produced as part of the\\nEdinburgh Tram Inquiry. Keywords: risk assessment, risk management, infrastructure, megaprojects,\\noptimism bias, strategic misrepresentation, planning fallacy, behavioral\\nscience.\",\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Can SDN Mitigate Disasters?',\n",
       "  'text': 'Datacenter networks and services are at risk in the face of disasters. Existing fault-tolerant storage services cannot even achieve a nil recovery\\npoint objective (RPO) as client-generated data may get lost before the\\ntermination of their migration across geo-replicated datacenters. SDN has\\nproved instrumental in exploiting application-level information to optimise the\\nrouting of information. In this paper, we propose Software Defined Edge (SDE)\\nor the implementation of SDN at the network edge to achieve nil RPO. We\\nillustrate our proposal with a fault-tolerant key-value store that\\nexperimentally recovers from disaster within 30s.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Can SDN Mitigate Disasters?',\n",
       "  'text': 'Although SDE is inherently\\nfault-tolerant and scalable, its deployment raises new challenges on the\\npartnership between ISPs and CDN providers. We conclude that failure detection\\ninformation at the SDN-level can effectively benefit applications to recover\\nfrom disaster.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Spectral Risk Measures and the Choice of Risk Aversion Function',\n",
       "  'text': \"Spectral risk measures are attractive risk measures as they allow the user to\\nobtain risk measures that reflect their risk-aversion functions. To date there\\nhas been very little guidance on the choice of risk-aversion functions\\nunderlying spectral risk measures. This paper addresses this issue by examining\\ntwo popular risk aversion functions, based on exponential and power utility\\nfunctions respectively. We find that the former yields spectral risk measures\\nwith nice intuitive properties, but the latter yields spectral risk measures\\nthat can have perverse properties. More work therefore needs to be done before\\nwe can be sure that arbitrary but respectable utility functions will always\\nyield 'well-behaved' spectral risk measures.\",\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Attribution of Responsibility and Blame Regarding a Man-made Disaster:\\n  #FlintWaterCrisis',\n",
       "  'text': 'Attribution of responsibility and blame are important topics in political\\nscience especially as individuals tend to think of political issues in terms of\\nquestions of responsibility, and as blame carries far more weight in voting\\nbehavior than that of credit. However, surprisingly, there is a paucity of\\nstudies on the attribution of responsibility and blame in the field of disaster\\nresearch. The Flint water crisis is a story of government failure at all levels. By\\nstudying microblog posts about it, we understand how citizens assign\\nresponsibility and blame regarding such a man-made disaster online.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Attribution of Responsibility and Blame Regarding a Man-made Disaster:\\n  #FlintWaterCrisis',\n",
       "  'text': 'We form\\nhypotheses based on social scientific theories in disaster research and then\\noperationalize them on unobtrusive, observational social media data. In\\nparticular, we investigate the following phenomena: the source for blame; the\\npartisan predisposition; the concerned geographies; and the contagion of\\ncomplaining. This paper adds to the sociology of disasters research by exploiting a new,\\nrarely used data source (the social web), and by employing new computational\\nmethods (such as sentiment analysis and retrospective cohort study design) on\\nthis new form of data. In this regard, this work should be seen as the first\\nstep toward drawing more challenging inferences on the sociology of disasters\\nfrom \"big social data\".',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Uneven Coverage of Natural Disasters in Wikipedia: the Case of Flood',\n",
       "  'text': 'The usage of non-authoritative data for disaster management presents the\\nopportunity of accessing timely information that might not be available through\\nother means, as well as the challenge of dealing with several layers of biases. Wikipedia, a collaboratively-produced encyclopedia, includes in-depth\\ninformation about many natural and human-made disasters, and its editors are\\nparticularly good at adding information in real-time as a crisis unfolds. In\\nthis study, we focus on the English version of Wikipedia, that is by far the\\nmost comprehensive version of this encyclopedia. Wikipedia tends to have good\\ncoverage of disasters, particularly those having a large number of fatalities.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Uneven Coverage of Natural Disasters in Wikipedia: the Case of Flood',\n",
       "  'text': 'However, we also show that a tendency to cover events in wealthy countries and\\nnot cover events in poorer ones permeates Wikipedia as a source for\\ndisaster-related information. By performing careful automatic content analysis\\nat a large scale, we show how the coverage of floods in Wikipedia is skewed\\ntowards rich, English-speaking countries, in particular the US and Canada. We\\nalso note how coverage of floods in countries with the lowest income, as well\\nas countries in South America, is substantially lower than the coverage of\\nfloods in middle-income countries.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Uneven Coverage of Natural Disasters in Wikipedia: the Case of Flood',\n",
       "  'text': 'These results have implications for systems\\nusing Wikipedia or similar collaborative media platforms as an information\\nsource for detecting emergencies or for gathering valuable information for\\ndisaster response.',\n",
       "  'meta': {'_split_id': 2}},\n",
       " {'name': 'Risk averse fractional trading using the current drawdown',\n",
       "  'text': 'In this paper the fractional trading ansatz of money management is\\nreconsidered with special attention to chance and risk parts in the goal\\nfunction of the related optimization problem. By changing the goal function\\nwith due regards to other risk measures like current drawdowns, the optimal\\nfraction solutions reflect the needs of risk averse investors better than the\\noriginal optimal f solution of Ralph Vince. Keywords: fractional trading, optimal f, current drawdown, terminal wealth\\nrelative, risk aversion',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Data Challenges in High-Performance Risk Analytics',\n",
       "  'text': 'Risk Analytics is important to quantify, manage and analyse risks from the\\nmanufacturing to the financial setting. In this paper, the data challenges in\\nthe three stages of the high-performance risk analytics pipeline, namely risk\\nmodelling, portfolio risk management and dynamic financial analysis is\\npresented.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Value-at-Risk and Expected Shortfall for the major digital currencies',\n",
       "  'text': 'Digital currencies and cryptocurrencies have hesitantly started to penetrate\\nthe investors, and the next step will be the regulatory risk management\\nframework. We examine the Value-at-Risk and Expected Shortfall properties for\\nthe major digital currencies, Bitcoin, Ethereum, Litecoin, and Ripple. The\\nmethodology used is GARCH modelling followed by Filtered Historical Simulation. We find that digital currencies are subject to a higher risk, therefore, to\\nhigher sufficient buffer and risk capital to cover potential losses.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Complex Valued Risk Diversification',\n",
       "  'text': 'Risk diversification is one of the dominant concerns for portfolio managers. Various portfolio constructions have been proposed to minimize the risk of the\\nportfolio under some constrains including expected returns. We propose a\\nportfolio construction method that incorporates the complex valued principal\\ncomponent analysis into the risk diversification portfolio construction. The\\nproposed method is verified to outperform the conventional risk parity and risk\\ndiversification portfolio constructions.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Towards an Understanding of the Effects of Augmented Reality Games on\\n  Disaster Management',\n",
       "  'text': \"Location-based augmented reality games have entered the mainstream with the\\nnearly overnight success of Niantic's Pok\\\\'emon Go. Unlike traditional video\\ngames, the fact that players of such games carry out actions in the external,\\nphysical world to accomplish in-game objectives means that the large-scale\\nadoption of such games motivate people, en masse, to do things and go places\\nthey would not have otherwise done in unprecedented ways. The social\\nimplications of such mass-mobilisation of individual players are, in general,\\ndifficult to anticipate or characterise, even for the short-term.\",\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Towards an Understanding of the Effects of Augmented Reality Games on\\n  Disaster Management',\n",
       "  'text': \"In this work,\\nwe focus on disaster relief, and the short- and long-term implications that a\\nproliferation of AR games like Pok\\\\'emon Go, may have in disaster-prone regions\\nof the world. We take a distributed cognition approach and focus on one natural\\ndisaster-prone region of New Zealand, the city of Wellington.\",\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Applications of Online Deep Learning for Crisis Response Using Social\\n  Media Information',\n",
       "  'text': 'During natural or man-made disasters, humanitarian response organizations\\nlook for useful information to support their decision-making processes. Social\\nmedia platforms such as Twitter have been considered as a vital source of\\nuseful information for disaster response and management. Despite advances in\\nnatural language processing techniques, processing short and informal Twitter\\nmessages is a challenging task. In this paper, we propose to use Deep Neural\\nNetwork (DNN) to address two types of information needs of response\\norganizations: 1) identifying informative tweets and 2) classifying them into\\ntopical classes. DNNs use distributed representation of words and learn the\\nrepresentation as well as higher level features automatically for the\\nclassification task.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Applications of Online Deep Learning for Crisis Response Using Social\\n  Media Information',\n",
       "  'text': 'We propose a new online algorithm based on stochastic\\ngradient descent to train DNNs in an online fashion during disaster situations. We test our models using a crisis-related real-world Twitter dataset.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Cross-referencing Social Media and Public Surveillance Camera Data for\\n  Disaster Response',\n",
       "  'text': 'Physical media (like surveillance cameras) and social media (like Instagram\\nand Twitter) may both be useful in attaining on-the-ground information during\\nan emergency or disaster situation. However, the intersection and reliability\\nof both surveillance cameras and social media during a natural disaster are not\\nfully understood. To address this gap, we tested whether social media is of\\nutility when physical surveillance cameras went off-line during Hurricane Irma\\nin 2017. Specifically, we collected and compared geo-tagged Instagram and\\nTwitter posts in the state of Florida during times and in areas where public\\nsurveillance cameras went off-line.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Cross-referencing Social Media and Public Surveillance Camera Data for\\n  Disaster Response',\n",
       "  'text': 'We report social media content and\\nfrequency and content to determine the utility for emergency managers or first\\nresponders during a natural disaster.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Cost Overruns and Demand Shortfalls in Urban Rail and Other\\n  Infrastructure',\n",
       "  'text': 'Risk, including economic risk, is increasingly a concern for public policy\\nand management. The possibility of dealing effectively with risk is hampered,\\nhowever, by lack of a sound empirical basis for risk assessment and management. The paper demonstrates the general point for cost and demand risks in urban\\nrail projects. The paper presents empirical evidence that allow valid economic\\nrisk assessment and management of urban rail projects, including benchmarking\\nof individual or groups of projects. Benchmarking of the Copenhagen Metro is\\npresented as a case in point.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Cost Overruns and Demand Shortfalls in Urban Rail and Other\\n  Infrastructure',\n",
       "  'text': 'The approach developed is proposed as a model for\\nother types of policies and projects in order to improve economic and financial\\nrisk assessment and management in policy and planning.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Assignment of Freight Traffic in a Large-Scale Intermodal Network under\\n  Uncertainty',\n",
       "  'text': 'This paper presents a methodology for freight traffic assignment in a\\nlarge-scale road-rail intermodal network under uncertainty. Network\\nuncertainties caused by natural disasters have dramatically increased in recent\\nyears. Several of these disasters (e.g., Hurricane Sandy, Mississippi River\\nFlooding, Hurricane Harvey) severely disrupted the U.S. freight transport\\nnetwork, and consequently, the supply chain. To account for these network\\nuncertainties, a stochastic freight traffic assignment model is formulated. An\\nalgorithmic framework, involving the sample average approximation and gradient\\nprojection algorithm, is proposed to solve this challenging problem. The\\ndeveloped methodology is tested on the U.S. intermodal network with freight\\nflow data from the Freight Analysis Framework.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Assignment of Freight Traffic in a Large-Scale Intermodal Network under\\n  Uncertainty',\n",
       "  'text': 'The experiments consider four\\ntypes of natural disasters that have different risks and impacts on the\\ntransportation network: earthquake, hurricane, tornado, and flood. The results\\ndemonstrate the feasibility of the model and algorithmic framework to obtain\\nfreight flows for a realistic-sized network in reasonable time (between 417 and\\n716 minutes). It is found that for all disaster scenarios the freight ton-miles\\nare higher compared to the base case without uncertainty. The increase in\\nfreight ton-miles is the highest under the flooding scenario; this is due to\\nthe fact that there are more states in the flood-risk areas and they are\\nscattered throughout the U.S.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': \"Reducing individuals' risk sensitiveness can promote positive and\\n  non-alarmist views about catastrophic events in an agent-based simulation\",\n",
       "  'text': 'We present a cognitive model of opinion dynamics which studies the behavior\\nof a population of interacting individuals in the context of risk of natural\\ndisaster. In particular, we investigate the response of the individuals to the\\ninformation received by institutional sources about the correct behaviors for\\nprevention and harm reduction. The results of our study show that alarmist\\nopinions are more likely to be adopted by populations, since worried people',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Towards knowledge sharing in disaster management: An agent oriented\\n  knowledge analysis framework',\n",
       "  'text': 'Disaster Management (DM) is a complex set of interrelated activities. The\\nactivities are often knowledge intensive and time sensitive. Sharing the\\nrequired knowledge timely is critical for DM. In developed countries, for\\nrecurring disasters (e.g. floods), there are dedicated document repositories of\\nDisaster Management Plans (DMP) that can be accessed as needs arise. However,\\naccessing the appropriate plan in a timely manner and sharing activities\\nbetween plans often requires domain knowledge and intimate knowledge of the\\nplans in the first place. In this paper, we introduce an agent-based knowledge\\nanalysis method to convert DMPs into a collection of knowledge units that can\\nbe stored into a unified repository.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Towards knowledge sharing in disaster management: An agent oriented\\n  knowledge analysis framework',\n",
       "  'text': 'The repository of DM actions then enables\\nthe mixing and matching knowledge between different plans. The repository is\\nstructured as a layered abstraction according to Meta Object Facility (MOF). We\\nuse the flood management plans used by SES in NSW to illustrate and give a\\npreliminary validation of the approach. It is illustrated using DMPs along the\\nflood prone Murrumbidgee River in central NSW.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Assessing Interaction Networks with Applications to Catastrophe Dynamics\\n  and Disaster Management',\n",
       "  'text': 'In this paper we present a versatile method for the investigation of\\ninteraction networks and show how to use it to assess effects of indirect\\ninteractions and feedback loops. The method allows to evaluate the impact of\\noptimization measures or failures on the system. Here, we will apply it to the\\ninvestigation of catastrophes, in particular to the temporal development of\\ndisasters (catastrophe dynamics). The mathematical methods are related to the\\nmaster equation, which allows the application of well-known solution methods. We will also indicate connections of disaster management with excitable media\\nand supply networks.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Assessing Interaction Networks with Applications to Catastrophe Dynamics\\n  and Disaster Management',\n",
       "  'text': 'This facilitates to study the effects of measures taken by\\nthe emergency management or the local operation units. With a fictious, but\\nmore or less realistic example of a spreading epidemic disease or a wave of\\ninfluenza, we illustrate how this method can, in principle, provide decision\\nsupport to the emergency management during such a disaster. Similar\\nconsiderations may help to assess measures to fight the SARS epidemics,\\nalthough immunization is presently not possible.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Risk Management with Tail Quasi-Linear Means',\n",
       "  'text': 'We generalize Quasi-Linear Means by restricting to the tail of the risk\\ndistribution and show that this can be a useful quantity in risk management\\nsince it comprises in its general form the Value at Risk, the Tail Value at\\nRisk and the Entropic Risk Measure in a unified way. We then investigate the\\nfundamental properties of the proposed measure and show its unique features and\\nimplications in the risk measurement process. Furthermore, we derive formulas\\nfor truncated elliptical models of losses and provide formulas for selected\\nmembers of such models.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Project Risk Management Model Based on PRINCE2 and Scrum Frameworks',\n",
       "  'text': 'There is a lack of formal risk management techniques in agile software\\ndevelopment methods Scrum. The need to manage risks in agile project management\\nis also identified by various authors. Authors of this paper conducted a survey\\nto find out the current practices in agile project management. Furthermore\\nauthors discuss the new integrated framework of Scrum and PRINCE2 with focus on\\nrisk management. Enrichment of Scrum with selected practices from the\\nheavy-weight project management framework PRINCE2 promises better results in\\ndelivering software products especially in global development projects.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'A shortcut to sign Incremental Value-at-Risk for risk allocation',\n",
       "  'text': 'Approximate Incremental Value-at-Risk formulae provide an easy-to-use\\npreliminary guideline for risk allocation. Both the cases of risk adding and\\nrisk pooling are examined and beta-based formulae achieved. Results highlight\\nhow much the conditions for adding new risky positions are stronger than those\\nrequired for risk pooling. Key words: Incremental Value-at-Risk (IVaR); Risk pooling; Risk adding.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Transitioning from Blackboard to Moodle amidst Natural Disaster: Faculty\\n  and Students Perceptions',\n",
       "  'text': 'Higher educational institutions continuously look for ways to improve the\\nquality of their eLearning services and adapt learning solutions to suit the\\nneeds of the institution. During the 2016 Fall Semester, a university located\\nin the Southern part of United States decided to transition from the Blackboard\\nlearning management system (LMS) to the Moodle learning management system. Typically such a transition presents a huge challenge for the University staff,\\nfaculty, and students. Additionally, on August 2016, what CNN themedthe worst\\nnatural disaster, to strike the United States since Hurricane Sandy, occurred\\nin Louisiana during the transition. This led to massive disruptions in\\nactivities throughout the state.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Transitioning from Blackboard to Moodle amidst Natural Disaster: Faculty\\n  and Students Perceptions',\n",
       "  'text': 'This paper examines the perceptions of both\\nfaculty and student on the transition from one LMS to another and also what\\nimpact, if any, the natural disaster had on the process. Faculty and students\\nwere surveyed to gain understanding of how they perceived the transitioning\\nprocess, their perception of both systems, their preferences, and why. Furthermore, we identified issues peculiar to transitioning during a natural\\ndisaster. The results of this study can be used to anticipate issues that may\\nbe associated with transitioning from one LMS to the other and issues peculiar\\nto transitioning amidst a natural disaster. It can also be used to identify\\nareas for improvement.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Risk Measures in Quantitative Finance',\n",
       "  'text': \"This paper was presented and written for two seminars: a national UK\\nUniversity Risk Conference and a Risk Management industry workshop. The target\\naudience is therefore a cross section of Academics and industry professionals. The current ongoing global credit crunch has highlighted the importance of\\nrisk measurement in Finance to companies and regulators alike. Despite risk\\nmeasurement's central importance to risk management, few papers exist reviewing\\nthem or following their evolution from its foremost beginnings up to the\\npresent day risk measures.\",\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Risk Measures in Quantitative Finance',\n",
       "  'text': \"This paper reviews the most important portfolio risk measures in Financial\\nMathematics, from Bernoulli (1738) to Markowitz's Portfolio Theory, to the\\npresently preferred risk measures such as CVaR (conditional Value at Risk). We\\nprovide a chronological review of the risk measures and survey less commonly\\nknown risk measures e.g. Treynor ratio.\",\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Fair Influence Maximization: A Welfare Optimization Approach',\n",
       "  'text': 'Several behavioral, social, and public health interventions, such as\\nsuicide/HIV prevention or community preparedness against natural disasters,\\nleverage social network information to maximize outreach. Algorithmic influence\\nmaximization techniques have been proposed to aid with the choice of \"peer\\nleaders\" or \"influencers\" in such interventions. Yet, traditional algorithms\\nfor influence maximization have not been designed with these interventions in\\nmind. As a result, they may disproportionately exclude minority communities\\nfrom the benefits of the intervention. This has motivated research on fair\\ninfluence maximization. Existing techniques come with two major drawbacks. First, they require committing to a single fairness measure.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Fair Influence Maximization: A Welfare Optimization Approach',\n",
       "  'text': 'Second, these\\nmeasures are typically imposed as strict constraints leading to undesirable\\nproperties such as wastage of resources. To address these shortcomings, we provide a principled characterization of\\nthe properties that a fair influence maximization algorithm should satisfy. In\\nparticular, we propose a framework based on social welfare theory, wherein the\\ncardinal utilities derived by each community are aggregated using the\\nisoelastic social welfare functions. Under this framework, the trade-off\\nbetween fairness and efficiency can be controlled by a single inequality\\naversion design parameter. We then show under what circumstances our proposed\\nprinciples can be satisfied by a welfare function.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Fair Influence Maximization: A Welfare Optimization Approach',\n",
       "  'text': 'The resulting optimization\\nproblem is monotone and submodular and can be solved efficiently with\\noptimality guarantees. Our framework encompasses as special cases leximin and\\nproportional fairness. Extensive experiments on synthetic and real world\\ndatasets including a case study on landslide risk management demonstrate the\\nefficacy of the proposed framework.',\n",
       "  'meta': {'_split_id': 2}},\n",
       " {'name': 'Solving multi-resource allocation and location problems in disaster\\n  management through linear programming',\n",
       "  'text': 'In this paper we propose a new efficient linear programming based approach\\nfor multi-resource allocation and location problems in disaster management. Such problems require an integer solution and therefore, in most cases, the\\ncomputations rely on integer and mixed-integer linear programming solvers. In\\ngeneral, these solvers can not handle large scaled problem. In this paper we\\ndemonstrate that there exists a large class of disaster management problems\\nwhose exact solutions can be obtained by applying the simplex method (linear\\nprogramming). The results of numerical experiments are provided. Another\\nimportant contribution of this paper is related to general cluster analysis and\\nallocation.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Solving multi-resource allocation and location problems in disaster\\n  management through linear programming',\n",
       "  'text': 'Namely, we demonstrate that the classical $k$-medoid clustering\\nmethod can be implemented using linear programming techniques (simplex method)\\nwithout relying on integer solvers.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Viewing Risk Measures as Information',\n",
       "  'text': \"Regulation and risk management in banks depend on underlying risk measures. In general this is the only purpose that is seen for risk measures. In this\\npaper we suggest that the reporting of risk measures can be used to determine\\nthe loss distribution function for a financial entity. We demonstrate that a\\nlack of sufficient information can lead to ambiguous risk situations. We give\\nexamples, showing the need for the reporting of multiple risk measures in order\\nto determine a bank's loss distribution. We conclude by suggesting a regulatory\\nrequirement of multiple risk measures being reported by banks, giving specific\\nrecommendations.\",\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Scenario-based Risk Evaluation',\n",
       "  'text': 'Risk measures such as Expected Shortfall (ES) and Value-at-Risk (VaR) have\\nbeen prominent in banking regulation and financial risk management. Motivated\\nby practical considerations in the assessment and management of risks,\\nincluding tractability, scenario relevance and robustness, we consider\\ntheoretical properties of scenario-based risk evaluation. We propose several\\nnovel scenario-based risk measures, including various versions of Max-ES and\\nMax-VaR, and study their properties. We establish axiomatic characterizations\\nof scenario-based risk measures that are comonotonic-additive or coherent and\\nan ES-based representation result is obtained. These results provide a\\ntheoretical foundation for the recent Basel III & IV market risk calculation\\nformulas. We illustrate the theory with financial data examples.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Complex risk statistics with scenario analysis',\n",
       "  'text': 'Complex risk is a critical factor for both intelligent systems and risk\\nmanagement. In this paper, we consider a special class of risk statistics,\\nnamed complex risk statistics. Our result provides a new approach for\\naddressing complex risk, especially in deep neural networks. By further\\ndeveloping the properties related to complex risk statistics, we are able to\\nderive dual representation for such risk.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Reduced form models of bond portfolios',\n",
       "  'text': 'We derive simple return models for several classes of bond portfolios. With\\nonly one or two risk factors our models are able to explain most of the return\\nvariations in portfolios of fixed rate government bonds, inflation linked\\ngovernment bonds and investment grade corporate bonds. The underlying risk\\nfactors have natural interpretations which make the models well suited for risk\\nmanagement and portfolio design.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Law-invariant risk measures: extension properties and qualitative\\n  robustness',\n",
       "  'text': 'We characterize when a convex risk measure associated to a law-invariant\\nacceptance set in $L^\\\\infty$ can be extended to $L^p$, $1\\\\leq p<\\\\infty$,\\npreserving finiteness and continuity. This problem is strongly connected to the\\nstatistical robustness of the corresponding risk measures. Special attention is\\npaid to concrete examples including risk measures based on expected utility,\\nmax-correlation risk measures, and distortion risk measures.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Empirical Insights for Designing Information and Communication\\n  Technology for International Disaster Response',\n",
       "  'text': 'Due to the increase in natural disasters in the past years, Disaster Response\\nOrganizations (DROs) are faced with the challenge of coping with more and\\nlarger operations. Currently appointed Information and Communications\\nTechnology (ICT) used for coordination and communication is sometimes outdated\\nand does not scale, while novel technologies have the potential to greatly\\nimprove disaster response efficiency. To allow adoption of these novel\\ntechnologies, ICT system designers have to take into account the particular\\nneeds of DROs and characteristics of International Disaster Response (IDR). This work attempts to bring the humanitarian and ICT communities closer\\ntogether. In this work, we analyze IDR-related documents and conduct expert\\ninterviews.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Empirical Insights for Designing Information and Communication\\n  Technology for International Disaster Response',\n",
       "  'text': 'Using open coding, we extract empirical insights and translate the\\npeculiarities of DRO coordination and operation into tangible ICT design\\nrequirements. This information is based on interviews with active IDR staff as\\nwell as DRO guidelines and reports. Ultimately, the goal of this paper is to\\nserve as a reference for future ICT research endeavors to support and increase\\nthe efficiency of IDR operations.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Managing Derivative Exposure',\n",
       "  'text': 'We present an approach to derivative exposure management based on subjective\\nand implied probabilities. We suggest to maximize the valuation difference\\nsubject to risk constraints and propose a class of risk measures derived from\\nthe subjective distribution. We illustrate this process with specific examples\\nfor the two and three dimensional case. In these cases the optimization can be\\nperformed graphically.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'NFV and SDN - based Distributed IoT Gateway for Large-Scale Disaster\\n  Management',\n",
       "  'text': 'Large-scale disaster management applications are among the several realistic\\napplications of the IoT. Fire detection and earthquake early warning\\napplications are just two examples. Several IoT devices are used in such\\napplications e.g., sensors and robots. These sensors and robots are usually\\nheterogeneous. Moreover, in disaster scenarios, the existing communication\\ninfrastructure may become completely or partially destroyed, leaving mobile\\nad-hoc networks the only alternative to provide connectivity. Utilizing these\\napplications raises new challenges such as the need for dynamic, flexible, and\\ndistributed gateways which can accommodate new applications and new IoT\\ndevices.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'NFV and SDN - based Distributed IoT Gateway for Large-Scale Disaster\\n  Management',\n",
       "  'text': 'Network Functions Virtualization (NFV) and Software Defined Networking\\n(SDN) are emerging paradigms that can help to overcome these challenges. This\\npaper leverages NFV and SDN to propose an architecture for on-the-fly\\ndistributed gateway provisioning in large-scale disaster management. In the\\nproposed architecture, the gateway functions are provisioned as Virtual Network\\nFunctions (VNFs) that are chained on-the-fly in the IoT domain using SDN. A\\nprototype is built and the performance results are presented.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Risk-Consistent Conditional Systemic Risk Measures',\n",
       "  'text': 'We axiomatically introduce risk-consistent conditional systemic risk measures\\ndefined on multidimensional risks. This class consists of those conditional\\nsystemic risk measures which can be decomposed into a state-wise conditional\\naggregation and a univariate conditional risk measure. Our studies extend known\\nresults for unconditional risk measures on finite state spaces. We argue in\\nfavor of a conditional framework on general probability spaces for assessing\\nsystemic risk. Mathematically, the problem reduces to selecting a realization\\nof a random field with suitable properties. Moreover, our approach covers many\\nprominent examples of systemic risk measures from the literature and used in\\npractice.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'SlideVaR: a risk measure with variable risk attitudes',\n",
       "  'text': \"To find a trade-off between profitability and prudence, financial\\npractitioners need to choose appropriate risk measures. Two key points are:\\nFirstly, investors' risk attitudes under uncertainty conditions should be an\\nimportant reference for risk measures. Secondly, risk attitudes are not\\nabsolute. For different market performance, investors have different risk\\nattitudes. We proposed a new risk measure named SlideVaR which sufficiently\\nreflects the different subjective attitudes of investors and the impact of\\nmarket changes on investors' attitudes. We proposed the concept of risk-tail\\nregion and risk-tail sub-additivity and proved that SlideVaR satisfies several\\nimportant mathematical properties. Moreover, SlideVaR has a simple and\\nintuitive form of expression for practical application.\",\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'SlideVaR: a risk measure with variable risk attitudes',\n",
       "  'text': 'Several simulate and\\nempirical computations show that SlideVaR has obvious advantages in markets\\nwhere the state changes frequently.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Maintaining both availability and integrity of communications:\\n  Challenges and guidelines for data security and privacy during disasters and\\n  crises',\n",
       "  'text': 'Communications play a vital role in the response to disasters and crises. However, existing communications infrastructure is often impaired, destroyed or\\noverwhelmed during such events. This leads to the use of substitute\\ncommunications solutions including analog two-way radio or unsecured internet\\naccess. Often provided by unknown third parties, these solutions may have less\\nsophisticated security characteristics than is desirable. While substitute\\ncommunications are often invaluable, care is required to minimize the risk to\\nNGOs and individuals stemming from the use of communications channels with\\nreduced or unknown security properties. This is particularly true if private\\ninformation is involved, including the location and disposition of individuals\\nand first responders.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Maintaining both availability and integrity of communications:\\n  Challenges and guidelines for data security and privacy during disasters and\\n  crises',\n",
       "  'text': 'In this work we enumerate the principal risks and\\nchallenges that may arise, and provide practical guidelines for mitigating them\\nduring crises. We take plausible threats from contemporary disaster and crisis\\nevents into account and discuss the security and privacy features of\\nstate-of-the-art communications mechanisms.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Prototyping Low-Cost Automatic Weather Stations for Natural Disaster\\n  Monitoring',\n",
       "  'text': 'Weather events put human lives at risk mostly when people might reside in\\nareas susceptible to natural disasters. Weather monitoring is a pivotal task\\nthat is accomplished in vulnerable areas with the support of reliable weather\\nstations. Such stations are front-end equipment typically mounted on a fixed\\nmast structure with a set of digital and magnetic weather sensors connected to\\na datalogger. While remote sensing from a number of stations is paramount, the\\ncost of professional weather instruments is extremely high. This imposes a\\nchallenge for large-scale deployment and maintenance of weather stations for\\nbroad natural disaster monitoring.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Prototyping Low-Cost Automatic Weather Stations for Natural Disaster\\n  Monitoring',\n",
       "  'text': \"To address this problem, in this paper, we\\nvalidate the hypothesis that a Low-Cost Automatic Weather Station system\\n(LCAWS) entirely developed from commercial-off-the-shelf and open-source IoT\\ntechnologies is able to provide data as reliable as a Professional Weather\\nStation (PWS) of reference for natural disaster monitoring. To achieve data\\nreliability, we propose an intelligent sensor calibration method to correct\\nweather parameters. From the experimental results of a 30-day uninterrupted\\nobservation period, we show that the results of the calibrated LCAWS sensors\\nhave no statistically significant differences with the PWS's results.\",\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Prototyping Low-Cost Automatic Weather Stations for Natural Disaster\\n  Monitoring',\n",
       "  'text': 'Together\\nwith The Brazilian National Center for Monitoring and Early Warning of Natural\\nDisasters (Cemaden), LCAWS has opened new opportunities towards reducing\\nmaintenance cost of its weather observational network.',\n",
       "  'meta': {'_split_id': 2}},\n",
       " {'name': 'An ontology-based chatbot for crises management: use case coronavirus',\n",
       "  'text': 'Today is the era of intelligence in machines. With the advances in Artificial\\nIntelligence, machines have started to impersonate different human traits, a\\nchatbot is the next big thing in the domain of conversational services. A\\nchatbot is a virtual person who is capable to carry out a natural conversation\\nwith people. They can include skills that enable them to converse with the\\nhumans in audio, visual, or textual formats. Artificial intelligence\\nconversational entities, also called chatbots, conversational agents, or\\ndialogue system, are an excellent example of such machines. Obtaining the right\\ninformation at the right time and place is the key to effective disaster\\nmanagement.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'An ontology-based chatbot for crises management: use case coronavirus',\n",
       "  'text': 'The term \"disaster management\" encompasses both natural and\\nhuman-caused disasters. To assist citizens, our project is to create a COVID\\nAssistant to provide the need of up to date information to be available 24\\nhours. With the growth in the World Wide Web, it is quite intelligible that\\nusers are interested in the swift and relatedly correct information for their\\nhunt. A chatbot can be seen as a question-and-answer system in which experts\\nprovide knowledge to solicit users. This master thesis is dedicated to discuss\\nCOVID Assistant chatbot and explain each component in detail.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'An ontology-based chatbot for crises management: use case coronavirus',\n",
       "  'text': 'The design of the\\nproposed chatbot is introduced by its seven components: Ontology, Web Scraping\\nmodule, DB, State Machine, keyword Extractor, Trained chatbot, and User\\nInterface.',\n",
       "  'meta': {'_split_id': 2}},\n",
       " {'name': 'Real or Fake? User Behavior and Attitudes Related to Determining the\\n  Veracity of Social Media Posts',\n",
       "  'text': \"Citizens and Emergency Managers need to be able to distinguish ''fake''\\n(untrue) news posts from real news posts on social media during disasters. This\\npaper is based on an online survey conducted in 2018 that produced 341\\nresponses from invitations distributed via email and through Facebook. It\\nexplores to what extent and how citizens generally assess whether postings are\\n''true'' or ''fake,'' and describes indicators of the trustworthiness of\\ncontent that users would like. The mean response on a semantic differential\\nscale measuring how frequently users attempt to verify the news trustworthiness\\n(a scale from 1-never to 5-always) was 3.37.\",\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Real or Fake? User Behavior and Attitudes Related to Determining the\\n  Veracity of Social Media Posts',\n",
       "  'text': \"The most frequent message\\ncharacteristics citizens' use are grammar and the trustworthiness of the\\nsender. Most respondents would find an indicator of trustworthiness helpful,\\nwith the most popular choice being a colored graphic. Limitations and\\nimplications for assessments of trustworthiness during disasters are discussed.\",\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Weakly-supervised Fine-grained Event Recognition on Social Media Texts\\n  for Disaster Management',\n",
       "  'text': 'People increasingly use social media to report emergencies, seek help or\\nshare information during disasters, which makes social networks an important\\ntool for disaster management. To meet these time-critical needs, we present a\\nweakly supervised approach for rapidly building high-quality classifiers that\\nlabel each individual Twitter message with fine-grained event categories. Most\\nimportantly, we propose a novel method to create high-quality labeled data in a\\ntimely manner that automatically clusters tweets containing an event keyword\\nand asks a domain expert to disambiguate event word senses and label clusters\\nquickly.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Weakly-supervised Fine-grained Event Recognition on Social Media Texts\\n  for Disaster Management',\n",
       "  'text': 'In addition, to process extremely noisy and often rather short\\nuser-generated messages, we enrich tweet representations using preceding\\ncontext tweets and reply tweets in building event recognition classifiers. The\\nevaluation on two hurricanes, Harvey and Florence, shows that using only 1-2\\nperson-hours of human supervision, the rapidly trained weakly supervised\\nclassifiers outperform supervised classifiers trained using more than ten\\nthousand annotated tweets created in over 50 person-hours.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Physics-informed GANs for Coastal Flood Visualization',\n",
       "  'text': 'As climate change increases the intensity of natural disasters, society needs\\nbetter tools for adaptation. Floods, for example, are the most frequent natural\\ndisaster, but during hurricanes the area is largely covered by clouds and\\nemergency managers must rely on nonintuitive flood visualizations for mission\\nplanning. To assist these emergency managers, we have created a deep learning\\npipeline that generates visual satellite images of current and future coastal\\nflooding. We advanced a state-of-the-art GAN called pix2pixHD, such that it\\nproduces imagery that is physically-consistent with the output of an\\nexpert-validated storm surge model (NOAA SLOSH).',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Physics-informed GANs for Coastal Flood Visualization',\n",
       "  'text': 'By evaluating the imagery\\nrelative to physics-based flood maps, we find that our proposed framework\\noutperforms baseline models in both physical-consistency and photorealism. While this work focused on the visualization of coastal floods, we envision the\\ncreation of a global visualization of how climate change will shape our earth.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Risk Management Practices in Information Security: Exploring the Status\\n  Quo in the DACH Region',\n",
       "  'text': 'Information security management aims at ensuring proper protection of\\ninformation values and information processing systems (i.e. assets). Information security risk management techniques are incorporated to deal with\\nthreats and vulnerabilities that impose risks to information security\\nproperties of these assets. This paper investigates the current state of risk\\nmanagement practices being used in information security management in the DACH\\nregion (Germany, Austria, Switzerland). We used an anonymous online survey\\ntargeting strategic and operative information security and risk managers and\\ncollected data from 26 organizations.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Risk Management Practices in Information Security: Exploring the Status\\n  Quo in the DACH Region',\n",
       "  'text': 'We analyzed general practices,\\ndocumentation artifacts, patterns of stakeholder collaboration as well as tool\\ntypes and data sources used by enterprises to conduct information security\\nmanagement activities. Our findings show that the state of practice of\\ninformation security risk management is in need of improvement. Current\\nindustrial practice heavily relies on manual data collection and complex\\npotentially subjective decision processes with multiple stakeholders involved. Dedicated risk management tools and methods are used selectively and neglected\\nin favor of general-purpose documentation tools and direct communication\\nbetween stakeholders.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Risk Management Practices in Information Security: Exploring the Status\\n  Quo in the DACH Region',\n",
       "  'text': 'In light of our results we propose guidelines for the\\ndevelopment of risk management practices that are better aligned with the\\ncurrent operational situation in information security management.',\n",
       "  'meta': {'_split_id': 2}},\n",
       " {'name': 'Estimating financial risk measures for futures positions: a\\n  non-parametric approach',\n",
       "  'text': 'This paper presents non-parametric estimates of spectral risk measures\\napplied to long and short positions in 5 prominent equity futures contracts. It\\nalso compares these to estimates of two popular alternative measures, the\\nValue-at-Risk (VaR) and Expected Shortfall (ES). The spectral risk measures are\\nconditioned on the coefficient of absolute risk aversion, and the latter two\\nare conditioned on the confidence level. Our findings indicate that all risk\\nmeasures increase dramatically and their estimators deteriorate in precision\\nwhen their respective conditioning parameter increases. Results also suggest\\nthat estimates of spectral risk measures and their precision levels are of\\ncomparable orders of magnitude as those of more conventional risk measures.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Estimating financial risk measures for futures positions: a\\n  non-parametric approach',\n",
       "  'text': 'Running head: financial risk measures for futures positions',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Modeling systemic risks in financial markets',\n",
       "  'text': 'We survey systemic risks to financial markets and present a high-level\\ndescription of an algorithm that measures systemic risk in terms of coupled\\nnetworks.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Remark on the Paper \"Entropic Value-at-Risk: A New Coherent Risk\\n  Measure\" by Amir Ahmadi-Javid, J. Opt. Theory and Appl., 155\\n  (2001),1105--1123',\n",
       "  'text': 'The paper mentioned in the title introduces the entropic value at risk. I\\ngive some extra comments and using the general theory make a relation with some\\ncommonotone risk measures.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Multi-synchronous collaboration between desktop and mobile users: A case\\n  study of report writing for emergency management',\n",
       "  'text': 'The development of multi-synchronous decision support systems to facilitate\\ncollaboration between diverse users is an emerging field in emergency\\nmanagement. Traditionally, information management for emergency response has\\nbeen a centralised effort. However, modern devices such as smartphones provide\\nnew methods for gaining real-time information about a disaster from users in\\nthe field. In this paper, we present a framework for multi-synchronous\\ncollaborative report writing in the scope of emergency management. This\\nframework supports desktop-based users as information providers and consumers,\\nalongside mobile users as information providers to facilitate multi-synchronous\\ncollaboration. We consider the benefits of our framework for writing\\ncollaborative Situation Reports and discuss future directions for research.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Modelo de maturidade em gerenciamento de riscos em projetos (Project\\n  Risk Management Model Maturity)',\n",
       "  'text': \"The globalization feeded by the technology explosion that begans in the end\\nof the last century, started the world to change faster every day. The only\\ntoday's certain is the tomorrow's uncertain. Risk is defined as uncertain where\\none or many causes composed of ocurrence probality can generate an impact or\\nconsequence (threat if negative and oportunity if positive, to a determinated\\ngoal). The Risk Management is composed of culture, procedure and process of an\\norganization or individual care of uncertain, aiming to minimize threats e\\nmaximizing the oportunities, to reach a desired goal.\",\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Modelo de maturidade em gerenciamento de riscos em projetos (Project\\n  Risk Management Model Maturity)',\n",
       "  'text': 'The \"Risk maturity model\\nin projects\" proposed on this document, wants to measure the organizations\\ncapacity and skills to manage the riks involved in projects when adopting a\\ngeneric risk management methodology.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Dynamic risk measures',\n",
       "  'text': 'This paper gives an overview of the theory of dynamic convex risk measures\\nfor random variables in discrete time setting. We summarize robust\\nrepresentation results of conditional convex risk measures, and we characterize\\nvarious time consistency properties of dynamic risk measures in terms of\\nacceptance sets, penalty functions, and by supermartingale properties of risk\\nprocesses and penalty functions.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Modelling catastrophic risk in international equity markets: An extreme\\n  value approach',\n",
       "  'text': 'This letter uses the Block Maxima Extreme Value approach to quantify\\ncatastrophic risk in international equity markets. Risk measures are generated\\nfrom a set threshold of the distribution of returns that avoids the pitfall of\\nusing absolute returns for markets exhibiting diverging levels of risk. From an\\napplication to leading markets, the letter finds that the Nikkei is more prone\\nto catastrophic risk than the FTSE and Dow Jones Indexes.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Wrong-Way Bounds in Counterparty Credit Risk Management',\n",
       "  'text': 'We study the problem of finding the worst-case joint distribution of a set of\\nrisk factors given prescribed multivariate marginals and a nonlinear loss\\nfunction. We show that when the risk measure is CVaR, and the distributions are\\ndiscretized, the problem can be conveniently solved using linear programming\\ntechnique. The method has applications to any situation where marginals are\\nprovided, and bounds need to be determined on total portfolio risk. This arises\\nin many financial contexts, including pricing and risk management of exotic\\noptions, analysis of structured finance instruments, and aggregation of\\nportfolio risk across risk types.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Wrong-Way Bounds in Counterparty Credit Risk Management',\n",
       "  'text': 'Applications to counterparty credit risk are\\nemphasized, and they include assessing wrong-way risk in the credit valuation\\nadjustment, and counterparty credit risk measurement. Lastly a detailed\\napplication of the algorithm for counterparty risk measurement to a real\\nportfolio case is also presented in this paper.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Coherent measurement of factor risks',\n",
       "  'text': 'We propose a new procedure for the risk measurement of large portfolios. It\\nemploys the following objects as the building blocks: - coherent risk measures\\nintroduced by Artzner, Delbaen, Eber, and Heath; - factor risk measures\\nintroduced in this paper, which assess the risks driven by particular factors\\nlike the price of oil, S&P500 index, or the credit spread; - risk contributions\\nand factor risk contributions, which provide a coherent alternative to the\\nsensitivity coefficients. We also propose two particular classes of coherent risk measures called Alpha\\nV@R and Beta V@R, for which all the objects described above admit an extremely\\nsimple empirical estimation procedure.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Coherent measurement of factor risks',\n",
       "  'text': \"This procedure uses no model assumptions\\non the structure of the price evolution. Moreover, we consider the problem of the risk management on a firm's level. It is shown that if the risk limits are imposed on the risk contributions of\\nthe desks to the overall risk of the firm (rather than on their outstanding\\nrisks) and the desks are allowed to trade these limits within a firm, then the\\ndesks automatically find the globally optimal portfolio.\",\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Expected Shortfall as a Tool for Financial Risk Management',\n",
       "  'text': 'We study the properties of Expected Shortfall from the point of view of\\nfinancial risk management. This measure --- which emerges as a natural remedy\\nin some cases where Value at Risk (VaR) is not able to distinguish portfolios\\nwhich bear different levels of risk --- is indeed shown to have much better\\nproperties than VaR. We show in fact that unlike VaR this variable is in\\ngeneral subadditive and therefore it is a Coherent Measure of Risk in the sense\\nof reference (artzner)',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Second Order Risk',\n",
       "  'text': 'Managing a portfolio to a risk model can tilt the portfolio toward weaknesses\\nof the model. As a result, the optimized portfolio acquires downside exposure\\nto uncertainty in the model itself, what we call \"second order risk.\" We\\npropose a risk measure that accounts for this bias. Studies of real portfolios,\\nin asset-by-asset and factor model contexts, demonstrate that second order risk\\ncontributes significantly to realized volatility, and that the proposed measure\\naccurately forecasts the out-of-sample behavior of optimized portfolios.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'When does a disaster become a systemic event? Estimating indirect\\n  economic losses from natural disasters',\n",
       "  'text': 'Reliable estimates of indirect economic losses arising from natural disasters\\nare currently out of scientific reach. To address this problem, we propose a\\nnovel approach that combines a probabilistic physical damage catastrophe model\\nwith a new generation of macroeconomic agent-based models (ABMs). The ABM moves\\nbeyond the state of the art by exploiting large data sets from detailed\\nnational accounts, census data, and business information, etc., to simulate\\ninteractions of millions of agents representing \\\\emph{each} natural person or\\nlegal entity in a national economy. The catastrophe model introduces a copula\\napproach to assess flood losses, considering spatial dependencies of the flood\\nhazard.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'When does a disaster become a systemic event? Estimating indirect\\n  economic losses from natural disasters',\n",
       "  'text': 'These loss estimates are used in a damage scenario generator that\\nprovides input for the ABM, which then estimates indirect economic losses due\\nto the event. For the first time, we are able to link environmental and\\neconomic processes in a computer simulation at this level of detail. We show\\nthat moderate disasters induce comparably small but positive short- to\\nmedium-term, and negative long-term economic impacts. Large-scale events,\\nhowever, trigger a pronounced negative economic response immediately after the\\nevent and in the long term, while exhibiting a temporary short- to medium-term\\neconomic boost. We identify winners and losers in different economic sectors,\\nincluding the fiscal consequences for the government.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'When does a disaster become a systemic event? Estimating indirect\\n  economic losses from natural disasters',\n",
       "  'text': 'We quantify the critical\\ndisaster size beyond which the resilience of an economy to rebuild reaches its\\nlimits. Our results might be relevant for the management of the consequences of\\nsystemic events due to climate change and other disasters.',\n",
       "  'meta': {'_split_id': 2}},\n",
       " {'name': 'A Framework for Cloud Security Risk Management Based on the Business\\n  Objectives of Organizations',\n",
       "  'text': 'Security is considered one of the top ranked risks of Cloud Computing (CC)\\ndue to the outsourcing of sensitive data onto a third party. In addition, the\\ncomplexity of the cloud model results in a large number of heterogeneous\\nsecurity controls that must be consistently managed. Hence, no matter how\\nstrongly the cloud model is secured, organizations continue suffering from lack\\nof trust on CC and remain uncertain about its security risk consequences. Traditional risk management frameworks do not consider the impact of CC\\nsecurity risks on the business objectives of the organizations.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'A Framework for Cloud Security Risk Management Based on the Business\\n  Objectives of Organizations',\n",
       "  'text': 'In this paper,\\nwe propose a novel Cloud Security Risk Management Framework (CSRMF) that helps\\norganizations adopting CC identify, analyze, evaluate, and mitigate security\\nrisks in their Cloud platforms. Unlike traditional risk management frameworks,\\nCSRMF is driven by the business objectives of the organizations. It allows any\\norganization adopting CC to be aware of cloud security risks and align their\\nlow-level management decisions according to high-level business objectives. In\\nessence, it is designed to address impacts of cloud-specific security risks\\ninto business objectives in a given organization.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'A Framework for Cloud Security Risk Management Based on the Business\\n  Objectives of Organizations',\n",
       "  'text': 'Consequently, organizations\\nare able to conduct a cost-value analysis regarding the adoption of CC\\ntechnology and gain an adequate level of confidence in Cloud technology. On the\\nother hand, Cloud Service Providers (CSP) are able to improve productivity and\\nprofitability by managing cloud-related risks. The proposed framework has been\\nvalidated and evaluated through a use-case scenario.',\n",
       "  'meta': {'_split_id': 2}},\n",
       " {'name': 'Measuring and Modeling Behavioral Decision Dynamics in Collective\\n  Evacuation',\n",
       "  'text': 'Identifying and quantifying factors influencing human decision making remains\\nan outstanding challenge, impacting the performance and predictability of\\nsocial and technological systems. In many cases, system failures are traced to\\nhuman factors including congestion, overload, miscommunication, and delays. Here we report results of a behavioral network science experiment, targeting\\ndecision making in a natural disaster. In each scenario, individuals are faced\\nwith a forced \"go\" versus \"no go\" evacuation decision, based on information\\navailable on competing broadcast and peer-to-peer sources.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Measuring and Modeling Behavioral Decision Dynamics in Collective\\n  Evacuation',\n",
       "  'text': 'In this controlled\\nsetting, all actions and observations are recorded prior to the decision,\\nenabling development of a quantitative decision making model that accounts for\\nthe disaster likelihood, severity, and temporal urgency, as well as competition\\nbetween networked individuals for limited emergency resources. Individual\\ndifferences in behavior within this social setting are correlated with\\nindividual differences in inherent risk attitudes, as measured by standard\\npsychological assessments. Identification of robust methods for quantifying\\nhuman decisions in the face of risk has implications for policy in disasters\\nand other threat scenarios.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Who ya gonna call? (Alerting Authorities): Measuring Namespaces, Web\\n  Certificates, and DNSSEC',\n",
       "  'text': 'During disasters, crisis, and emergencies the public relies on online\\nservices provided by official authorities to receive timely alerts, trustworthy\\ninformation, and access to relief programs. It is therefore crucial for the\\nauthorities to reduce risks when accessing their online services. This includes\\ncatering to secure identification of service, secure resolution of name to\\nnetwork service, and content security and privacy as a minimum base for\\ntrustworthy communication. In this paper, we take a first look at Alerting Authorities (AA) in the US\\nand investigate security measures related to trustworthy and secure\\ncommunication. We study the domain namespace structure, DNSSEC penetration, and\\nweb certificates.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Who ya gonna call? (Alerting Authorities): Measuring Namespaces, Web\\n  Certificates, and DNSSEC',\n",
       "  'text': 'We introduce an integrative threat model to better understand\\nwhether and how the online presence and services of AAs are harmed. As an\\nillustrative example, we investigate 1,388 Alerting Authorities, backed by the\\nUnited States Federal Emergency Management Agency (US FEMA). We observe partial\\nheightened security relative to the global Internet trends, yet find cause for\\nconcern as about 80% of service providers fail to deploy measures of\\ntrustworthy service provision.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Who ya gonna call? (Alerting Authorities): Measuring Namespaces, Web\\n  Certificates, and DNSSEC',\n",
       "  'text': 'Our analysis shows two major shortcomings: About\\n50% of organizations do not own their dedicated domain names and are dependent\\non others, 55% opt for unrestricted-use namespaces, which simplifies phishing,\\nand less than 0.4% of unique AA domain names are secured by DNSSEC, which can\\nlead to DNS poisoning and possibly to certificate misissuance. Furthermore, 15%\\nof all hosts provide none or invalid certificates, thus cannot cater to\\nconfidentiality and data integrity, 64% of the hosts provide domain validation\\ncertificates that lack any identity information, and shared certificates have\\ngained on popularity, which leads to fate-sharing and can be a cause for\\ninstability.',\n",
       "  'meta': {'_split_id': 2}},\n",
       " {'name': 'Data-driven Operation of the Resilient Electric Grid: A Case of COVID-19',\n",
       "  'text': 'Electrical energy is a vital part of modern life, and expectations for grid\\nresilience to allow a continuous and reliable energy supply has tremendously\\nincreased even during adverse events (e.g., Ukraine cyber-attack, Hurricane\\nMaria). The global pandemic COVID-19 has raised the electric energy reliability\\nrisk due to potential workforce disruptions, supply chain interruptions, and\\nincreased possible cybersecurity threats. The pandemic introduces a significant\\ndegree of uncertainly to the grid operation in the presence of other extreme\\nevents like natural disasters, unprecedented outages, aging power grids, high\\nproliferation of distributed generation, and cyber-attacks.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Data-driven Operation of the Resilient Electric Grid: A Case of COVID-19',\n",
       "  'text': 'This situation\\nincreases the need for measures for the resiliency of power grids to mitigate\\nthe impacts of the pandemic as well as simultaneous extreme events. Solutions\\nto manage such an adverse scenario will be multi-fold: a) emergency planning\\nand organizational support, b) following safety protocol, c) utilizing enhanced\\nautomation and sensing for situational awareness, and d) integration of\\nadvanced technologies and data points for ML-driven enhanced decision support. Enhanced digitalization and automation resulted in better network visibility at\\nvarious levels, including generation, transmission, and distribution. These\\ndata or information can be utilized to take advantage of advanced machine\\nlearning techniques for automation and increased power grid resilience.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Data-driven Operation of the Resilient Electric Grid: A Case of COVID-19',\n",
       "  'text': 'In this\\npaper, a) we review the impact of COVID-19 on power grid operations and actions\\ntaken by operators/organizations to minimize the impact of COVID-19, and b) we\\nhave presented the recently developed tool and concepts using natural language\\nprocessing (NLP) in the domain of machine learning and artificial intelligence\\nthat can be used for increasing resiliency of power systems in normal and in\\nextreme scenarios such as COVID-19 pandemics.',\n",
       "  'meta': {'_split_id': 2}},\n",
       " {'name': 'Systematic and multifactor risk models revisited',\n",
       "  'text': 'Systematic and multifactor risk models are revisited via methods which were\\nalready successfully developed in signal processing and in automatic control. The results, which bypass the usual criticisms on those risk modeling, are\\nillustrated by several successful computer experiments.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Comparative Study of Geometric and Image Based Modelling and Rendering\\n  Techniques',\n",
       "  'text': 'This is a comparative study of the traditional 3D computer graphics technique\\nof geometric modelling and image-based rendering techniques that were surveyed\\nand implemented.We have discussed the classifications and representative\\nmethods of both the techniques. The study has shown that there is a strong\\ncontinuum between both the techniques and a hybrid of the two is most suitable\\nfor further implementations.This hybridisation study is underway to create\\nmodels of real life situations and provide disaster management training.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'On the Necessity of Five Risk Measures',\n",
       "  'text': \"The banking systems that deal with risk management depend on underlying risk\\nmeasures. Following the Basel II accord, there are two separate methods by\\nwhich banks may determine their capital requirement. The Value at Risk measure\\nplays an important role in computing the capital for both approaches. In this\\npaper we analyze the errors produced by using this measure. We discuss other\\nmeasures, demonstrating their strengths and shortcomings. We give examples,\\nshowing the need for the information from multiple risk measures in order to\\ndetermine a bank's loss distribution. We conclude by suggesting a regulatory\\nrequirement of multiple risk measures being reported by banks, giving specific\\nrecommendations.\",\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Extended Gini-type measures of risk and variability',\n",
       "  'text': 'The aim of this paper is to introduce a risk measure that extends the\\nGini-type measures of risk and variability, the Extended Gini Shortfall, by\\ntaking risk aversion into consideration. Our risk measure is coherent and\\ncatches variability, an important concept for risk management. The analysis is\\nmade under the Choquet integral representations framework. We expose results\\nfor analytic computation under well-known distribution functions. Furthermore,\\nwe provide a practical application.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Machine Learning Risk Models',\n",
       "  'text': 'We give an explicit algorithm and source code for constructing risk models\\nbased on machine learning techniques. The resultant covariance matrices are not\\nfactor models. Based on empirical backtests, we compare the performance of\\nthese machine learning risk models to other constructions, including\\nstatistical risk models, risk models based on fundamental industry\\nclassifications, and also those utilizing multilevel clustering based industry\\nclassifications.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Risk aggregation and capital allocation using a new generalized\\n  Archimedean copula',\n",
       "  'text': 'In this paper, we address risk aggregation and capital allocation problems in\\nthe presence of dependence between risks. The dependence structure is defined\\nby a mixed Bernstein copula which represents a generalization of the well-known\\nArchimedean copulas. Using this new copula, the probability density function\\nand the cumulative distribution function of the aggregate risk are obtained. Then, closed-form expressions for basic risk measures, such as tail\\nvalue-at-risk(TVaR) and TVaR-based allocations, are derived.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Management of Cascading Outage Risk Based on Risk Gradient and Markovian\\n  Tree Search',\n",
       "  'text': 'Since cascading outages are major threats to power systems, it is important\\nto reduce the risk of potential cascading outages. In this paper, a risk\\nmanagement method of cascading outages based on Markovian tree search is\\nproposed. With the tree expansion on the cascading outage risk, risk gradient\\nis computed efficiently by a forward-backward tree search scheme with good\\nconvergence, and it is then employed in an optimization model to minimize\\ncontrol cost while effectively reducing the cascading outage risk. To overcome\\nthe limitation with linearization in computing risk gradient, an iterative risk\\nmanagement (IRM) approach is further developed.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Management of Cascading Outage Risk Based on Risk Gradient and Markovian\\n  Tree Search',\n",
       "  'text': 'Tests on the RTS-96 3-area\\nsystem verify the accuracy of the computed risk gradient and its effectiveness\\nfor risk reduction. Time performance of the proposed IRM approach is tested on\\nthe RTS-96 system, a 410-bus US-Canada northeast system and a 1354-bus\\nMid-European system, and demonstrates its potentials for decision support on\\npractical power systems online or on hourly basis.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Exponential Spectral Risk Measures',\n",
       "  'text': 'Spectral risk measures are attractive risk measures as they allow the user to\\nobtain risk measures that reflect their subjective risk-aversion. This paper\\nexamines spectral risk measures based on an exponential utility function, and\\nfinds that these risk measures have nice intuitive properties. It also\\ndiscusses how they can be estimated using numerical quadrature methods, and how\\nconfidence intervals for them can be estimated using a parametric bootstrap. Illustrative results suggest that estimated exponential spectral risk measures\\nobtained using such methods are quite precise in the presence of normally\\ndistributed losses.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Extreme Spectral Risk Measures: An Application to Futures Clearinghouse\\n  Margin Requirements',\n",
       "  'text': \"This paper applies the Extreme-Value (EV) Generalised Pareto distribution to\\nthe extreme tails of the return distributions for the S&P500, FT100, DAX, Hang\\nSeng, and Nikkei225 futures contracts. It then uses tail estimators from these\\ncontracts to estimate spectral risk measures, which are coherent risk measures\\nthat reflect a user's risk-aversion function. It compares these to VaR and\\nExpected Shortfall (ES) risk measures, and compares the precision of their\\nestimators. It also discusses the usefulness of these risk measures in the\\ncontext of clearinghouses setting initial margin requirements, and compares\\nthese to the SPAN measures typically used. Keywords: Spectral risk measures,\\nExpected Shortfall, Value at Risk, Extreme Value\",\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Spectral Risk Measures, With Adaptions For Stochastic Optimization',\n",
       "  'text': 'Stochastic optimization problems often involve the expectation in its\\nobjective. When risk is incorporated in the problem description as well, then\\nrisk measures have to be involved in addition to quantify the acceptable risk,\\noften in the objective. For this purpose it is important to have an adjusted,\\nadapted and efficient evaluation scheme for the risk measure available. In this\\narticle different representations of an important class of risk measures, the\\nspectral risk measures, are elaborated. The results allow concise problem\\nformulations, they are particularly adapted for stochastic optimization\\nproblems.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Spectral Risk Measures, With Adaptions For Stochastic Optimization',\n",
       "  'text': 'Efficient evaluation algorithms can be built on these new results,\\nwhich finally make optimization problems involving spectral risk measures\\neligible for stochastic optimization.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Robust Optimal Risk Sharing and Risk Premia in Expanding Pools',\n",
       "  'text': 'We consider the problem of optimal risk sharing in a pool of cooperative\\nagents. We analyze the asymptotic behavior of the certainty equivalents and\\nrisk premia associated with the Pareto optimal risk sharing contract as the\\npool expands. We first study this problem under expected utility preferences\\nwith an objectively or subjectively given probabilistic model. Next, we develop\\na robust approach by explicitly taking uncertainty about the probabilistic\\nmodel (ambiguity) into account. The resulting robust certainty equivalents and\\nrisk premia compound risk and ambiguity aversion. We provide explicit results\\non their limits and rates of convergence, induced by Pareto optimal risk\\nsharing in expanding pools.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Regulator-based risk statistics for portfolios',\n",
       "  'text': 'Risk statistic is a critical factor not only for risk analysis but also for\\nfinancial application. However, the traditional risk statistics may fail to\\ndescribe the characteristics of regulator-based risk. In this paper, we\\nconsider the regulator-based risk statistics for portfolios. By further\\ndeveloping the properties related to regulator-based risk statistics, we are\\nable to derive dual representation for such risk.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Set-Valued Dynamic Risk Measures for Processes and Vectors',\n",
       "  'text': 'The relationship between set-valued risk measures for processes and vectors\\non the optional filtration is investigated. The equivalence of risk measures\\nfor processes and vectors and the equivalence of their penalty function\\nformulations are provided. In contrast with scalar risk measures, this\\nequivalence requires an augmentation of the set-valued risk measures for\\nprocesses. We utilize this result to deduce a new dual representation for risk\\nmeasures for processes in the set-valued framework. Finally, the equivalence of\\nmultiportfolio time consistency between set-valued risk measures for processes\\nand vectors are provided; to accomplish this, an augmented definition for\\nmultiportfolio time consistency of set-valued risk measures for processes is\\nproposed.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': \"Forecasting People's Needs in Hurricane Events from Social Network\",\n",
       "  'text': \"Social networks can serve as a valuable communication channel for calls for\\nhelp, offering assistance, and coordinating rescue activities in disaster. Social networks such as Twitter allow users to continuously update relevant\\ninformation, which is especially useful during a crisis, where the rapidly\\nchanging conditions make it crucial to be able to access accurate information\\npromptly. Social media helps those directly affected to inform others of\\nconditions on the ground in real time and thus enables rescue workers to\\ncoordinate their efforts more effectively, better meeting the survivors' need.\",\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': \"Forecasting People's Needs in Hurricane Events from Social Network\",\n",
       "  'text': \"This paper presents a new sequence to sequence based framework for forecasting\\npeople's needs during disasters using social media and weather data. It\\nconsists of two Long Short-Term Memory (LSTM) models, one of which encodes\\ninput sequences of weather information and the other plays as a conditional\\ndecoder that decodes the encoded vector and forecasts the survivors' needs. Case studies utilizing data collected during Hurricane Sandy in 2012, Hurricane\\nHarvey and Hurricane Irma in 2017 were analyzed and the results compared with\\nthose obtained using a statistical language model n-gram and an LSTM generative\\nmodel.\",\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': \"Forecasting People's Needs in Hurricane Events from Social Network\",\n",
       "  'text': \"Our proposed sequence to sequence method forecast people's needs more\\nsuccessfully than either of the other models. This new approach shows great\\npromise for enhancing disaster management activities such as evacuation\\nplanning and commodity flow management.\",\n",
       "  'meta': {'_split_id': 2}},\n",
       " {'name': 'Emergency Management Systems and Algorithms: a Comprehensive Survey',\n",
       "  'text': 'Owing to the increasing frequency and destruction of natural and manmade\\ndisasters to modern highly-populated societies, emergency management, which\\nprovides solutions to prevent or address disasters, have drawn considerable\\nresearch over the last few decades and become a multidisciplinary area. Because\\nof its open and inclusive nature, new technologies always tend to influence,\\nchange or even revolutionise this research area. Hence, it is imperative to\\nconsolidate the state-of-the-art studies and knowledge to meet the research\\nneeds and identify the future research directions. The paper presents a\\ncomprehensive and systemic review of the existing research in the field of\\nemergency management from both the system design aspect and algorithm\\nengineering aspect.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Emergency Management Systems and Algorithms: a Comprehensive Survey',\n",
       "  'text': 'We begin with the history and evolution of the emergency\\nmanagement research. Then the two main research topics of this area, \"emergency\\nnavigation\" and \"emergency search and rescue planning\", are introduced and\\ndiscussed. Finally, we suggest the emerging challenges and opportunities from\\nsystem optimisation, evacuee behaviour modelling and optimisation, computing\\npatterns, data analysis, energy and cyber security aspects.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Establishing A Minimum Generic Skill Set For Risk Management Teaching In\\n  A Spreadsheet Training Course',\n",
       "  'text': 'Past research shows that spreadsheet models are prone to such a high\\nfrequency of errors and data security implications that the risk management of\\nspreadsheet development and spreadsheet use is of great importance to both\\nindustry and academia. The underlying rationale for this paper is that\\nspreadsheet training courses should specifically address risk management in the\\ndevelopment process both from a generic and a domain-specific viewpoint. This\\nresearch specifically focuses on one of these namely those generic issues of\\nrisk management that should be present in a training course that attempts to\\nmeet good-practice within industry.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Establishing A Minimum Generic Skill Set For Risk Management Teaching In\\n  A Spreadsheet Training Course',\n",
       "  'text': 'A pilot questionnaire was constructed\\nshowing a possible minimum set of risk management issues and sent to academics\\nand industry practitioners for feedback. The findings from this pilot survey\\nwill be used to refine the questionnaire for sending to a larger body of\\npossible respondents. It is expected these findings will form the basis of a\\nrisk management teaching approach to be trialled in a number of selected\\nongoing spreadsheet training courses.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Risk-Aware Planning and Assignment for Ground Vehicles using Uncertain\\n  Perception from Aerial Vehicles',\n",
       "  'text': 'We propose a risk-aware framework for multi-robot, multi-demand assignment\\nand planning in unknown environments. Our motivation is disaster response and\\nsearch-and-rescue scenarios where ground vehicles must reach demand locations\\nas soon as possible. We consider a setting where the terrain information is\\navailable only in the form of an aerial, georeferenced image. Deep learning\\ntechniques can be used for semantic segmentation of the aerial image to create\\na cost map for safe ground robot navigation. Such segmentation may still be\\nnoisy. Hence, we present a joint planning and perception framework that\\naccounts for the risk introduced due to noisy perception.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Risk-Aware Planning and Assignment for Ground Vehicles using Uncertain\\n  Perception from Aerial Vehicles',\n",
       "  'text': 'Our contributions are\\ntwo-fold: (i) we show how to use Bayesian deep learning techniques to extract\\nrisk at the perception level; and (ii) use a risk-theoretical measure, CVaR,\\nfor risk-aware planning and assignment. The pipeline is theoretically\\nestablished, then empirically analyzed through two datasets. We find that\\naccounting for risk at both levels produces quantifiably safer paths and\\nassignments.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Estimating value at risk and conditional tail expectation for extreme\\n  and aggregate risks',\n",
       "  'text': 'In this paper, we investigate risk measures such as value at risk (VaR) and\\nthe conditional tail expectation (CTE) of the extreme (maximum and minimum) and\\nthe aggregate (total) of two dependent risks. In finance, insurance and the\\nother fields, when people invest their money in two or more dependent or\\nindependent markets, it is very important to know the extreme and total risk\\nbefore the investment. To find these risk measures for dependent cases is quite\\nchallenging, which has not been reported in the literature to the best of our\\nknowledge.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Estimating value at risk and conditional tail expectation for extreme\\n  and aggregate risks',\n",
       "  'text': 'We use the FGM copula for modelling the dependence as it is\\nrelatively simple for computational purposes and has empirical successes. The\\nmarginal of the risks are considered as exponential and pareto, separately, for\\nthe case of extreme risk and as exponential for the case of the total risk. The\\neffect of the degree of dependency on the VaR and CTE of the extreme and total\\nrisks is analyzed. We also make comparisons for the dependent and independent\\nrisks. Moreover, we propose a new risk measure called median of tail (MoT) and\\ninvestigate MoT for the extreme and aggregate dependent risks.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Digitalization of COVID-19 pandemic management and cyber risk from\\n  connected systems',\n",
       "  'text': 'What makes cyber risks arising from connected systems challenging during the\\nmanagement of a pandemic? Assuming that a variety of cyber-physical systems are\\nalready operational-collecting, analyzing, and acting on data autonomously-what\\nrisks might arise in their application to pandemic management? We already have\\nthese systems operational, collecting, and analyzing data autonomously, so how\\nwould a pandemic monitoring app be different or riskier? In this review\\narticle, we discuss the digitalization of COVID-19 pandemic management and\\ncyber risk from connected systems.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'A Coupled Markov Chain Approach to Credit Risk Modeling',\n",
       "  'text': 'We propose a Markov chain model for credit rating changes. We do not use any\\ndistributional assumptions on the asset values of the rated companies but\\ndirectly model the rating transitions process. The parameters of the model are\\nestimated by a maximum likelihood approach using historical rating transitions\\nand heuristic global optimization techniques. We benchmark the model against a GLMM model in the context of bond portfolio\\nrisk management. The proposed model yields stronger dependencies and higher\\nrisks than the GLMM model. As a result, the risk optimal portfolios are more\\nconservative than the decisions resulting from the benchmark model.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'A risk management approach to capital allocation',\n",
       "  'text': 'The European insurance sector will soon be faced with the application of\\nSolvency 2 regulation norms. It will create a real change in risk management\\npractices. The ORSA approach of the second pillar makes the capital allocation\\nan important exercise for all insurers and specially for groups. Considering\\nmulti-branches firms, capital allocation has to be based on a multivariate risk\\nmodeling. Several allocation methods are present in the literature and insurers\\npractices. In this paper, we present a new risk allocation method, we study its\\ncoherence using an axiomatic approach, and we try to define what the best\\nallocation choice for an insurance group is.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Leverage and Uncertainty',\n",
       "  'text': 'Risk and uncertainty will always be a matter of experience, luck, skills, and\\nmodelling. Leverage is another concept, which is critical for the investor\\ndecisions and results. Adaptive skills and quantitative probabilistic methods\\nneed to be used in successful management of risk, uncertainty and leverage. The\\nauthor explores how uncertainty beyond risk determines consistent leverage in a\\nsimple model of the world with fat tails due to significant, not fully\\nquantifiable and not too rare events. Among particular technical results, for\\nthe single asset fractional Kelly criterion is derived in the presence of the\\nfat tails associated with subjective uncertainty.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Leverage and Uncertainty',\n",
       "  'text': 'For the multi-asset\\nportfolio, Kelly criterion provides an insightful perspective on Risk Parity\\nstrategies, which can be extended for the assets with fat tails.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Deep xVA solver -- A neural network based counterparty credit risk\\n  management framework',\n",
       "  'text': 'In this paper, we present a novel computational framework for portfolio-wide\\nrisk management problems, where the presence of a potentially large number of\\nrisk factors makes traditional numerical techniques ineffective. The new method\\nutilises a coupled system of BSDEs for the valuation adjustments (xVA) and\\nsolves these by a recursive application of a neural network based BSDE solver. This not only makes the computation of xVA for high-dimensional problems\\nfeasible, but also produces hedge ratios and dynamic risk measures for xVA, and\\nallows simulations of the collateral account.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Incorporating Financial Big Data in Small Portfolio Risk Analysis:\\n  Market Risk Management Approach',\n",
       "  'text': 'When applying Value at Risk (VaR) procedures to specific positions or\\nportfolios, we often focus on developing procedures only for the specific\\nassets in the portfolio. However, since this small portfolio risk analysis\\nignores information from assets outside the target portfolio, there may be\\nsignificant information loss. In this paper, we develop a dynamic process to\\nincorporate the ignored information. We also study how to overcome the curse of\\ndimensionality and discuss where and when benefits occur from a large number of\\nassets, which is called the blessing of dimensionality. We find empirical\\nsupport for the proposed method.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Designing a Disaster-resilient Network with Software Defined Networking',\n",
       "  'text': 'With the wide deployment of network facilities and the increasing requirement\\nof network reliability, the disruptive event like natural disaster, power\\noutage or malicious attack has become a non-negligible threat to the current\\ncommunication network. Such disruptive event can simultaneously destroy all\\ndevices in a specific geographical area and affect many network based\\napplications for a long time. Hence, it is essential to build\\ndisaster-resilient network for future highly survivable communication services. In this paper, we consider the problem of designing a highly resilient network\\nthrough the technique of SDN (Software Defined Networking).',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Designing a Disaster-resilient Network with Software Defined Networking',\n",
       "  'text': 'In contrast to the\\nconventional idea of handling all the failures on the control plane (the\\ncontroller), we focus on an integrated design to mitigate disaster risks by\\nadding some redundant functions on the data plane. Our design consists of a\\nsub-graph based proactive protection approach on the data plane and a splicing\\napproach at the controller for effective restoration on the control plane. Such\\na systematic design is implemented in the OpenFlow framework through the\\nMininet emulator and Nox controller. Numerical results show that our approach\\ncan achieve high robustness with low control overhead.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Custom v. Standardized Risk Models',\n",
       "  'text': 'We discuss when and why custom multi-factor risk models are warranted and\\ngive source code for computing some risk factors. Pension/mutual funds do not\\nrequire customization but standardization. However, using standardized risk\\nmodels in quant trading with much shorter holding horizons is suboptimal: 1)\\nlonger horizon risk factors (value, growth, etc.) increase noise trades and\\ntrading costs; 2) arbitrary risk factors can neutralize alpha; 3)\\n\"standardized\" industries are artificial and insufficiently granular; 4)\\nnormalization of style risk factors is lost for the trading universe; 5)\\ndiversifying risk models lowers P&L correlations, reduces turnover and market\\nimpact, and increases capacity. We discuss various aspects of custom risk model\\nbuilding.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Customising Agent Based Analysis Towards Analysis of Disaster Management\\n  Knowledge',\n",
       "  'text': 'In developed countries such as Australia, for recurring disasters (e.g. floods), there are dedicated document repositories of Disaster Management Plans\\n(DISPLANs), and supporting doctrine and processes that are used to prepare\\norganisations and communities for disasters. They are maintained on an ongoing\\ncyclical basis and form a key information source for community education,\\nengagement and awareness programme in the preparation for and mitigation of\\ndisasters. DISPLANS, generally in semi-structured text document format, are\\nthen accessed and activated during the response and recovery to incidents to\\ncoordinate emergency service and community safety actions.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Customising Agent Based Analysis Towards Analysis of Disaster Management\\n  Knowledge',\n",
       "  'text': 'However, accessing\\nthe appropriate plan and the specific knowledge within the text document from\\nacross its conceptual areas in a timely manner and sharing activities between\\nstakeholders requires intimate domain knowledge of the plan contents and its\\ndevelopment. This paper describes progress on an ongoing project with NSW State\\nEmergency Service (NSW SES) to convert DISPLANs into a collection of knowledge\\nunits that can be stored in a unified repository with the goal to form the\\nbasis of a future knowledge sharing capability.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Customising Agent Based Analysis Towards Analysis of Disaster Management\\n  Knowledge',\n",
       "  'text': 'All Australian emergency\\nservices covering a wide range of hazards develop DISPLANs of various structure\\nand intent, in general the plans are created as instances of a template, for\\nexample those which are developed centrally by the NSW and Victorian SESs State\\nplanning policies. In this paper, we illustrate how by using selected templates\\nas part of an elaborate agent-based process, we can apply agent-oriented\\nanalysis more efficiently to convert extant DISPLANs into a centralised\\nrepository. The repository is structured as a layered abstraction according to\\nMeta Object Facility (MOF). The work is illustrated using DISPLANs along the\\nflood-prone Murrumbidgee River in central NSW.',\n",
       "  'meta': {'_split_id': 2}},\n",
       " {'name': 'An Approach to Select Cost-Effective Risk Countermeasures Exemplified in\\n  CORAS',\n",
       "  'text': 'Risk is unavoidable in business and risk management is needed amongst others\\nto set up good security policies. Once the risks are evaluated, the next step\\nis to decide how they should be treated. This involves managers making\\ndecisions on proper countermeasures to be implemented to mitigate the risks. The countermeasure expenditure, together with its ability to mitigate risks, is\\nfactors that affect the selection. While many approaches have been proposed to\\nperform risk analysis, there has been less focus on delivering the prescriptive\\nand specific information that managers require to select cost-effective\\ncountermeasures.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'An Approach to Select Cost-Effective Risk Countermeasures Exemplified in\\n  CORAS',\n",
       "  'text': 'This paper proposes a generic approach to integrate the cost\\nassessment into risk analysis to aid such decision making. The approach makes\\nuse of a risk model which has been annotated with potential countermeasures,\\nestimates for their cost and effect. A calculus is then employed to reason\\nabout this model in order to support decision in terms of decision diagrams. We\\nexemplify the instantiation of the generic approach in the CORAS method for\\nsecurity risk analysis.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Towards Trustworthy Mobile Social Networking Services for Disaster\\n  Response',\n",
       "  'text': 'Situational awareness is crucial for effective disaster management. However,\\nobtaining information about the actual situation is usually difficult and\\ntime-consuming. While there has been some effort in terms of incorporating the\\naffected population as a source of information, the issue of obtaining\\ntrustworthy information has not yet received much attention. Therefore, we\\nintroduce the concept of witness-based report verification, which enables users\\nfrom the affected population to evaluate reports issued by other users. We\\npresent an extensive overview of the objectives to be fulfilled by such a\\nscheme and provide a first approach considering security and privacy. Finally,\\nwe evaluate the performance of our approach in a simulation study.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Towards Trustworthy Mobile Social Networking Services for Disaster\\n  Response',\n",
       "  'text': 'Our results\\nhighlight synergetic effects of group mobility patterns that are likely in\\ndisaster situations.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Performance of Social Network Sensors During Hurricane Sandy',\n",
       "  'text': 'Information flow during catastrophic events is a critical aspect of disaster\\nmanagement. Modern communication platforms, in particular online social\\nnetworks, provide an opportunity to study such flow, and a mean to derive\\nearly-warning sensors, improving emergency preparedness and response. Performance of the social networks sensor method, based on topological and\\nbehavioural properties derived from the \"friendship paradox\", is studied here\\nfor over 50 million Twitter messages posted before, during, and after Hurricane\\nSandy.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Performance of Social Network Sensors During Hurricane Sandy',\n",
       "  'text': 'We find that differences in user\\'s network centrality effectively\\ntranslate into moderate awareness advantage (up to 26 hours); and that\\ngeo-location of users within or outside of the hurricane-affected area plays\\nsignificant role in determining the scale of such advantage. Emotional response\\nappears to be universal regardless of the position in the network topology, and\\ndisplays characteristic, easily detectable patterns, opening a possibility of\\nimplementing a simple \"sentiment sensing\" technique to detect and locate\\ndisasters.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'The Fog Makes Sense: Enabling Social Sensing Services With Limited\\n  Internet Connectivity',\n",
       "  'text': 'Social sensing services use humans as sensor carriers, sensor operators and\\nsensors themselves in order to provide situation-awareness to applications. This promises to provide a multitude of benefits to the users, for example in\\nthe management of natural disasters or in community empowerment. However,\\ncurrent social sensing services depend on Internet connectivity since the\\nservices are deployed on central Cloud platforms. In many circumstances,\\nInternet connectivity is constrained, for instance when a natural disaster\\ncauses Internet outages or when people do not have Internet access due to\\neconomical reasons.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'The Fog Makes Sense: Enabling Social Sensing Services With Limited\\n  Internet Connectivity',\n",
       "  'text': 'In this paper, we propose the emerging Fog Computing\\ninfrastructure to become a key-enabler of social sensing services in situations\\nof constrained Internet connectivity. To this end, we develop a generic\\narchitecture and API of Fog-enabled social sensing services. We exemplify the\\nusage of the proposed social sensing architecture on a number of concrete use\\ncases from two different scenarios.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Automatic Image Filtering on Social Networks Using Deep Learning and\\n  Perceptual Hashing During Crises',\n",
       "  'text': 'The extensive use of social media platforms, especially during disasters,\\ncreates unique opportunities for humanitarian organizations to gain situational\\nawareness and launch relief operations accordingly. In addition to the textual\\ncontent, people post overwhelming amounts of imagery data on social networks\\nwithin minutes of a disaster hit. Studies point to the importance of this\\nonline imagery content for emergency response. Despite recent advances in the\\ncomputer vision field, automatic processing of the crisis-related social media\\nimagery data remains a challenging task. It is because a majority of which\\nconsists of redundant and irrelevant content.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Automatic Image Filtering on Social Networks Using Deep Learning and\\n  Perceptual Hashing During Crises',\n",
       "  'text': 'In this paper, we present an\\nimage processing pipeline that comprises de-duplication and relevancy filtering\\nmechanisms to collect and filter social media image content in real-time during\\na crisis event. Results obtained from extensive experiments on real-world\\ncrisis datasets demonstrate the significance of the proposed pipeline for\\noptimal utilization of both human and machine computing resources.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Indonesian Earthquake Decision Support System',\n",
       "  'text': \"Earthquake DSS is an information technology environment which can be used by\\ngovernment to sharpen, make faster and better the earthquake mitigation\\ndecision. Earthquake DSS can be delivered as E-government which is not only for\\ngovernment itself but in order to guarantee each citizen's rights for\\neducation, training and information about earthquake and how to overcome the\\nearthquake. Knowledge can be managed for future use and would become mining by\\nsaving and maintain all the data and information about earthquake and\\nearthquake mitigation in Indonesia. Using Web technology will enhance global\\naccess and easy to use.\",\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Indonesian Earthquake Decision Support System',\n",
       "  'text': 'Datawarehouse as unNormalized database for\\nmultidimensional analysis will speed the query process and increase reports\\nvariation. Link with other Disaster DSS in one national disaster DSS, link with\\nother government information system and international will enhance the\\nknowledge and sharpen the reports.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'A Review of Augmented Reality Applications for Building Evacuation',\n",
       "  'text': 'Evacuation is one of the main disaster management solutions to reduce the\\nimpact of man-made and natural threats on building occupants. To date, several\\nmodern technologies and gamification concepts, e.g. immersive virtual reality\\nand serious games, have been used to enhance building evacuation preparedness\\nand effectiveness. Those tools have been used both to investigate human\\nbehavior during building emergencies and to train building occupants on how to\\ncope with building evacuations. Augmented Reality (AR) is novel technology that can enhance this process\\nproviding building occupants with virtual contents to improve their evacuation\\nperformance. This work aims at reviewing existing AR applications developed for\\nbuilding evacuation.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'A Review of Augmented Reality Applications for Building Evacuation',\n",
       "  'text': 'This review identifies the disasters and types of building\\nthose tools have been applied for. Moreover, the application goals, hardware\\nand evacuation stages affected by AR are also investigated in the review. Finally, this review aims at identifying the challenges to face for further\\ndevelopment of AR evacuation tools.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Condition Sensing for Electricity Infrastructure in Disasters by Mining\\n  Public Topics from Social Media',\n",
       "  'text': 'Timely and reliable sensing of infrastructure conditions is critical in\\ndisaster management for planning effective infrastructure restorations. Social\\nmedia, a near real-time information source, has been widely used in disasters\\nfor forming timely situational awareness. Yet, using social media to sense\\nelectricity infrastructure conditions has not been explored. This study aims to\\naddress the research gap through mining public topics from social media.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Condition Sensing for Electricity Infrastructure in Disasters by Mining\\n  Public Topics from Social Media',\n",
       "  'text': 'To\\nachieve this purpose, we proposed a systematic and customized approach wherein\\n(1) electricity-related social media data is extracted by the classifier\\ndeveloped based on Bidirectional Encoder Representations from Transformers\\n(BERT); and (2) public topics are modeled with unigrams, bigrams, and trigrams\\nto incorporate the formulaic expressions of infrastructure conditions in social\\nmedia. Electricity infrastructures in Florida impacted by Hurricane Irma are\\nstudied for illustration and demonstration. Results show that the proposed\\napproach is capable of sensing the temporal evolutions and geographic\\ndifferences of electricity infrastructure conditions.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'A Bespoke Workflow Management System for Data-Driven Urgent HPC',\n",
       "  'text': 'In this paper we present a workflow management system which permits the kinds\\nof data-driven workflows required by urgent computing, namely where new data is\\nintegrated into the workflow as a disaster progresses in order refine the\\npredictions as time goes on. This allows the workflow to adapt to new data at\\nruntime, a capability that most workflow management systems do not possess. The\\nworkflow management system was developed for the EU-funded VESTEC project,\\nwhich aims to fuse HPC with real-time data for supporting urgent decision\\nmaking.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'A Bespoke Workflow Management System for Data-Driven Urgent HPC',\n",
       "  'text': 'We first describe an example workflow from the VESTEC project, and show\\nwhy existing workflow technologies do not meet the needs of the project. We\\nthen go on to present the design of our Workflow Management System, describe\\nhow it is implemented into the VESTEC system, and provide an example of the\\nworkflow system in use for a test case.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'On decomposing risk in a financial-intermediate market and reserving',\n",
       "  'text': 'We consider the problem of decomposing monetary risk in the presence of a\\nfully traded market in {\\\\it some} risks. We show that a mark-to-market approach\\nto pricing leads to such a decomposition if the risk measure is time-consistent\\nin the sense of Delbaen.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Computational Dynamic Market Risk Measures in Discrete Time Setting',\n",
       "  'text': 'Different approaches to defining dynamic market risk measures are available\\nin the literature. Most are focused or derived from probability theory,\\neconomic behavior or dynamic programming. Here, we propose an approach to\\ndefine and implement dynamic market risk measures based on recursion and state\\neconomy representation. The proposed approach is to be implementable and to\\ninherit properties from static market risk measures.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Dual Moments and Risk Attitudes',\n",
       "  'text': 'In decision under risk, the primal moments of mean and variance play a\\ncentral role to define the local index of absolute risk aversion. In this\\npaper, we show that in canonical non-EU models dual moments have to be used\\ninstead of, or on par with, their primal counterparts to obtain an equivalent\\nindex of absolute risk aversion.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'What risk measures are time consistent for all filtrations?',\n",
       "  'text': 'We study coherent risk measures which are time-consistent for multiple\\nfiltrations. We show that a coherent risk measure is time-consistent for every\\nfiltration if and only if it is one of four main types. Furthermore, if the\\nrisk measure is strictly monotone it is linear, and if the reference\\nprobability space is not atomic then it is either linear or an essential\\nsupremum.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Systemic Risk, Maximum Entropy and Interbank Contagion',\n",
       "  'text': 'We discuss the systemic risk implied by the interbank exposures reconstructed\\nwith the maximum entropy method. The maximum entropy method severely\\nunderestimates the risk of interbank contagion by assuming a fully connected\\nnetwork, while in reality the structure of the interbank network is sparsely\\nconnected. Here, we formulate an algorithm for sparse network reconstruction,\\nand we show numerically that it provides a more reliable estimation of the\\nsystemic risk.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'On a law of large numbers for insurance risks',\n",
       "  'text': 'This note presents a kind of the strong law of large numbers for an insurance\\nrisk caused by a single catastrophic event rather than by an accumulation of\\nindependent and identically distributed risks. We derive this result by a large\\ndiversification effect resulting from optimal allocation of the risk to many\\nreinsurers or investors.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Valuations and dynamic convex risk measures',\n",
       "  'text': 'This paper approaches the definition and properties of dynamic convex risk\\nmeasures through the notion of a family of concave valuation operators\\nsatisfying certain simple and credible axioms. Exploring these in the simplest\\ncontext of a finite time set and finite sample space, we find natural\\nrisk-transfer and time-consistency properties for a firm seeking to spread its\\nrisk across a group of subsidiaries.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Simulation Methods for Robust Risk Assessment and the Distorted Mix\\n  Approach',\n",
       "  'text': 'Uncertainty requires suitable techniques for risk assessment. Combining\\nstochastic approximation and stochastic average approximation, we propose an\\nefficient algorithm to compute the worst case average value at risk in the face\\nof tail uncertainty. Dependence is modelled by the distorted mix method that\\nflexibly assigns different copulas to different regions of multivariate\\ndistributions. We illustrate the application of our approach in the context of\\nfinancial markets and cyber risk.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'What is the Sharpe Ratio, and how can everyone get it wrong?',\n",
       "  'text': 'The Sharpe ratio is the most widely used risk metric in the quantitative\\nfinance community - amazingly, essentially everyone gets it wrong. In this\\nnote, we will make a quixotic effort to rectify the situation.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Managing layers of risk: Uncertainty in large development programs\\n  combining agile software development and traditional project management',\n",
       "  'text': 'How risks are managed implicitly and explicitly at multiple levels of agile\\nprojects has not been extensively studied and there is a need to investigate\\nhow risk management can be used in large agile projects. This is the objective\\nof this exploratory study which investigates the following research question:\\nHow does a large software/hardware development project using agile practices\\nmanage uncertainty at project/subproject and work package levels?',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'A Stochastic Processes Toolkit for Risk Management',\n",
       "  'text': 'In risk management it is desirable to grasp the essential statistical\\nfeatures of a time series representing a risk factor. This tutorial aims to\\nintroduce a number of different stochastic processes that can help in grasping\\nthe essential features of risk factors describing different asset classes or\\nbehaviors. This paper does not aim at being exhaustive, but gives examples and\\na feeling for practically implementable models allowing for stylised features\\nin the data.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'A Stochastic Processes Toolkit for Risk Management',\n",
       "  'text': 'The reader may also use these models as building blocks to build\\nmore complex models, although for a number of risk management applications the\\nmodels developed here suffice for the first step in the quantitative analysis. The broad qualitative features addressed here are {fat tails} and {mean\\nreversion}. We give some orientation on the initial choice of a suitable\\nstochastic process and then explain how the process parameters can be estimated\\nbased on historical data. Once the process has been calibrated, typically\\nthrough maximum likelihood estimation, one may simulate the risk factor and\\nbuild future scenarios for the risky portfolio.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'A Stochastic Processes Toolkit for Risk Management',\n",
       "  'text': 'On the terminal simulated\\ndistribution of the portfolio one may then single out several risk measures,\\nalthough here we focus on the stochastic processes estimation preceding the\\nsimulation of the risk factors Finally, this first survey report focuses on\\nsingle time series. Correlation or more generally dependence across risk\\nfactors, leading to multivariate processes modeling, will be addressed in\\nfuture work.',\n",
       "  'meta': {'_split_id': 2}},\n",
       " {'name': 'Impact of storm risk on Faustmann rotation',\n",
       "  'text': 'Global warming may induce in Western Europe an increase in storms. Hence the\\nforest managers will have to take into account the risk increase. We study the\\nimpact of storm risk at the stand level. From the analytical expressions of the\\nFaustmann criterion and the Expected Long-Run Average Yield, we deduce in\\npresence of storm risk the influence of criteria and of discount rate in terms\\nof optimal thinnings and cutting age. We discuss the validity of using a risk\\nadjusted discount rate (a rate of storm risk added to the discount rate)\\nwithout risk to mimic the storm risk case in terms of optimal thinnings.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Assessing Financial Model Risk',\n",
       "  'text': 'Model risk has a huge impact on any risk measurement procedure and its\\nquantification is therefore a crucial step. In this paper, we introduce three\\nquantitative measures of model risk when choosing a particular reference model\\nwithin a given class: the absolute measure of model risk, the relative measure\\nof model risk and the local measure of model risk. Each of the measures has a\\nspecific purpose and so allows for flexibility. We illustrate the various\\nnotions by studying some relevant examples, so as to emphasize the\\npracticability and tractability of our approach.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Risk measures with the CxLS property',\n",
       "  'text': 'In the present contribution we characterize law determined convex risk\\nmeasures that have convex level sets at the level of distributions. By relaxing\\nthe assumptions in Weber (2006), we show that these risk measures can be\\nidentified with a class of generalized shortfall risk measures. As a direct\\nconsequence, we are able to extend the results in Ziegel (2014) and Bellini and\\nBignozzi (2014) on convex elicitable risk measures and confirm that expectiles\\nare the only elicitable coherent risk measures. Further, we provide a simple\\ncharacterization of robustness for convex risk measures in terms of a weak\\nnotion of mixture continuity.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Model Spaces for Risk Measures',\n",
       "  'text': 'We show how risk measures originally defined in a model free framework in\\nterms of acceptance sets and reference assets imply a meaningful underlying\\nprobability structure. Hereafter we construct a maximal domain of definition of\\nthe risk measure respecting the underlying ambiguity profile. We particularly\\nemphasise liquidity effects and discuss the correspondence between properties\\nof the risk measure and the structure of this domain as well as\\nsubdifferentiability properties. Keywords: Model free risk assessment, extension of risk measures, continuity\\nproperties of risk measures, subgradients.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Operational risk modeled analytically II: the consequences of\\n  classification invariance',\n",
       "  'text': \"Most of the banks' operational risk internal models are based on loss pooling\\nin risk and business line categories. The parameters and outputs of operational\\nrisk models are sensitive to the pooling of the data and the choice of the risk\\nclassification. In a simple model, we establish the link between the number of\\nrisk cells and the model parameters by requiring invariance of the bank's loss\\ndistribution upon a change in classification. We provide details on the impact\\nof this requirement on the domain of attraction of the loss distribution, on\\ndiversification effects and on cell risk correlations.\",\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Portfolio Risk Assessment using Copula Models',\n",
       "  'text': \"In the paper, we use and investigate copulas models to represent multivariate\\ndependence in financial time series. We propose the algorithm of risk measure\\ncomputation using copula models. Using the optimal mean-$CVaR$ portfolio we\\ncompute portfolio's Profit and Loss series and corresponded risk measures\\ncurves. Value-at-risk and Conditional-Value-at-risk curves were simulated by\\nthree copula models: full Gaussian, Student's $t$ and regular vine copula. These risk curves are lower than historical values of the risk measures curve. All three models have superior prediction ability than a usual empirical\\nmethod. Further directions of research are described.\",\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Regulator-based risk statistics with scenario analysis',\n",
       "  'text': 'As regulators pay more attentions to losses rather than gains, we are able to\\nderive a new class of risk statistics, named regulator-based risk statistics\\nwith scenario analysis in this paper. This new class of risk statistics can be\\nconsidered as a kind of risk extension of risk statistics introduced by Kou et\\nal. \\\\cite{11}, and also data-based versions of loss-based risk measures\\nintroduced by Cont et al. \\\\cite{5} and Sun et al. \\\\cite{12}.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Cash sub-additive risk statistics for portfolios',\n",
       "  'text': 'The portfolios are a critical factor not only in risk analysis, but also in\\ninsurance and financial applications. In this paper, we consider a special\\nclass of risk statistics from the perspective of time value of the money. This\\nnew risk statistic can be uesd for the quantification of portfolio risk. By\\nfurther developing the properties related to cash sub-additive risk statistics,\\nwe are able to derive representation results for such risk.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'On the Concavity of Expected Shortfall',\n",
       "  'text': 'It is well known that Expected Shortfall (also called Average Value-at-Risk)\\nis a convex risk measure, i. e. Expected Shortfall of a convex linear\\ncombination of arbitrary risk positions is not greater than a convex linear\\ncombination with the same weights of Expected Shortfalls of the same risk\\npositions. In this short paper we prove that Expected Shortfall is a concave\\nrisk measure with respect to probability distributions, i. e. Expected\\nShortfall of a finite mixture of arbitrary risk positions is not lower than the\\nlinear combination of Expected Shortfalls of the same risk positions (with the\\nsame weights as in the mixture).',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Convex Risk Measures based on Divergence',\n",
       "  'text': \"Risk measures connect probability theory or statistics to optimization,\\nparticularly to convex optimization. They are nowadays standard in applications\\nof finance and in insurance involving risk aversion. This paper investigates a\\nwide class of risk measures on Orlicz spaces. The characterizing function\\ndescribes the decision maker's risk assessment towards increasing losses. We\\nlink the risk measures to a crucial formula developed by Rockafellar for the\\nAverage Value-at-Risk based on convex duality, which is fundamental in\\ncorresponding optimization problems. We characterize the dual and provide\\ncomplementary representations.\",\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Monetary Risk Measures',\n",
       "  'text': 'In this paper, we study general monetary risk measures (without any convexity\\nor weak convexity). A monetary (respectively, positively homogeneous) risk\\nmeasure can be characterized as the lower envelope of a family of convex\\n(respectively, coherent) risk measures. The proof does not depend on but easily\\nleads to the classical representation theorems for convex and coherent risk\\nmeasures. When the law-invariance and the SSD (second-order stochastic\\ndominance)-consistency are involved, it is not the convexity (respectively,\\ncoherence) but the comonotonic convexity (respectively, comonotonic coherence)\\nof risk measures that can be used for such kind of lower envelope\\ncharacterizations in a unified form.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Monetary Risk Measures',\n",
       "  'text': 'The representation of a law-invariant risk\\nmeasure in terms of VaR is provided.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Problems with Risk Matrices Using Ordinal Scales',\n",
       "  'text': 'In this paper, we discuss various problems in the usage and definition of\\nrisk matrices. We give an overview of the general process of risk assessment\\nwith risk matrices and ordinal scales. Furthermore, we explain the fallacies in\\neach phase of this process and give hints on which decisions may lead to more\\nproblems than others and how to avoid them. Among those 24 discussed problems\\nare ordinal scales, semi-quantitative arithmetics, range compression, risk\\ninversion, ambiguity, and neglection of uncertainty. Finally, we make a case\\nfor avoiding risk matrices altogether and instead propose using fully\\nquantitative risk assessment methods.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Sharing of longevity basis risk in pension schemes with income-drawdown\\n  guarantees',\n",
       "  'text': \"This work studies a stochastic optimal control problem for a pension scheme\\nwhich provides an income-drawdown policy to its members after their retirement. To manage the scheme efficiently, the manager and members agree to share the\\ninvestment risk based on a pre-decided risk-sharing rule. The objective is to\\nmaximise both sides' utilities by controlling the manager's investment in risky\\nassets and members' benefit withdrawals. We use stochastic affine class models\\nto describe the force of mortality of the members' population and consider a\\nlongevity bond whose coupon payment is linked to a survival index.\",\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Sharing of longevity basis risk in pension schemes with income-drawdown\\n  guarantees',\n",
       "  'text': \"In our\\nframework, we also investigate the longevity basis risk, which arises when the\\nmembers' and the longevity bond's reference populations show different\\nmortality behaviours. By applying the dynamic programming principle to solve\\nthe corresponding HJB equations, we derive optimal solutions for the single-\\nand sub-population cases. Our numerical results show that by sharing the risk,\\nboth manager and members increase their utility. Moreover, even in the presence\\nof longevity basis risk, we demonstrate that the longevity bond acts as an\\neffective hedging instrument.\",\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Incentive Games and Mechanisms for Risk Management',\n",
       "  'text': 'Incentives play an important role in (security and IT) risk management of a\\nlarge-scale organization with multiple autonomous divisions. This paper\\npresents an incentive mechanism design framework for risk management based on a\\ngame-theoretic approach. The risk manager acts as a mechanism designer\\nproviding rules and incentive factors such as assistance or subsidies to\\ndivisions or units, which are modeled as selfish players of a strategic\\n(noncooperative) game. Based on this model, incentive mechanisms with various\\nobjectives are developed that satisfy efficiency, preference-compatibility, and\\nstrategy-proofness criteria.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Incentive Games and Mechanisms for Risk Management',\n",
       "  'text': \"In addition, iterative and distributed algorithms\\nare presented, which can be implemented under information limitations such as\\nthe risk manager not knowing the individual units' preferences. An example\\nscenario illustrates the framework and results numerically. The incentive\\nmechanism design approach presented is useful for not only deriving guidelines\\nbut also developing computer-assistance systems for large-scale risk\\nmanagement.\",\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Parametric Risk Parity',\n",
       "  'text': 'Any optimization algorithm based on the risk parity approach requires the\\nformulation of portfolio total risk in terms of marginal contributions. In this\\npaper we use the independence of the underlying factors in the market to derive\\nthe centered moments required in the risk decomposition process when the\\nmodified versions of Value at Risk and Expected Shortfall are considered. The choice of the Mixed Tempered Stable distribution seems adequate for\\nfitting skewed and heavy tailed distributions. The ensuing detailed description\\nof the optimization procedure is due to the existence of analytical higher\\norder moments. Better results are achieved in terms of out of sample\\nperformance and greater diversification.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'A General Framework for Portfolio Theory. Part II: drawdown risk\\n  measures',\n",
       "  'text': 'The aim of this paper is to provide several examples of convex risk measures\\nnecessary for the application of the general framework for portfolio theory of\\nMaier-Paape and Zhu, presented in Part I of this series (arXiv:1710.04579\\n[q-fin.PM]). As alternative to classical portfolio risk measures such as the\\nstandard deviation we in particular construct risk measures related to the\\ncurrent drawdown of the portfolio equity. Combined with the results of Part I\\n(arXiv:1710.04579 [q-fin.PM]), this allows us to calculate efficient portfolios\\nbased on a drawdown risk measure constraint.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Expected Shortfall and Beyond',\n",
       "  'text': 'Financial institutions have to allocate so-called \"economic capital\" in order\\nto guarantee solvency to their clients and counter parties. Mathematically\\nspeaking, any methodology of allocating capital is a \"risk measure\", i.e. a\\nfunction mapping random variables to the real numbers. Nowadays\\n\"value-at-risk\", which is defined as a fixed level quantile of the random\\nvariable under consideration, is the most popular risk measure. Unfortunately,\\nit fails to reward diversification, as it is not \"subadditive\". In the search\\nfor a suitable alternative to value-at-risk, \"Expected Shortfall\" (or\\n\"conditional value-at-risk\" or \"tail value-at-risk\") has been characterized as\\nthe smallest \"coherent\" and \"law invariant\" risk measure to dominate\\nvalue-at-risk.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Expected Shortfall and Beyond',\n",
       "  'text': 'We discuss these and some other properties of Expected Shortfall\\nas well as its generalization to a class of coherent risk measures which can\\nincorporate higher moment effects. Moreover, we suggest a general method on how\\nto attribute Expected Shortfall \"risk contributions\" to portfolio components. Key words: Expected Shortfall; Value-at-Risk; Spectral Risk Measure;\\ncoherence; risk contribution.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Set-valued shortfall and divergence risk measures',\n",
       "  'text': 'Risk measures for multivariate financial positions are studied in a\\nutility-based framework. Under a certain incomplete preference relation,\\nshortfall and divergence risk measures are defined as the optimal values of\\nspecific set minimization problems. The dual relationship between these two\\nclasses of multivariate risk measures is constructed via a recent Lagrange\\nduality for set optimization. In particular, it is shown that a shortfall risk\\nmeasure can be written as an intersection over a family of divergence risk\\nmeasures indexed by a scalarization parameter. Examples include set-valued\\nversions of the entropic risk measure and the average value at risk.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Set-valued shortfall and divergence risk measures',\n",
       "  'text': 'As a\\nsecond step, the minimization of these risk measures subject to trading\\nopportunities is studied in a general convex market in discrete time. The\\noptimal value of the minimization problem, called the market risk measure, is\\nalso a set-valued risk measure. A dual representation for the market risk\\nmeasure that decomposes the effects of the original risk measure and the\\nfrictions of the market is proved.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Risk measuring under liquidity risk',\n",
       "  'text': 'We present a general framework for measuring the liquidity risk. The\\ntheoretical framework defines a class of risk measures that incorporate the\\nliquidity risk into the standard risk measures. We consider a one-period risk\\nmeasurement model. The liquidity risk is defined as the risk that a given\\nsecurity or a portfolio of securities cannot be easily sold or bought by the\\nfinancial institutions without causing significant changes in prices. The new\\nrisk measures present some differences with respect to the standard risk\\nmeasures. In particular, they are increasing monotonic and convex cash\\nsub-additive on long positions. The contrary, in certain situations, holds for\\nthe sell positions.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Risk measuring under liquidity risk',\n",
       "  'text': 'For the long positions case, we provide these new risk\\nmeasures with a dual representation. In some specific cases also the sell\\npositions can be equipped with a dual representation. We apply our framework to\\nthe situation in which financial institutions break up large trades into many\\nsmall ones. Dual representation results are also obtained. We give many\\npractical examples of risk measures and derive for each of them the respective\\ncapital requirement. As a particular example, we discuss the VaR measure.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Risk Loadings in Classification Ratemaking',\n",
       "  'text': 'The risk premium of a policy is the sum of the pure premium and the risk\\nloading. In the classification ratemaking process, generalized linear models\\nare usually used to calculate pure premiums, and various premium principles are\\napplied to derive the risk loadings. No matter which premium principle is used,\\nsome risk loading parameters should be given in advance subjectively. To\\novercome this subjective problem and calculate the risk premium more reasonably\\nand objectively, we propose a top-down method to calculate these risk loading\\nparameters. First, we implement the bootstrap method to calculate the total\\nrisk premium of the portfolio.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Risk Loadings in Classification Ratemaking',\n",
       "  'text': \"Then, under the constraint that the portfolio's\\ntotal risk premium should equal the sum of the risk premiums of each policy,\\nthe risk loading parameters are determined. During this process, besides using\\ngeneralized linear models, three kinds of quantile regression models are also\\napplied, namely, traditional quantile regression model, fully parametric\\nquantile regression model, and quantile regression model with coefficient\\nfunctions. The empirical result shows that the risk premiums calculated by the\\nmethod proposed in this study can reasonably differentiate the heterogeneity of\\ndifferent risk classes.\",\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'CrisisMMD: Multimodal Twitter Datasets from Natural Disasters',\n",
       "  'text': 'During natural and man-made disasters, people use social media platforms such\\nas Twitter to post textual and multime- dia content to report updates about\\ninjured or dead people, infrastructure damage, and missing or found people\\namong other information types. Studies have revealed that this on- line\\ninformation, if processed timely and effectively, is ex- tremely useful for\\nhumanitarian organizations to gain situational awareness and plan relief\\noperations. In addition to the analysis of textual content, recent studies have\\nshown that imagery content on social media can boost disaster response\\nsignificantly.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'CrisisMMD: Multimodal Twitter Datasets from Natural Disasters',\n",
       "  'text': 'Despite extensive research that mainly focuses on textual\\ncontent to extract useful information, limited work has focused on the use of\\nimagery content or the combination of both content types. One of the reasons is\\nthe lack of labeled imagery data in this domain. Therefore, in this paper, we\\naim to tackle this limitation by releasing a large multi-modal dataset\\ncollected from Twitter during different natural disasters. We provide three\\ntypes of annotations, which are useful to address a number of crisis response\\nand management tasks for different humanitarian organizations.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Deep-Learning-Based Aerial Image Classification for Emergency Response\\n  Applications Using Unmanned Aerial Vehicles',\n",
       "  'text': 'Unmanned Aerial Vehicles (UAVs), equipped with camera sensors can facilitate\\nenhanced situational awareness for many emergency response and disaster\\nmanagement applications since they are capable of operating in remote and\\ndifficult to access areas. In addition, by utilizing an embedded platform and\\ndeep learning UAVs can autonomously monitor a disaster stricken area, analyze\\nthe image in real-time and alert in the presence of various calamities such as\\ncollapsed buildings, flood, or fire in order to faster mitigate their effects\\non the environment and on human population. To this end, this paper focuses on\\nthe automated aerial scene classification of disaster events from on-board a\\nUAV.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Deep-Learning-Based Aerial Image Classification for Emergency Response\\n  Applications Using Unmanned Aerial Vehicles',\n",
       "  'text': 'Specifically, a dedicated Aerial Image Database for Emergency Response\\n(AIDER) applications is introduced and a comparative analysis of existing\\napproaches is performed. Through this analysis a lightweight convolutional\\nneural network (CNN) architecture is developed, capable of running efficiently\\non an embedded platform achieving ~3x higher performance compared to existing\\nmodels with minimal memory requirements with less than 2% accuracy drop\\ncompared to the state-of-the-art. These preliminary results provide a solid\\nbasis for further experimentation towards real-time aerial image classification\\nfor emergency response applications using UAVs.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Deep Sensing of Urban Waterlogging',\n",
       "  'text': 'In the monsoon season, sudden flood events occur frequently in urban areas,\\nwhich hamper the social and economic activities and may threaten the\\ninfrastructure and lives. The use of an efficient large-scale waterlogging\\nsensing and information system can provide valuable real-time disaster\\ninformation to facilitate disaster management and enhance awareness of the\\ngeneral public to alleviate losses during and after flood disasters. Therefore,\\nin this study, a visual sensing approach driven by deep neural networks and\\ninformation and communication technology was developed to provide an end-to-end\\nmechanism to realize waterlogging sensing and event-location mapping.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Deep Sensing of Urban Waterlogging',\n",
       "  'text': 'The use\\nof a deep sensing system in the monsoon season in Taiwan was demonstrated, and\\nwaterlogging events were predicted on the island-wide scale. The system could\\nsense approximately 2379 vision sources through an internet of video things\\nframework and transmit the event-location information in 5 min. The proposed\\napproach can sense waterlogging events at a national scale and provide an\\nefficient and highly scalable alternative to conventional waterlogging sensing\\nmethods.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Estimation Error of Expected Shortfall',\n",
       "  'text': 'The problem of estimation error of Expected Shortfall is analyzed, with a\\nview of its introduction as a global regulatory risk measure.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Scenarios and their Aggregation in the Regulatory Risk Measurement\\n  Environment',\n",
       "  'text': 'We define scenarios, propose different methods of aggregating them, discuss\\ntheir properties and benchmark them against quadrant requirements.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Portfolio Insurance under a risk-measure constraint',\n",
       "  'text': 'We study the problem of portfolio insurance from the point of view of a fund\\nmanager, who guarantees to the investor that the portfolio value at maturity\\nwill be above a fixed threshold. If, at maturity, the portfolio value is below\\nthe guaranteed level, a third party will refund the investor up to the\\nguarantee. In exchange for this protection, the third party imposes a limit on\\nthe risk exposure of the fund manager, in the form of a convex monetary risk\\nmeasure.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Portfolio Insurance under a risk-measure constraint',\n",
       "  'text': \"The fund manager therefore tries to maximize the investor's utility\\nfunction subject to the risk measure constraint.We give a full solution to this\\nnonconvex optimization problem in the complete market setting and show in\\nparticular that the choice of the risk measure is crucial for the optimal\\nportfolio to exist. Explicit results are provided for the entropic risk measure\\n(for which the optimal portfolio always exists) and for the class of spectral\\nrisk measures (for which the optimal portfolio may fail to exist in some\\ncases).\",\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'An initial approach to Risk Management of Funding Costs',\n",
       "  'text': 'In this note we sketch an initial tentative approach to funding costs\\nanalysis and management for contracts with bilateral counterparty risk in a\\nsimplified setting. We depart from the existing literature by analyzing the\\nissue of funding costs and benefits under the assumption that the associated\\nrisks cannot be hedged properly. We also model the treasury funding spread by\\nmeans of a stochastic Weighted Cost of Funding Spread (WCFS) which helps\\ndescribing more realistic financing policies of a financial institution.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'An initial approach to Risk Management of Funding Costs',\n",
       "  'text': 'We\\nelaborate on some limitations in replication-based Funding / Credit Valuation\\nAdjustments we worked on ourselves in the past, namely CVA, DVA, FVA and\\nrelated quantities as generally discussed in the industry. We advocate as a\\ndifferent possibility, when replication is not possible, the analysis of the\\nfunding profit and loss distribution and explain how long term funding spreads,\\nwrong way risk and systemic risk are generally overlooked in most of the\\ncurrent literature on risk measurement of funding costs.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'An initial approach to Risk Management of Funding Costs',\n",
       "  'text': 'As a matter of initial\\nillustration, we discuss in detail the funding management of interest rate\\nswaps with bilateral counterparty risk in the simplified setup of our framework\\nthrough numerical examples and via a few simplified assumptions.',\n",
       "  'meta': {'_split_id': 2}},\n",
       " {'name': 'Evaluating the role of risk networks on risk identification,\\n  classification and emergence',\n",
       "  'text': 'Modern society heavily relies on strongly connected, socio-technical systems. As a result, distinct risks threatening the operation of individual systems can\\nno longer be treated in isolation. Consequently, risk experts are actively\\nseeking for ways to relax the risk independence assumption that undermines\\ntypical risk management models. Prominent work has advocated the use of risk\\nnetworks as a way forward. Yet, the inevitable biases introduced during the\\ngeneration of these survey-based risk networks limit our ability to examine\\ntheir topology, and in turn challenge the utility of the very notion of a risk\\nnetwork. To alleviate these concerns, we proposed an alternative methodology\\nfor generating weighted risk networks.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Evaluating the role of risk networks on risk identification,\\n  classification and emergence',\n",
       "  'text': 'We subsequently applied this methodology\\nto an empirical dataset of financial data. This paper reports our findings on\\nthe study of the topology of the resulting risk network. We observed a modular\\ntopology, and reasoned on its use as a robust risk classification framework. Using these modules, we highlight a tendency of specialization during the risk\\nidentification process, with some firms being solely focused on a subset of the\\navailable risk classes. Finally, we considered the independent and systemic\\nimpact of some risks and attributed possible mismatches to their emerging\\nnature.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'A Remark on the Structure of Expectiles',\n",
       "  'text': 'Expectiles were defined using a minimisation principle. They form a special\\nclass of coherent risk measures. We will describe the scenario set and we will\\nshow that there is a most severe commonotonic risk measure that is smaller than\\nthe given expectile.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Elicitability and backtesting: Perspectives for banking regulation',\n",
       "  'text': 'Conditional forecasts of risk measures play an important role in internal\\nrisk management of financial institutions as well as in regulatory capital\\ncalculations. In order to assess forecasting performance of a risk measurement\\nprocedure, risk measure forecasts are compared to the realized financial losses\\nover a period of time and a statistical test of correctness of the procedure is\\nconducted. This process is known as backtesting. Such traditional backtests are\\nconcerned with assessing some optimality property of a set of risk measure\\nestimates. However, they are not suited to compare different risk estimation\\nprocedures.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Elicitability and backtesting: Perspectives for banking regulation',\n",
       "  'text': 'We investigate the proposal of comparative backtests, which are\\nbetter suited for method comparisons on the basis of forecasting accuracy, but\\nnecessitate an elicitable risk measure. We argue that supplementing traditional\\nbacktests with comparative backtests will enhance the existing trading book\\nregulatory framework for banks by providing the correct incentive for accuracy\\nof risk measure forecasts. In addition, the comparative backtesting framework\\ncould be used by banks internally as well as by researchers to guide selection\\nof forecasting methods. The discussion focuses on three risk measures,\\nValue-at-Risk, expected shortfall and expectiles, and is supported by a\\nsimulation study and data analysis.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Loss-Based Risk Measures',\n",
       "  'text': 'Starting from the requirement that risk measures of financial portfolios\\nshould be based on their losses, not their gains, we define the notion of\\nloss-based risk measure and study the properties of this class of risk\\nmeasures. We characterize loss-based risk measures by a representation theorem\\nand give examples of such risk measures. We then discuss the statistical\\nrobustness of estimators of loss-based risk measures: we provide a general\\ncriterion for qualitative robustness of risk estimators and compare this\\ncriterion with sensitivity analysis of estimators based on influence functions. Finally, we provide examples of statistically robust estimators for loss-based\\nrisk measures.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'The importance of dynamic risk constraints for limited liability\\n  operators',\n",
       "  'text': 'Previous literature shows that prevalent risk measures such as Value at Risk\\nor Expected Shortfall are ineffective to curb excessive risk-taking by a\\ntail-risk-seeking trader with S-shaped utility function in the context of\\nportfolio optimisation. However, these conclusions hold only when the\\nconstraints are static in the sense that the risk measure is just applied to\\nthe terminal portfolio value. In this paper, we consider a portfolio\\noptimisation problem featuring S-shaped utility and a dynamic risk constraint\\nwhich is imposed throughout the entire trading horizon.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'The importance of dynamic risk constraints for limited liability\\n  operators',\n",
       "  'text': \"Provided that the risk\\ncontrol policy is sufficiently strict relative to the asset performance, the\\ntrader's portfolio strategies and the resulting maximal expected utility can be\\neffectively constrained by a dynamic risk measure. Finally, we argue that\\ndynamic risk constraints might still be ineffective if the trader has access to\\na derivatives market.\",\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Marking Systemic Portfolio Risk with Application to the Correlation Skew\\n  of Equity Baskets',\n",
       "  'text': \"The downside risk of a portfolio of (equity)assets is generally substantially\\nhigher than the downside risk of its components. In particular in times of\\ncrises when assets tend to have high correlation, the understanding of this\\ndifference can be crucial in managing systemic risk of a portfolio. In this\\npaper we generalize Merton's option formula in the presence jumps to the\\nmulti-asset case. It is shown how common jumps across assets provide an\\nintuitive and powerful tool to describe systemic risk that is consistent with\\ndata. The methodology provides a new way to mark and risk-manage systemic risk\\nof portfolios in a systematic way.\",\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Which measure for PFE? The Risk Appetite Measure, A',\n",
       "  'text': 'Potential Future Exposure (PFE) is a standard risk metric for managing\\nbusiness unit counterparty credit risk but there is debate on how it should be\\ncalculated. The debate has been whether to use one of many historical\\n(\"physical\") measures (one per calibration setup), or one of many risk-neutral\\nmeasures (one per numeraire). However, we argue that limits should be based on\\nthe bank\\'s own risk appetite provided that this is consistent with regulatory\\nbacktesting and that whichever measure is used it should behave (in a sense\\nmade precise) like a historical measure.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Which measure for PFE? The Risk Appetite Measure, A',\n",
       "  'text': \"Backtesting is only required by\\nregulators for banks with IMM approval but we expect that similar methods are\\npart of limit maintenance generally. We provide three methods for computing the\\nbank price of risk from readily available business unit data, i.e. business\\nunit budgets (rate of return) and limits (e.g. exposure percentiles). Hence we\\ndefine and propose a Risk Appetite Measure, A, for PFE and suggest that this is\\nuniquely consistent with the bank's Risk Appetite Framework as required by\\nsound governance.\",\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Portfolio optimization with two coherent risk measures',\n",
       "  'text': 'We provide analytical results for a static portfolio optimization problem\\nwith two coherent risk measures. The use of two risk measures is motivated by\\njoint decision-making for portfolio selection where the risk perception of the\\nportfolio manager is of primary concern, hence, it appears in the objective\\nfunction, and the risk perception of an external authority needs to be taken\\ninto account as well, which appears in the form of a risk constraint. The\\nproblem covers the risk minimization problem with an expected return constraint\\nand the expected return maximization problem with a risk constraint, as special\\ncases.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Portfolio optimization with two coherent risk measures',\n",
       "  'text': 'For the general case of an arbitrary joint distribution for the asset\\nreturns, under certain conditions, we characterize the optimal portfolio as the\\noptimal Lagrange multiplier associated to an equality-constrained dual problem. Then, we consider the special case of Gaussian returns for which it is possible\\nto identify all cases where an optimal solution exists and to give an explicit\\nformula for the optimal portfolio whenever it exists.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Portfolio Optimisation within a Wasserstein Ball',\n",
       "  'text': \"We consider the problem of active portfolio management where a loss-averse\\nand/or gain-seeking investor aims to outperform a benchmark strategy's risk\\nprofile while not deviating too much from it. Specifically, an investor\\nconsiders alternative strategies that co-move with the benchmark and whose\\nterminal wealth lies within a Wasserstein ball surrounding it. The investor\\nthen chooses the alternative strategy that minimises their personal risk\\npreferences, modelled in terms of a distortion risk measure. In a general\\nmarket model, we prove that an optimal dynamic strategy exists and is unique,\\nand provide its characterisation through the notion of isotonic projections.\",\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Portfolio Optimisation within a Wasserstein Ball',\n",
       "  'text': \"Finally, we illustrate how investors with different risk preferences invest and\\nimprove upon the benchmark using the Tail Value-at-Risk, inverse S-shaped\\ndistortion risk measures, and lower- and upper-tail risk measures as examples. We find that investors' optimal terminal wealth distribution has larger\\nprobability masses in regions that reduce their risk measure relative to the\\nbenchmark while preserving some aspects of the benchmark.\",\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Portfolio optimization with two quasiconvex risk measures',\n",
       "  'text': 'We study a static portfolio optimization problem with two risk measures: a\\nprinciple risk measure in the objective function and a secondary risk measure\\nwhose value is controlled in the constraints. This problem is of interest when\\nit is necessary to consider the risk preferences of two parties, such as a\\nportfolio manager and a regulator, at the same time. A special case of this\\nproblem where the risk measures are assumed to be coherent (positively\\nhomogeneous) is studied recently in a joint work of the author. The present\\npaper extends the analysis to a more general setting by assuming that the two\\nrisk measures are only quasiconvex.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Portfolio optimization with two quasiconvex risk measures',\n",
       "  'text': 'First, we study the case where the\\nprincipal risk measure is convex. We introduce a dual problem, show that there\\nis zero duality gap between the portfolio optimization problem and the dual\\nproblem, and finally identify a condition under which the Lagrange multiplier\\nassociated to the dual problem at optimality gives an optimal portfolio. Next,\\nwe study the general case without the convexity assumption and show that an\\napproximately optimal solution with prescribed optimality gap can be achieved\\nby using the well-known bisection algorithm combined with a duality result that\\nwe prove.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'High-resolution human mobility data reveal race and wealth disparities\\n  in disaster evacuation patterns',\n",
       "  'text': 'Major disasters such as extreme weather events can magnify and exacerbate\\npre-existing social disparities, with disadvantaged populations bearing\\ndisproportionate costs. Despite the implications for equity and emergency\\nplanning, we lack a quantitative understanding of how these social fault lines\\ntranslate to different behaviors in large-scale emergency contexts. Here we\\ninvestigate this problem in the context of Hurricane Harvey, using over 30\\nmillion anonymized GPS records from over 150,000 opted-in users in the Greater\\nHouston Area to quantify patterns of disaster-inflicted relocation activities\\nbefore, during, and after the shock.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'High-resolution human mobility data reveal race and wealth disparities\\n  in disaster evacuation patterns',\n",
       "  'text': 'We show that evacuation distance is highly\\nhomogenous across individuals from different types of neighborhoods classified\\nby race and wealth, obeying a truncated power-law distribution. Yet here the\\nsimilarities end: we find that both race and wealth strongly impact evacuation\\npatterns, with disadvantaged minority populations less likely to evacuate than\\nwealthier white residents. Finally, there are considerable discrepancies in\\nterms of departure and return times by race and wealth, with strong social\\ncohesion among evacuees from advantaged neighborhoods in their destination\\nchoices. These empirical findings bring new insights into mobility and\\nevacuations, providing policy recommendations for residents, decision makers,\\nand disaster managers alike.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Variance-covariance based risk allocation in credit portfolios:\\n  analytical approximation',\n",
       "  'text': 'High precision analytical approximation is proposed for variance-covariance\\nbased risk allocation in a portfolio of risky assets. A general case of a\\nsingle-period multi-factor Merton-type model with stochastic recovery is\\nconsidered. The accuracy of the approximation as well as its speed are compared\\nto and shown to be superior to those of Monte Carlo simulation.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Long run risk sensitive portfolio with general factors',\n",
       "  'text': 'In the paper portfolio optimization over long run risk sensitive criterion is\\nconsidered. It is assumed that economic factors which stimulate asset prices\\nare ergodic but non necessarily uniformly ergodic. Solution to suitable Bellman\\nequation using local span contraction with weighted norms is shown. The form of\\noptimal strategy is presented and examples of market models satisfying imposed\\nassumptions are shown.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Cash Sub-additive Risk Measures and Interest Rate Ambiguity',\n",
       "  'text': 'A new class of risk measures called cash sub-additive risk measures is\\nintroduced to assess the risk of future financial, nonfinancial and insurance\\npositions. The debated cash additive axiom is relaxed into the cash sub\\nadditive axiom to preserve the original difference between the numeraire of the\\ncurrent reserve amounts and future positions. Consequently, cash sub-additive\\nrisk measures can model stochastic and/or ambiguous interest rates or\\ndefaultable contingent claims. Practical examples are presented and in such\\ncontexts cash additive risk measures cannot be used. Several representations of\\nthe cash sub-additive risk measures are provided.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Cash Sub-additive Risk Measures and Interest Rate Ambiguity',\n",
       "  'text': 'The new risk measures are\\ncharacterized by penalty functions defined on a set of sub-linear probability\\nmeasures and can be represented using penalty functions associated with cash\\nadditive risk measures defined on some extended spaces. The issue of the\\noptimal risk transfer is studied in the new framework using inf-convolution\\ntechniques. Examples of dynamic cash sub-additive risk measures are provided\\nvia BSDEs where the generator can locally depend on the level of the cash\\nsub-additive risk measure.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Multivariate Stop loss Mixed Erlang Reinsurance risk: Aggregation,\\n  Capital allocation and Default risk',\n",
       "  'text': 'In this paper, we address the aggregation of dependent stop loss reinsurance\\nrisks where the dependence among the ceding insurer(s) risks is governed by the\\nSarmanov distribution and each individual risk belongs to the class of Erlang\\nmixtures. We investigate the effects of the ceding insurer(s) risk dependencies\\non the reinsurer risk profile by deriving a closed formula for the distribution\\nfunction of the aggregated stop loss reinsurance risk.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Multivariate Stop loss Mixed Erlang Reinsurance risk: Aggregation,\\n  Capital allocation and Default risk',\n",
       "  'text': 'Furthermore,\\ndiversification effects from aggregating reinsurance risks are examined by\\nderiving a closed expression for the risk capital needed for the whole\\nportfolio of the reinsurer and also the allocated risk capital for each\\nbusiness unit under the TVaR capital allocation principle. Moreover, given the\\nrisk capital that the reinsurer holds, we express the default probability of\\nthe reinsurer analytically. In case the reinsurer is in default, we determine\\nanalytical expressions for the amount of the aggregate reinsured unpaid losses\\nand the unpaid losses of each reinsured line of business of the ceding\\ninsurer(s). These results are illustrated by numerical examples.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Systemic Risk: Conditional Distortion Risk Measures',\n",
       "  'text': 'In this paper, we introduce the rich classes of conditional distortion (CoD)\\nrisk measures and distortion risk contribution ($\\\\Delta$CoD) measures as\\nmeasures of systemic risk and analyze their properties and representations. The\\nclasses include the well-known conditional Value-at-Risk, conditional Expected\\nShortfall, and risk contribution measures in terms of the VaR and ES as special\\ncases. Sufficient conditions are presented for two random vectors to be ordered\\nby the proposed CoD-risk measures and distortion risk contribution measures. These conditions are expressed using the conventional stochastic dominance,\\nincreasing convex/concave, dispersive, and excess wealth orders of the\\nmarginals and canonical positive/negative stochastic dependence notions.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Systemic Risk: Conditional Distortion Risk Measures',\n",
       "  'text': 'Numerical examples are provided to illustrate our theoretical findings. This\\npaper is the second in a triplet of papers on systemic risk by the same\\nauthors. In \\\\cite{DLZorder2018a}, we introduce and analyze some new stochastic\\norders related to systemic risk. In a third (forthcoming) paper, we attribute\\nsystemic risk to the different participants in a given risky environment.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Quantifying Uncertainty in Risk Assessment using Fuzzy Theory',\n",
       "  'text': 'Risk specialists are trying to understand risk better and use complex models\\nfor risk assessment, while many risks are not yet well understood. The lack of\\nempirical data and complex causal and outcome relationships make it difficult\\nto estimate the degree to which certain risk types are exposed. Traditional\\nrisk models are based on classical set theory. In comparison, fuzzy logic\\nmodels are built on fuzzy set theory and are useful for analyzing risks with\\ninsufficient knowledge or inaccurate data. Fuzzy logic systems help to make\\nlarge-scale risk management frameworks more simple.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Quantifying Uncertainty in Risk Assessment using Fuzzy Theory',\n",
       "  'text': \"For risks that do not have\\nan appropriate probability model, a fuzzy logic system can help model the cause\\nand effect relationships, assess the level of risk exposure, rank key risks in\\na consistent way, and consider available data and experts'opinions. Besides, in\\nfuzzy logic systems, some rules explicitly explain the connection, dependence,\\nand relationships between model factors. This can help identify risk mitigation\\nsolutions. Resources can be used to mitigate risks with very high levels of\\nexposure and relatively low hedging costs.\",\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Quantifying Uncertainty in Risk Assessment using Fuzzy Theory',\n",
       "  'text': 'Fuzzy set and fuzzy logic models can\\nbe used with Bayesian and other types of method recognition and decision\\nmodels, including artificial neural networks and decision tree models. These\\ndeveloped models have the potential to solve difficult risk assessment\\nproblems. This research paper explores areas in which fuzzy logic models can be\\nused to improve risk assessment and risk decision making. We will discuss the\\nmethodology, framework, and process of using fuzzy logic systems in risk\\nassessment.',\n",
       "  'meta': {'_split_id': 2}},\n",
       " {'name': 'A Systematic Review of Uncertainties in Software Project Management',\n",
       "  'text': 'It is no secret that many projects fail, regardless of the business sector,\\nsoftware projects are notoriously disaster victims, not necessarily because of\\ntechnological failure, but more often due to their uncertainties. The threats\\nidentified by uncertainty in day-to-day of a project are real and immediate and\\nthe stakes in a project are often high. This paper presents a systematic review\\nabout software project management uncertainties.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'A Systematic Review of Uncertainties in Software Project Management',\n",
       "  'text': 'It helps to identify the\\ndifficulties and the actions that can minimize the uncertainties effects in the\\nprojects and how managers and teams can prepare themselves for the challenges\\nof their projects scenario, with the aim of contributing to the improvement of\\nproject management in organizations as well as contributing to project success.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Applications of Social Media in Hydroinformatics: A Survey',\n",
       "  'text': 'Floods of research and practical applications employ social media data for a\\nwide range of public applications, including environmental monitoring, water\\nresource managing, disaster and emergency response.Hydroinformatics can benefit\\nfrom the social media technologies with newly emerged data, techniques and\\nanalytical tools to handle large datasets, from which creative ideas and new\\nvalues could be mined.This paper first proposes a 4W (What, Why, When, hoW)\\nmodel and a methodological structure to better understand and represent the\\napplication of social media to hydroinformatics, then provides an overview of\\nacademic research of applying social media to hydroinformatics such as water\\nenvironment, water resources, flood, drought and water Scarcity management.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Applications of Social Media in Hydroinformatics: A Survey',\n",
       "  'text': 'At\\nlast,some advanced topics and suggestions of water related social media\\napplications from data collection, data quality management, fake news\\ndetection, privacy issues, algorithms and platforms was present to\\nhydroinformatics managers and researchers based on previous discussion.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Measuring and Managing Carbon Risk in Investment Portfolios',\n",
       "  'text': 'This article studies the impact of carbon risk on stock pricing. To address\\nthis, we consider the seminal approach of G\\\\\"orgen \\\\textsl{et al.} (2019), who\\nproposed estimating the carbon financial risk of equities by their carbon beta. To achieve this, the primary task is to develop a brown-minus-green (or BMG)\\nrisk factor, similar to Fama and French (1992). Secondly, we must estimate the\\ncarbon beta using a multi-factor model. While G\\\\\"orgen \\\\textsl{et al.} (2019)\\nconsidered that the carbon beta is constant, we propose a time-varying\\nestimation model to assess the dynamics of the carbon risk.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Measuring and Managing Carbon Risk in Investment Portfolios',\n",
       "  'text': 'Moreover, we test\\nseveral specifications of the BMG factor to understand which climate\\nchange-related dimensions are priced in by the stock market. In the second part\\nof the article, we focus on the carbon risk management of investment\\nportfolios. First, we analyze how carbon risk impacts the construction of a\\nminimum variance portfolio. As the goal of this portfolio is to reduce\\nunrewarded financial risks of an investment, incorporating the carbon risk into\\nthis approach fulfils this objective. Second, we propose a new framework for\\nbuilding enhanced index portfolios with a lower exposure to carbon risk than\\ncapitalization-weighted stock indices.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Measuring and Managing Carbon Risk in Investment Portfolios',\n",
       "  'text': 'Finally, we explore how carbon\\nsensitivities can improve the robustness of factor investing portfolios.',\n",
       "  'meta': {'_split_id': 2}},\n",
       " {'name': 'Limits of Risk Predictability in a Cascading Alternating Renewal Process\\n  Model',\n",
       "  'text': \"Most risk analysis models systematically underestimate the probability and\\nimpact of catastrophic events (e.g., economic crises, natural disasters, and\\nterrorism) by not taking into account interconnectivity and interdependence of\\nrisks. To address this weakness, we propose the Cascading Alternating Renewal\\nProcess (CARP) to forecast interconnected global risks. However, assessments of\\nthe model's prediction precision are limited by lack of sufficient ground truth\\ndata. Here, we establish prediction precision as a function of input data size\\nby using alternative long ground truth data generated by simulations of the\\nCARP model with known parameters.\",\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Limits of Risk Predictability in a Cascading Alternating Renewal Process\\n  Model',\n",
       "  'text': 'We illustrate the approach on a model of\\nfires in artificial cities assembled from basic city blocks with diverse\\nhousing. The results confirm that parameter recovery variance exhibits power\\nlaw decay as a function of the length of available ground truth data. Using\\nCARP, we also demonstrate estimation using a disparate dataset that also has\\ndependencies: real-world prediction precision for the global risk model based\\non the World Economic Forum Global Risk Report. We conclude that the CARP model\\nis an efficient method for predicting catastrophic cascading events with\\npotential applications to emerging local and global interconnected risks.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'A Practical Approach to Managing Spreadsheet Risk in a Global Business',\n",
       "  'text': \"Spreadsheets are used extensively within today's organisations. Although\\nspreadsheets have many benefits, they can also present a significant risk\\nexposure, requiring appropriate management. Protiviti has worked with a number\\nof organisations, ranging in size up to huge multi-nationals, to help them\\nbuild appropriate spreadsheet governance frameworks, including the design and\\nimplementation of policies, minimum design standards, control processes,\\ntraining and awareness programmes and the consideration and implementation of\\nspreadsheet management tools. This paper presents a case-study explaining the\\npractical and pragmatic approach that was recently taken to control spreadsheet\\nrisk at one of Protiviti's clients - a global energy firm.\",\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Mean-Variance Asset-Liability Management with State-Dependent Risk\\n  Aversion',\n",
       "  'text': 'In this paper, we consider the asset-liability management under the\\nmean-variance criterion. The financial market consists of a risk-free bond and\\na stock whose price process is modeled by a geometric Brownian motion. The\\nliability of the investor is uncontrollable and is modeled by another geometric\\nBrownian motion. We consider a specific state-dependent risk aversion which\\ndepends on a power function of the liability. By solving a flow of FBSDEs with\\nbivariate state process, we obtain the equilibrium strategy among all the\\nopen-loop controls for this time-inconsistent control problem. It shows that\\nthe equilibrium strategy is a feedback control of the liability.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Anticipatory Thinking: A Metacognitive Capability',\n",
       "  'text': 'Anticipatory thinking is a complex cognitive process for assessing and\\nmanaging risk in many contexts. Humans use anticipatory thinking to identify\\npotential future issues and proactively take actions to manage their risks. In\\nthis paper we define a cognitive systems approach to anticipatory thinking as a\\nmetacognitive goal reasoning mechanism. The contributions of this paper include\\n(1) defining anticipatory thinking in the MIDCA cognitive architecture, (2)\\noperationalizing anticipatory thinking as a three step process for managing\\nrisk in plans, and (3) a numeric risk assessment calculating an expected\\ncost-benefit ratio for modifying a plan with anticipatory actions.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Trade-offs and synergies in managing coastal flood risk: A case study\\n  for New York City',\n",
       "  'text': 'Decisions on how to manage future flood risks are frequently informed by both\\nsophisticated and computationally expensive models. This complexity often\\nlimits the representation of uncertainties and the consideration of strategies. Here, we use an intermediate complexity model framework that enables us to\\nanalyze a rich set of strategies, objectives, and uncertainties. We find that\\nallowing for more combinations of risk mitigation strategies can expand the\\nsolution set, help explain synergies and trade-offs, and point to strategies\\nthat can improve outcomes.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Adaptive Bernstein Copulas and Risk Management',\n",
       "  'text': 'We present a constructive approach to Bernstein copulas with an admissible\\ndiscrete skeleton in arbitrary dimensions when the underlying marginal grid\\nsizes are smaller than the number of observations. This prevents an overfitting\\nof the estimated dependence model and reduces the simulation effort for\\nBernstein copulas a lot. In a case study, we compare different approaches of\\nBernstein and Gaussian copulas w.r.t. the estimation of risk measures in risk\\nmanagement.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Introduction to Risk Parity and Budgeting',\n",
       "  'text': \"Although portfolio management didn't change much during the 40 years after\\nthe seminal works of Markowitz and Sharpe, the development of risk budgeting\\ntechniques marked an important milestone in the deepening of the relationship\\nbetween risk and asset management. Risk parity then became a popular financial\\nmodel of investment after the global financial crisis in 2008. Today, pension\\nfunds and institutional investors are using this approach in the development of\\nsmart indexing and the redefinition of long-term investment policies. Introduction to Risk Parity and Budgeting provides an up-to-date treatment of\\nthis alternative method to Markowitz optimization.\",\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Introduction to Risk Parity and Budgeting',\n",
       "  'text': 'It builds financial exposure\\nto equities and commodities, considers credit risk in the management of bond\\nportfolios, and designs long-term investment policy. This book contains the solutions of tutorial exercices which are included in\\nIntroduction to Risk Parity and Budgeting.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'A General Duality Relation with Applications in Quantitative Risk\\n  Management',\n",
       "  'text': 'A fundamental problem in risk management is the robust aggregation of\\ndifferent sources of risk in a situation where little or no data are available\\nto infer information about their dependencies. A popular approach to solving\\nthis problem is to formulate an optimization problem under which one maximizes\\na risk measure over all multivariate distributions that are consistent with the\\navailable data. In several special cases of such models, there exist dual\\nproblems that are easier to solve or approximate, yielding robust bounds on the\\naggregated risk.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'A General Duality Relation with Applications in Quantitative Risk\\n  Management',\n",
       "  'text': 'In this chapter we formulate a general optimization problem,\\nwhich can be seen as a doubly infinite linear programming problem, and we show\\nthat the associated dual generalizes several well known special cases and\\nextends to new risk management models we propose.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'An Introduction to Hedge Funds',\n",
       "  'text': 'This report was originally written as an industry white paper on Hedge Funds. This paper gives an overview to Hedge Funds, with a focus on risk management\\nissues. We define and explain the general characteristics of Hedge Funds, their\\nmain investment strategies and the risk models employed. We address the\\nproblems in Hedge Fund modelling, survey current Hedge Funds available on the\\nmarket and those that have been withdrawn. Finally, we summarise the supporting\\nand opposing arguments for Hedge Fund usage.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'An Introduction to Hedge Funds',\n",
       "  'text': 'A unique value of this paper,\\ncompared to other Hedge Fund literature freely available on the internet, is\\nthat this review is fully sourced from academic references (such as peer\\nreviewed journals) and is thus a bona fide study. This paper will be of\\ninterest to: Hedge Fund and Mutual Fund Managers, Quantitative Analysts,\\n\"Front\" and \"Middle\" office banking functions e.g. Treasury Management,\\nRegulators concerned with Hedge Fund Financial Risk Management, Private and\\nInstitutional Investors, Academic Researchers in the area of Financial Risk\\nManagement and the general Finance community.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Risk management in design process for Factory of the future',\n",
       "  'text': 'The current globalization is faced to the rapid development of product design\\nprocess with the different structure of the actor relationships in the process. Currently, the risk in the failure relationship among different actors in the\\nproject is shaped by the complexity towards the future all kinds of challenges. When it comes to the interdependent failure effect, the risk management for\\nfuture organization structure in design process will be much more complex to\\ngrasp. In order to cope with adaption of Product-Process-Organization (P-P-O)\\nmodel for industry of the future, we propose a risk management methodology to\\ncope with this interdependent relationship structure.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Risk management in design process for Factory of the future',\n",
       "  'text': \"The main objective of\\nthis research is to manage the risks, so that the project manager can find the\\npriority order of all the actors' total effect to the project with the\\nconsideration of interdependent failure affection, and according to the order,\\nproject manager can release corresponding respond measures.\",\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Risks of Large Portfolios',\n",
       "  'text': 'Estimating and assessing the risk of a large portfolio is an important topic\\nin financial econometrics and risk management. The risk is often estimated by a\\nsubstitution of a good estimator of the volatility matrix. However, the\\naccuracy of such a risk estimator for large portfolios is largely unknown, and\\na simple inequality in the previous literature gives an infeasible upper bound\\nfor the estimation error. In addition, numerical studies illustrate that this\\nupper bound is very crude. In this paper, we propose factor-based risk\\nestimators under a large amount of assets, and introduce a high-confidence\\nlevel upper bound (H-CLUB) to assess the accuracy of the risk estimation.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Risks of Large Portfolios',\n",
       "  'text': 'The\\nH-CLUB is constructed based on three different estimates of the volatility\\nmatrix: sample covariance, approximate factor model with known factors, and\\nunknown factors (POET, Fan, Liao and Mincheva, 2013). For the first time in the\\nliterature, we derive the limiting distribution of the estimated risks in high\\ndimensionality. Our numerical results demonstrate that the proposed upper\\nbounds significantly outperform the traditional crude bounds, and provide\\ninsightful assessment of the estimation of the portfolio risks. In addition,\\nour simulated results quantify the relative error in the risk estimation, which\\nis usually negligible using 3-month daily data. Finally, the proposed methods\\nare applied to an empirical study.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Estimation of Risk Contributions with MCMC',\n",
       "  'text': 'Determining risk contributions of unit exposures to portfolio-wide economic\\ncapital is an important task in financial risk management. Computing risk\\ncontributions involves difficulties caused by rare-event simulations. In this\\nstudy, we address the problem of estimating risk contributions when the total\\nrisk is measured by value-at-risk (VaR). Our proposed estimator of VaR\\ncontributions is based on the Metropolis-Hasting (MH) algorithm, which is one\\nof the most prevalent Markov chain Monte Carlo (MCMC) methods. Unlike existing\\nestimators, our MH-based estimator consists of samples from conditional loss\\ndistribution given a rare event of interest. This feature enhances sample\\nefficiency compared with the crude Monte Carlo method.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Estimation of Risk Contributions with MCMC',\n",
       "  'text': 'Moreover, our method has\\nthe consistency and asymptotic normality, and is widely applicable to various\\nrisk models having joint loss density. Our numerical experiments based on\\nsimulation and real-world data demonstrate that in various risk models, even\\nthose having high-dimensional (approximately 500) inhomogeneous margins, our MH\\nestimator has smaller bias and mean squared error compared with existing\\nestimators.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'The Climate Extended Risk Model (CERM)',\n",
       "  'text': 'This paper is directed to the financial community and focuses on the\\nfinancial risks associated with climate change. It, specifically, addresses the\\nestimate of climate risk embedded within a bank loan portfolio. During the 21st\\ncentury, man-made carbon dioxide emissions in the atmosphere will raise global\\ntemperatures, resulting in severe and unpredictable physical damage across the\\nglobe. Another uncertainty associated with climate, known as the energy\\ntransition risk, comes from the unpredictable pace of political and legal\\nactions to limit its impact. The Climate Extended Risk Model (CERM) adapts well\\nknown credit risk models.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'The Climate Extended Risk Model (CERM)',\n",
       "  'text': 'It proposes a method to calculate incremental credit\\nlosses on a loan portfolio that are rooted into physical and transition risks. The document provides detailed description of the model hypothesis and steps. This work was initiated by the association Green RWA (Risk Weighted Assets). It\\nwas written in collaboration with Jean-Baptiste Gaudemet, Anne Gruz, and\\nOlivier Vinciguerra (cerm@greenrwa.org), who contributed their financial and\\nrisk expertise, taking care of its application to a pilot-portfolio. It extends\\nthe model proposed in a first white paper published by Green RWA\\n(https://www.greenrwa.org/).',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Spectral Risk Measures with an Application to Futures Clearinghouse\\n  Variation Margin Requirements',\n",
       "  'text': \"This paper applies an AR(1)-GARCH (1, 1) process to detail the conditional\\ndistributions of the return distributions for the S&P500, FT100, DAX, Hang\\nSeng, and Nikkei225 futures contracts. It then uses the conditional\\ndistribution for these contracts to estimate spectral risk measures, which are\\ncoherent risk measures that reflect a user's risk-aversion function. It\\ncompares these to more familiar VaR and Expected Shortfall (ES) measures of\\nrisk, and also compares the precision and discusses the relative usefulness of\\neach of these risk measures in setting variation margins that incorporate\\ntime-varying market conditions. The goodness of fit of the model is confirmed\\nby a variety of backtests.\",\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Evaluating the Precision of Estimators of Quantile-Based Risk Measures',\n",
       "  'text': 'This paper examines the precision of estimators of Quantile-Based Risk\\nMeasures (Value at Risk, Expected Shortfall, Spectral Risk Measures). It first\\naddresses the question of how to estimate the precision of these estimators,\\nand proposes a Monte Carlo method that is free of some of the limitations of\\nexisting approaches. It then investigates the distribution of risk estimators,\\nand presents simulation results suggesting that the common practice of relying\\non asymptotic normality results might be unreliable with the sample sizes\\ncommonly available to them.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Evaluating the Precision of Estimators of Quantile-Based Risk Measures',\n",
       "  'text': 'Finally, it investigates the relationship between\\nthe precision of different risk estimators and the distribution of underlying\\nlosses (or returns), and yields a number of useful conclusions.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'CRRA Utility Maximization under Risk Constraints',\n",
       "  'text': 'This paper studies the problem of optimal investment with CRRA (constant,\\nrelative risk aversion) preferences, subject to dynamic risk constraints on\\ntrading strategies. The market model considered is continuous in time and\\nincomplete. the prices of financial assets are modeled by It\\\\^o processes. The\\ndynamic risk constraints, which are time and state dependent, are generated by\\nrisk measures. Optimal trading strategies are characterized by a quadratic\\nBSDE. Within the class of \\\\textit{time consistent distortion risk measures}, a\\nthree-fund separation result is established. Numerical results emphasize the\\neffects of imposing risk constraints on trading.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Risk minimizing of derivatives via dynamic g-expectation and related\\n  topics',\n",
       "  'text': 'In this paper, we investigate risk minimization problem of derivatives based\\non non-tradable underlyings by means of dynamic g-expectations which are slight\\ndifferent from conditional g-expectations. In this framework, inspired by [1]\\nand [16], we introduce risk indifference price, marginal risk price and\\nderivative hedge and obtain their corresponding explicit expressions. The\\ninteresting thing is that their expressions have nothing to do with nonlinear\\ngenerator g, and one deep reason for this is due to the completeness of\\nfinancial market.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Risk minimizing of derivatives via dynamic g-expectation and related\\n  topics',\n",
       "  'text': 'By giving three useful special risk minimization problems, we\\nobtain the explicit optimal strategies with initial wealth involved,\\ndemonstrate some qualitative analysis among optimal strategies, risk aversion\\nparameter and market price of risk, together with some economic\\ninterpretations.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'A new approach for an unitary risk theory',\n",
       "  'text': 'The work deals with the risk assessment theory. An unitary risk algorithm is\\nelaborated. The algorithm is based on parallel curves. The basic curve of risk\\nis a hyperbolic curve, obtained as a multiplication between the probability of\\noccurrence of certain event and its impact. Section 1 contains the problem\\nformulation. Section 2 contains some specific notations and the mathematical\\nbackground of risk algorithm. A numerical application based on risk algorithm\\nis the content of section 3. Section 4 contains several conclusions.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Distortion Risk Measures and Elicitability',\n",
       "  'text': 'We discuss equivalent axiomatic characterizations of distortion risk\\nmeasures, and give a novel and concise proof of the characterization of\\nelicitable distortion risk measures. Elicitability has recently been discussed\\nas a desirable criterion for risk measures, motivated by statistical\\nconsiderations of forecasting. We reveal the mathematical conflict between the\\nrequirements of elicitability and comonotonic additivity which intuitively\\nexplains why only Value-at-Risk and the mean are elicitable distortion risk\\nmeasures in a general sense.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Multiple risk factor dependence structures: Distributional properties',\n",
       "  'text': 'We introduce a class of dependence structures, that we call the Multiple Risk\\nFactor (MRF) dependence structures. On the one hand, the new constructions\\nextend the popular CreditRisk+ approach, and as such they formally describe\\ndefault risk portfolios exposed to an arbitrary number of fatal risk factors\\nwith conditionally exponential and dependent hitting (or occurrence) times. On\\nthe other hand, the MRF structures can be seen as an encompassing family of\\nmultivariate probability distributions with univariate margins distributed\\nPareto of the 2nd kind, and in this role they can be used to model insurance\\nrisk portfolios of dependent and heavy tailed risk components.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Counterparty Risk FAQ: Credit VaR, PFE, CVA, DVA, Closeout, Netting,\\n  Collateral, Re-hypothecation, WWR, Basel, Funding, CCDS and Margin Lending',\n",
       "  'text': 'We present a dialogue on Counterparty Credit Risk touching on Credit Value at\\nRisk (Credit VaR), Potential Future Exposure (PFE), Expected Exposure (EE),\\nExpected Positive Exposure (EPE), Credit Valuation Adjustment (CVA), Debit\\nValuation Adjustment (DVA), DVA Hedging, Closeout conventions, Netting clauses,\\nCollateral modeling, Gap Risk, Re-hypothecation, Wrong Way Risk, Basel III,\\ninclusion of Funding costs, First to Default risk, Contingent Credit Default\\nSwaps (CCDS) and CVA restructuring possibilities through margin lending. The\\ndialogue is in the form of a Q&A between a CVA expert and a newly hired\\ncolleague.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Systematic and non-systematic mortality risk in pension portfolios',\n",
       "  'text': 'We study the effects of non-systematic and systematic mortality risks on the\\nrequired initial capital in a pension plan, in the presence of financial risks. We discover that for a pension plan with few members the impact of pooling on\\nthe required capital per person is strong, but non-systematic risk diminishes\\nrapidly as the number of members increases. Systematic mortality risk, on the\\nother hand, is a significant source of risk is a pension portfolio.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Dynamic Interaction Between Asset Prices and Bank Behavior: A Systemic\\n  Risk Perspective',\n",
       "  'text': 'Systemic risk in banking systems remains a crucial issue that it has not been\\ncompletely understood. In our toy model, banks are exposed to two sources of\\nrisks, namely, market risk from their investments in assets external to the\\nbanking system and credit risk from their lending in the interbank market. By\\nand large, both risks increase during severe financial turmoil. Under this\\nscenario, the paper shows the conditions under which both the individual and\\nthe systemic default tend to coincide.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Quasiconvex risk measures with markets volatility',\n",
       "  'text': 'Since the quasiconvex risk measures is a bigger class than the well known\\nconvex risk measures, the study of quasiconvex risk measures makes sense\\nespecially in the financial markets with volatility. In this paper, we will\\nstudy the quasiconvex risk measures defined on a special space $L^{p(\\\\cdot)}$\\nwhere the variable exponent $p(\\\\cdot)$ is no longer a given real number like\\nthe space $L^{p}$, but a random variable, which reflects the possible\\nvolatility of the financial markets. The dual representation for this\\nquasiconvex risk measures will also provided.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Model Risk Analysis via Investment Structuring',\n",
       "  'text': '\"What are the origins of risks?\" and \"How material are they?\" -- these are\\nthe two most fundamental questions of any risk analysis. Quantitative\\nStructuring -- a technology for building financial products -- provides\\neconomically meaningful answers for both of these questions. It does so by\\nconsidering risk as an investment opportunity. The structure of the investment\\nreveals the precise sources of risk and its expected performance measures\\nmateriality. We demonstrate these capabilities of Quantitative Structuring\\nusing a concrete practical example -- model risk in options on vol-targeted\\nindices.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'A Supermartingale Relation for Multivariate Risk Measures',\n",
       "  'text': 'The equivalence between multiportfolio time consistency of a dynamic\\nmultivariate risk measure and a supermartingale property is proven. Furthermore, the dual variables under which this set-valued supermartingale is\\na martingale are characterized as the worst-case dual variables in the dual\\nrepresentation of the risk measure. Examples of multivariate risk measures\\nsatisfying the supermartingale property are given. Crucial for obtaining the\\nresults are dual representations of scalarizations of set-valued dynamic risk\\nmeasures, which are of independent interest in the fast growing literature on\\nmultivariate risks.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Concentration of dynamic risk measures in a Brownian filtration',\n",
       "  'text': 'Motivated by liquidity risk in mathematical finance, D. Lacker introduced\\nconcentration inequalities for risk measures, i.e. upper bounds on the\\n\\\\emph{liquidity risk profile} of a financial loss. We derive these inequalities\\nin the case of time-consistent dynamic risk measures when the filtration is\\nassumed to carry a Brownian motion. The theory of backward stochastic\\ndifferential equations (BSDEs) and their dual formulation plays a crucial role\\nin our analysis. Natural by-products of concentration of risk measures are a\\ndescription of the tail behavior of the financial loss and transport-type\\ninequalities in terms of the generator of the BSDE, which in the present case\\ncan grow arbitrarily fast.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Systemic Greeks: Measuring risk in financial networks',\n",
       "  'text': 'Since the latest financial crisis, the idea of systemic risk has received\\nconsiderable interest. In particular, contagion effects arising from\\ncross-holdings between interconnected financial firms have been studied\\nextensively. Drawing inspiration from the field of complex networks, these\\nattempts are largely unaware of models and theories for credit risk of\\nindividual firms. Here, we note that recent network valuation models extend the\\nseminal structural risk model of Merton (1974). Furthermore, we formally\\ncompute sensitivities to various risk factors -- commonly known as Greeks -- in\\na network context. In particular, we propose the network $\\\\Delta$ as a\\nquantitative measure of systemic risk and illustrate our findings on some\\nnumerical examples.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Risk portofolio management under Zipf analysis based strategies',\n",
       "  'text': 'A so called Zipf analysis portofolio management technique is introduced in\\norder to comprehend the risk and returns. Two portofoios are built each from a\\nwell known financial index. The portofolio management is based on two\\napproaches: one called the \"equally weighted portofolio\", the other the\\n\"confidence parametrized portofolio\". A discussion of the (yearly) expected\\nreturn, variance, Sharpe ratio and $\\\\beta$ follows. Optimization levels of high\\nreturns or low risks are found.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Shrinkage = Factor Model',\n",
       "  'text': 'Shrunk sample covariance matrix is a factor model of a special form combining\\nsome (typically, style) risk factor(s) and principal components with a\\n(block-)diagonal factor covariance matrix. As such, shrinkage, which\\nessentially inherits out-of-sample instabilities of the sample covariance\\nmatrix, is not an alternative to multifactor risk models but one out of myriad\\npossible regularization schemes. We give an example of a scheme designed to be\\nless prone to said instabilities. We contextualize this within multifactor\\nmodels.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Dead Alphas as Risk Factors',\n",
       "  'text': 'We give an explicit algorithm and source code for extracting equity risk\\nfactors from dead (a.k.a. \"flatlined\" or \"hockey-stick\") alphas and using them\\nto improve performance characteristics of good (tradable) alphas. In a\\nnutshell, we use dead alphas to extract directions in the space of stock\\nreturns along which there is no money to be made (and/or those bets are too\\nvolatile). In practice the number of dead alphas can be large compared with the\\nnumber of underlying stocks and care is required in identifying the aforesaid\\ndirections.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Dynamic Contract Design for Systemic Cyber Risk Management of\\n  Interdependent Enterprise Networks',\n",
       "  'text': 'The interconnectivity of cyber and physical systems and Internet of things\\nhas created ubiquitous concerns of cyber threats for enterprise system\\nmanagers. It is common that the asset owners and enterprise network operators\\nneed to work with cybersecurity professionals to manage the risk by\\nremunerating them for their efforts that are not directly observable. In this\\npaper, we use a principal-agent framework to capture the service relationships\\nbetween the two parties, i.e., the asset owner (principal) and the cyber risk\\nmanager (agent).',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Dynamic Contract Design for Systemic Cyber Risk Management of\\n  Interdependent Enterprise Networks',\n",
       "  'text': 'Specifically, we consider a dynamic systemic risk management\\nproblem with asymmetric information where the principal can only observe cyber\\nrisk outcomes of the enterprise network rather than directly the efforts that\\nthe manager expends on protecting the resources. Under this information\\npattern, the principal aims to minimize the systemic cyber risks by designing a\\ndynamic contract that specifies the compensation flows and the anticipated\\nefforts of the manager by taking into account his incentives and rational\\nbehaviors. We formulate a bi-level mechanism design problem for dynamic\\ncontract design within the framework of a class of stochastic differential\\ngames.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Dynamic Contract Design for Systemic Cyber Risk Management of\\n  Interdependent Enterprise Networks',\n",
       "  'text': \"We show that the principal has rational controllability of the systemic\\nrisk by designing an incentive compatible estimator of the agent's hidden\\nefforts. We characterize the optimal solution by reformulating the problem as a\\nstochastic optimal control program which can be solved using dynamic\\nprogramming. We further investigate a benchmark scenario with complete\\ninformation and identify conditions that yield zero information rent and lead\\nto a new certainty equivalence principle for principal-agent problems. Finally,\\ncase studies over networked systems are carried out to illustrate the\\ntheoretical results obtained.\",\n",
       "  'meta': {'_split_id': 2}},\n",
       " {'name': 'Graphical Visualization of Risk Assessment for Effective Risk Management\\n  during Software Development Process',\n",
       "  'text': 'Success of any IT industry depends on the success rate of their projects,\\nwhich in turn depends on several factors such as cost, time, and availability\\nof resources. These factors formulate the risk areas, which needs to be\\naddressed in a proactive way. The rudimentary objective of risk management is\\nto circumvent the possibility of their occurrence by identifying the risks,\\npreparing the contingency plans and mitigation plans in order to reduce the\\nconsequences of the risks. Hence, effective risk management becomes one of the\\nimperative challenges in any organization, which if deemed in an apt way\\nassures the continued sustainability of the organization in the high-end\\ncompetitive environment.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Graphical Visualization of Risk Assessment for Effective Risk Management\\n  during Software Development Process',\n",
       "  'text': 'This paper provides visualization of risk assessment\\nthrough a graphical model. Further, the matrix representation of the risk\\nassessment aids the project personnel to identify all the risks, comprehend\\ntheir frequency and probability of their occurrence. In addition, the graphical\\nmodel enables one to analyze the impact of identified risks and henceforth to\\nassign their priorities. This mode of representation of risk assessment factors\\nhelps the organization in accurate prediction of success rate of the project.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'What is the Minimal Systemic Risk in Financial Exposure Networks?',\n",
       "  'text': 'Management of systemic risk in financial markets is traditionally associated\\nwith setting (higher) capital requirements for market participants. There are\\nindications that while equity ratios have been increased massively since the\\nfinancial crisis, systemic risk levels might not have lowered, but even\\nincreased. It has been shown that systemic risk is to a large extent related to\\nthe underlying network topology of financial exposures. A natural question\\narising is how much systemic risk can be eliminated by optimally rearranging\\nthese networks and without increasing capital requirements. Overlapping\\nportfolios with minimized systemic risk which provide the same market\\nfunctionality as empirical ones have been studied by [pichler2018].',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'What is the Minimal Systemic Risk in Financial Exposure Networks?',\n",
       "  'text': 'Here we\\npropose a similar method for direct exposure networks, and apply it to\\ncross-sectional interbank loan networks, consisting of 10 quarterly\\nobservations of the Austrian interbank market. We show that the suggested\\nframework rearranges the network topology, such that systemic risk is reduced\\nby a factor of approximately 3.5, and leaves the relevant economic features of\\nthe optimized network and its agents unchanged. The presented optimization\\nprocedure is not intended to actually re-configure interbank markets, but to\\ndemonstrate the huge potential for systemic risk management through rearranging\\nexposure networks, in contrast to increasing capital requirements that were\\nshown to have only marginal effects on systemic risk [poledna2017].',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'What is the Minimal Systemic Risk in Financial Exposure Networks?',\n",
       "  'text': 'Ways to\\nactually incentivize a self-organized formation toward optimal network\\nconfigurations were introduced in [thurner2013] and [poledna2016]. For\\nregulatory policies concerning financial market stability the knowledge of\\nminimal systemic risk for a given economic environment can serve as a benchmark\\nfor monitoring actual systemic risk in markets.',\n",
       "  'meta': {'_split_id': 2}},\n",
       " {'name': 'From Static to Dynamic Prediction: Wildfire Risk Assessment Based on\\n  Multiple Environmental Factors',\n",
       "  'text': 'Wildfire is one of the biggest disasters that frequently occurs on the west\\ncoast of the United States. Many efforts have been made to understand the\\ncauses of the increases in wildfire intensity and frequency in recent years. In\\nthis work, we propose static and dynamic prediction models to analyze and\\nassess the areas with high wildfire risks in California by utilizing a\\nmultitude of environmental data including population density, Normalized\\nDifference Vegetation Index (NDVI), Palmer Drought Severity Index (PDSI), tree\\nmortality area, tree mortality number, and altitude. Moreover, we focus on a\\nbetter understanding of the impacts of different factors so as to inform\\npreventive actions.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'From Static to Dynamic Prediction: Wildfire Risk Assessment Based on\\n  Multiple Environmental Factors',\n",
       "  'text': 'To validate our models and findings, we divide the land of\\nCalifornia into 4,242 grids of 0.1 degrees $\\\\times$ 0.1 degrees in latitude and\\nlongitude, and compute the risk of each grid based on spatial and temporal\\nconditions. By performing counterfactual analysis, we uncover the effects of\\nseveral possible methods on reducing the number of high risk wildfires. Taken\\ntogether, our study has the potential to estimate, monitor, and reduce the\\nrisks of wildfires across diverse areas provided that such environment data is\\navailable.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Closed-form solutions for worst-case law invariant risk measures with\\n  application to robust portfolio optimization',\n",
       "  'text': 'Worst-case risk measures refer to the calculation of the largest value for\\nrisk measures when only partial information of the underlying distribution is\\navailable. For the popular risk measures such as Value-at-Risk (VaR) and\\nConditional Value-at-Risk (CVaR), it is now known that their worst-case\\ncounterparts can be evaluated in closed form when only the first two moments\\nare known for the underlying distribution. These results are remarkable since\\nthey not only simplify the use of worst-case risk measures but also provide\\ngreat insight into the connection between the worst-case risk measures and\\nexisting risk measures.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Closed-form solutions for worst-case law invariant risk measures with\\n  application to robust portfolio optimization',\n",
       "  'text': 'We show in this paper that somewhat surprisingly\\nsimilar closed-form solutions also exist for the general class of law invariant\\ncoherent risk measures, which consists of spectral risk measures as special\\ncases that are arguably the most important extensions of CVaR. We shed light on\\nthe one-to-one correspondence between a worst-case law invariant risk measure\\nand a worst-case CVaR (and a worst-case VaR), which enables one to carry over\\nthe development of worst-case VaR in the context of portfolio optimization to\\nthe worst-case law invariant risk measures immediately.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'A Non-Gaussian Approach to Risk Measures',\n",
       "  'text': 'Reliable calculations of financial risk require that the fat-tailed nature of\\nprices changes is included in risk measures. To this end, a non-Gaussian\\napproach to financial risk management is presented, modeling the power-law\\ntails of the returns distribution in terms of a Student-t distribution. Non-Gaussian closed-form solutions for Value-at-Risk and Expected Shortfall are\\nobtained and standard formulae known in the literature under the normality\\nassumption are recovered as a special case.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'A Non-Gaussian Approach to Risk Measures',\n",
       "  'text': 'The implications of the approach\\nfor risk management are demonstrated through an empirical analysis of financial\\ntime series from the Italian stock market and in comparison with the results of\\nthe most widely used procedures of quantitative finance. Particular attention\\nis paid to quantify the size of the errors affecting the market risk measures\\nobtained according to different methodologies, by employing a bootstrap\\ntechnique.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Any Regulation of Risk Increases Risk',\n",
       "  'text': 'We show that any objective risk measurement algorithm mandated by central\\nbanks for regulated financial entities will result in more risk being taken on\\nby those financial entities than would otherwise be the case. Furthermore, the\\nrisks taken on by the regulated financial entities are far more systemically\\nconcentrated than they would have been otherwise, making the entire financial\\nsystem more fragile.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Any Regulation of Risk Increases Risk',\n",
       "  'text': 'This result leaves three directions for the future of\\nfinancial regulation: continue regulating by enforcing risk measurement\\nalgorithms at the cost of occasional severe crises, regulate more severely and\\nsubjectively by fully nationalizing all financial entities, or abolish all\\ncentral banking regulations including deposit insurance to let risk be\\ndetermined by the entities themselves and, ultimately, by their depositors\\nthrough voluntary market transactions rather than by the taxpayers through\\nenforced government participation.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Macrostate Parameter, an Econophysics Approach for the Risk Analysis of\\n  the Stock Exchange Market Transactions',\n",
       "  'text': 'In this paper we attempt to introduce an econophysics approach to evaluate\\nsome aspects of the risks in financial markets. For this purpose, the\\nthermodynamical methods and statistical physics results about entropy and\\nequilibrium states in the physical systems are used. Some considerations on\\neconomic value and financial information are made. Finally, on this basis, a\\nnew index for the financial risk estimation of the stock-exchange market\\ntransactions, named macrostate parameter, was introduced and discussed. Keywords: econophysics, stock-exchange markets, financial risk, informational\\nfascicle, entropy, macrostate parameter.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Analytical Framework for Credit Portfolios. Part I: Systematic Risk',\n",
       "  'text': 'Analytical, free of time consuming Monte Carlo simulations, framework for\\ncredit portfolio systematic risk metrics calculations is presented. Techniques\\nare described that allow calculation of portfolio-level systematic risk\\nmeasures (standard deviation, VaR and Expected Shortfall) as well as allocation\\nof risk down to individual transactions. The underlying model is the industry\\nstandard multi-factor Merton-type model with arbitrary valuation function at\\nhorizon (in contrast to the simplistic default-only case). High accuracy of the\\nproposed analytical technique is demonstrated by benchmarking against Monte\\nCarlo simulations.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Financial Risks and the Pension Protection Fund: Can it Survive Them?',\n",
       "  'text': 'This paper discusses the financial risks faced by the UK Pension Protection\\nFund (PPF) and what, if anything, it can do about them. It draws lessons from\\nthe regulatory regimes under which other financial institutions, such as banks\\nand insurance companies, operate and asks why pension funds are treated\\ndifferently. It also reviews the experience with other government-sponsored\\ninsurance schemes, such as the US Pension Benefit Guaranty Corporation, upon\\nwhich the PPF is modelled. We conclude that the PPF will live under the\\npermanent risk of insolvency as a consequence of the moral hazard, adverse\\nselection, and, especially, systemic risks that it faces.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'A Goal Programming Model with Satisfaction Function for Risk Management\\n  and Optimal Portfolio Diversification',\n",
       "  'text': 'We extend the classical risk minimization model with scalar risk measures to\\nthe general case of set-valued risk measures. The problem we obtain is a\\nset-valued optimization model and we propose a goal programming-based approach\\nwith satisfaction function to obtain a solution which represents the best\\ncompromise between goals and the achievement levels. Numerical examples are\\nprovided to illustrate how the method works in practical situations.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'An ergodic BSDE approach to forward entropic risk measures:\\n  representation and large-maturity behavior',\n",
       "  'text': 'Using elements from the theory of ergodic backward stochastic differential\\nequations (BSDE), we study the behavior of forward entropic risk measures. We\\nprovide their general representation results (via both BSDE and convex duality)\\nand examine their behavior for risk positions of long maturities. We show that\\nforward entropic risk measures converge to some constant exponentially fast. We\\nalso compare them with their classical counterparts and derive a parity result.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Value-at-Risk and backtesting with the APARCH model and the standardized\\n  Pearson type IV distribution',\n",
       "  'text': 'We examine the efficiency of the Asymmetric Power ARCH (APARCH) model in the\\ncase where the residuals follow the standardized Pearson type IV distribution. The model is tested with a variety of loss functions and the efficiency is\\nexamined via application of several statistical tests and risk measures. The\\nresults indicate that the APARCH model with the standardized Pearson type IV\\ndistribution is accurate, within the general financial risk modeling\\nperspective, providing the financial analyst with an additional skewed\\ndistribution for incorporation in the risk management tools.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Replica Approach for Minimal Investment Risk with Cost',\n",
       "  'text': 'In the present work, the optimal portfolio minimizing the investment risk\\nwith cost is discussed analytically, where this objective function is\\nconstructed in terms of two negative aspects of investment, the risk and cost. We note the mathematical similarity between the Hamiltonian in the\\nmean-variance model and the Hamiltonians in the Hopfield model and the\\nSherrington{Kirkpatrick model and show that we can analyze this portfolio\\noptimization problem by using replica analysis, and derive the minimal\\ninvestment risk with cost and the investment concentration of the optimal\\nportfolio. Furthermore, we validate our proposed method through numerical\\nsimulations.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Visualizing Treasury Issuance Strategy',\n",
       "  'text': 'We introduce simple cost and risk proxy metrics that can be attached to\\nTreasury issuance strategy to complement analysis of the resulting portfolio\\nweighted-average maturity (WAM). These metrics are based on mapping issuance\\nfractions to their long-term, asymptotic portfolio implications for cost and\\nrisk under mechanical debt-rolling dynamics. The resulting mapping enables one\\nto visualize tradeoffs involved in contemplated issuance reallocation, and\\nidentify an efficient frontier and optimal tenor. Historical Treasury issuance\\nstrategy is analyzed empirically using these cost and risk metrics to\\nillustrate how changes in issuance needs and strategy have translated into\\nstructural shifts in the cost and risk stance of Treasury issuance.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'RACORN-K: Risk-Aversion Pattern Matching-based Portfolio Selection',\n",
       "  'text': 'Portfolio selection is the central task for assets management, but it turns\\nout to be very challenging. Methods based on pattern matching, particularly the\\nCORN-K algorithm, have achieved promising performance on several stock markets. A key shortage of the existing pattern matching methods, however, is that the\\nrisk is largely ignored when optimizing portfolios, which may lead to\\nunreliable profits, particularly in volatile markets. We present a\\nrisk-aversion CORN-K algorithm, RACORN-K, that penalizes risk when searching\\nfor optimal portfolios.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'RACORN-K: Risk-Aversion Pattern Matching-based Portfolio Selection',\n",
       "  'text': 'Experiments on four datasets (DJIA, MSCI, SP500(N),\\nHSI) demonstrate that the new algorithm can deliver notable and reliable\\nimprovements in terms of return, Sharp ratio and maximum drawdown, especially\\non volatile markets.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Risk management with machine-learning-based algorithms',\n",
       "  'text': 'We propose some machine-learning-based algorithms to solve hedging problems\\nin incomplete markets. Sources of incompleteness cover illiquidity, untradable\\nrisk factors, discrete hedging dates and transaction costs. The proposed\\nalgorithms resulting strategies are compared to classical stochastic control\\ntechniques on several payoffs using a variance criterion. One of the proposed\\nalgorithm is flexible enough to be used with several existing risk criteria. We\\nfurthermore propose a new moment-based risk criteria.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Pareto models for risk management',\n",
       "  'text': 'The Pareto model is very popular in risk management, since simple analytical\\nformulas can be derived for financial downside risk measures (Value-at-Risk,\\nExpected Shortfall) or reinsurance premiums and related quantities (Large Claim\\nIndex, Return Period). Nevertheless, in practice, distributions are (strictly)\\nPareto only in the tails, above (possible very) large threshold. Therefore, it\\ncould be interesting to take into account second order behavior to provide a\\nbetter fit. In this article, we present how to go from a strict Pareto model to\\nPareto-type distributions. We discuss inference, and derive formulas for\\nvarious measures and indices, and finally provide applications on insurance\\nlosses and financial risks.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Parametric measures of variability induced by risk measures',\n",
       "  'text': 'We study general classes of parametric measures of variability with\\napplications in risk management. Particular focus is put on variability\\nmeasures induced by three classes of popular risk measures: the Value-at-Risk,\\nthe Expected Shortfall, and the expectiles. Properties of these variability\\nmeasures are explored in detail, and a characterization result is obtained via\\nthe mixture of inter-ES differences. Convergence properties and asymptotic\\nnormality of their empirical estimators are established. We provide an\\nillustration of the three classes of variability measures applied to financial\\ndata and analyze their relative advantages.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'EvaSylv: A user-friendly software to evaluate forestry scenarii\\n  including natural risk',\n",
       "  'text': 'Forest management relies on the evaluation of silviculture practices. The\\nincrease in natural risk due to climate change makes it necessary to consider\\nevaluation criteria that take natural risk into account. Risk integration in\\nexisting software requires advanced programming skills.We propose a\\nuser-friendly software to simulate even-aged and monospecific forest at the\\nstand level, in order to evaluate and optimize forest management. The software\\ngives the possibility to run management scenarii with or without considering\\nthe impact of natural risk. The control variables are the dates and rates of\\nthinning and the cutting age.The risk model is based on a Poisson processus.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'EvaSylv: A user-friendly software to evaluate forestry scenarii\\n  including natural risk',\n",
       "  'text': 'The Faustmann approach, including tree damage risk, is used to evaluate future\\nbenefits, economic or ecosystem services. It relies on the calculation of\\nexpected values, for which a dedicated mathematical development has been done. The optimized criteria used to evaluate the various scenarii are the Faustmann\\nvalue and the Averaged yield value.We illustrate the approach and the software\\non two case studies: economic optimization of a beech stand and carbon\\nsequestration optimization of a pine stand.Software interface makes it easy for\\nusers to write their own (growth-tree damage-economic) models without advanced\\nprogramming skills.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'EvaSylv: A user-friendly software to evaluate forestry scenarii\\n  including natural risk',\n",
       "  'text': 'The possibility to run management scenarii with/without\\nconsidering the impact of natural risk may contribute improving silviculture\\nguidelines and adapting them to climate change. We propose future lines of\\nresearch and improvement.',\n",
       "  'meta': {'_split_id': 2}},\n",
       " {'name': 'Liquidity Stress Testing in Asset Management -- Part 1. Modeling the\\n  Liability Liquidity Risk',\n",
       "  'text': 'This article is part of a comprehensive research project on liquidity risk in\\nasset management, which can be divided into three dimensions. The first\\ndimension covers liability liquidity risk (or funding liquidity) modeling, the\\nsecond dimension focuses on asset liquidity risk (or market liquidity)\\nmodeling, and the third dimension considers asset-liability liquidity risk\\nmanagement (or asset-liability matching). The purpose of this research is to\\npropose a methodological and practical framework in order to perform liquidity\\nstress testing programs, which comply with regulatory guidelines (ESMA, 2019)\\nand are useful for fund managers.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Liquidity Stress Testing in Asset Management -- Part 1. Modeling the\\n  Liability Liquidity Risk',\n",
       "  'text': 'The review of the academic literature and\\nprofessional research studies shows that there is a lack of standardized and\\nanalytical models. The aim of this research project is then to fill the gap\\nwith the goal to develop mathematical and statistical approaches, and provide\\nappropriate answers. In this first part that focuses on liability liquidity risk modeling, we\\npropose several statistical models for estimating redemption shocks. The\\nhistorical approach must be complemented by an analytical approach based on\\nzero-inflated models if we want to understand the true parameters that\\ninfluence the redemption shocks.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Liquidity Stress Testing in Asset Management -- Part 1. Modeling the\\n  Liability Liquidity Risk',\n",
       "  'text': 'Moreover, we must also distinguish aggregate\\npopulation models and individual-based models if we want to develop behavioral\\napproaches. Once these different statistical models are calibrated, the second\\nbig issue is the risk measure to assess normal and stressed redemption shocks. Finally, the last issue is to develop a factor model that can translate stress\\nscenarios on market risk factors into stress scenarios on fund liabilities.',\n",
       "  'meta': {'_split_id': 2}},\n",
       " {'name': 'Multivariate risks and depth-trimmed regions',\n",
       "  'text': 'We describe a general framework for measuring risks, where the risk measure\\ntakes values in an abstract cone. It is shown that this approach naturally\\nincludes the classical risk measures and set-valued risk measures and yields a\\nnatural definition of vector-valued risk measures. Several main constructions\\nof risk measures are described in this abstract axiomatic framework. It is shown that the concept of depth-trimmed (or central) regions from the\\nmultivariate statistics is closely related to the definition of risk measures. In particular, the halfspace trimming corresponds to the Value-at-Risk, while\\nthe zonoid trimming yields the expected shortfall.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Multivariate risks and depth-trimmed regions',\n",
       "  'text': 'In the abstract framework,\\nit is shown how to establish a both-ways correspondence between risk measures\\nand depth-trimmed regions. It is also demonstrated how the lattice structure of\\nthe space of risk values influences this relationship.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Liquidity, risk measures, and concentration of measure',\n",
       "  'text': 'Expanding on techniques of concentration of measure, we develop a\\nquantitative framework for modeling liquidity risk using convex risk measures. The fundamental objects of study are curves of the form $(\\\\rho(\\\\lambda\\nX))_{\\\\lambda \\\\ge 0}$, where $\\\\rho$ is a convex risk measure and $X$ a random\\nvariable, and we call such a curve a \\\\emph{liquidity risk profile}. The shape\\nof a liquidity risk profile is intimately linked with the tail behavior of the\\nunderlying $X$ for some notable classes of risk measures, namely shortfall risk\\nmeasures.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Liquidity, risk measures, and concentration of measure',\n",
       "  'text': 'We exploit this link to systematically bound liquidity risk profiles\\nfrom above by other real functions $\\\\gamma$, deriving tractable necessary and\\nsufficient conditions for \\\\emph{concentration inequalities} of the form\\n$\\\\rho(\\\\lambda X) \\\\le \\\\gamma(\\\\lambda)$, for all $\\\\lambda \\\\ge 0$. These\\nconcentration inequalities admit useful dual representations related to\\ntransport inequalities, and this leads to efficient uniform bounds for\\nliquidity risk profiles for large classes of $X$. On the other hand, some\\nmodest new mathematical results emerge from this analysis, including a new\\ncharacterization of some classical transport-entropy inequalities.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Liquidity, risk measures, and concentration of measure',\n",
       "  'text': 'Lastly, the\\nanalysis is deepened by means of a surprising connection between time\\nconsistency properties of law invariant risk measures and the tensorization of\\nconcentration inequalities.',\n",
       "  'meta': {'_split_id': 2}},\n",
       " {'name': 'Business Cycles as Collective Risk Fluctuations',\n",
       "  'text': 'We suggest use continuous numerical risk grades [0,1] of R for a single risk\\nor the unit cube in Rn for n risks as the economic domain. We consider risk\\nratings of economic agents as their coordinates in the economic domain. Economic activity of agents, economic or other factors change agents risk\\nratings and that cause motion of agents in the economic domain. Aggregations of\\nvariables and transactions of individual agents in small volume of economic\\ndomain establish the continuous economic media approximation that describes\\ncollective variables, transactions and their flows in the economic domain as\\nfunctions of risk coordinates.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Business Cycles as Collective Risk Fluctuations',\n",
       "  'text': 'Any economic variable A(t,x) defines mean risk\\nXA(t) as risk weighted by economic variable A(t,x). Collective flows of\\neconomic variables in bounded economic domain fluctuate from secure to risky\\narea and back. These fluctuations of flows cause time oscillations of\\nmacroeconomic variables A(t) and their mean risks XA(t) in economic domain and\\nare the origin of any business and credit cycles. We derive equations that\\ndescribe evolution of collective variables, transactions and their flows in the\\neconomic domain. As illustration we present simple self-consistent equations of\\nsupply-demand cycles that describe fluctuations of supply, demand and their\\nmean risks.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Different Ways of Thinking about Street Networks and Spatial Analysis',\n",
       "  'text': 'Street networks, as one of the oldest infrastructures of transport in the\\nworld, play a significant role in modernization, sustainable development, and\\nhuman daily activities in both ancient and modern times. Although street\\nnetworks have been well studied in a variety of engineering and scientific\\ndisciplines, including for instance transport, geography, urban planning,\\neconomics, and even physics, our understanding of street networks in terms of\\ntheir structure and dynamics remains limited, especially when dealing with such\\nreal-world problems as traffic jams, pollution, and human evacuations for\\ndisaster management. One goal of this special issue is to promote different\\nways of thinking about understanding street networks, and of conducting spatial\\nanalysis.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'LeaveNow: A Social Network-based Smart Evacuation System for Disaster\\n  Management',\n",
       "  'text': 'The importance of timely response to natural disasters and evacuating\\naffected people to safe areas is paramount to save lives. Emergency services\\nare often handicapped by the amount of rescue resources at their disposal. We\\npresent a system that leverages the power of a social network forming new\\nconnections among people based on \\\\textit{real-time location} and expands the\\nrescue resources pool by adding private sector cars. We also introduce a\\ncar-sharing algorithm to identify safe routes in an emergency with the aim of\\nminimizing evacuation time, maximizing pick-up of people without cars, and\\navoiding traffic congestion.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Dynamic Population Estimation Using Anonymized Mobility Data',\n",
       "  'text': 'Fine population distribution both in space and in time is crucial for\\nepidemic management, disaster prevention,urban planning and more. Human\\nmobility data have a great potential for mapping population distribution at a\\nhigh level of spatiotemporal resolution. Power law models are the most popular\\nones for mapping mobility data to population. However,they fail to provide\\nconsistent estimations under different spatial and temporal resolutions, i.e. they have to be recalibrated whenever the spatial or temporal partitioning\\nscheme changes. We propose a Bayesian model for dynamic population estimation\\nusing static census data and anonymized mobility data. Our model gives\\nconsistent population estimations under different spatial and temporal\\nresolutions.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'VaR and ES for linear portfolios with mixture of elliptic distributed\\n  Risk Factors',\n",
       "  'text': 'In this paper, we generalize the parametric Delta-VaR methods from portfolios\\nwith elliptic distributed risk factors to portfolios with mixture of\\nelliptically distributed ones. We treat both the Expected Shortfall and the\\nValue-at-Risk of such portfolios. Special attention is given to the particular\\ncase of the mixture of Student-t distributions.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Bankruptcy Risk Induced by Career Concerns of Regulators',\n",
       "  'text': \"We introduce a model in which a regulator employs mechanism design to embed\\nher human capital beta signal(s) in a firm's capital structure, in order to\\nenhance the value of her post career change indexed executive stock option\\ncontract with the firm. We prove that the agency cost of this revolving door\\nbehavior increases the firm's financial leverage, bankruptcy risk, and affects\\nestimation of firm value at risk (VaR).\",\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Value at risk and the diversification dogma',\n",
       "  'text': \"The so-called risk diversification principle is analyzed, showing that its\\nconvenience depends on individual characteristics of the risks involved and the\\ndependence relationship among them. -----\\nSe analiza el principio de diversificaci\\\\'on de riesgos y se demuestra que no\\nsiempre resulta mejor que no diversificar, pues esto depende de\\ncaracter\\\\'isticas individuales de los riesgos involucrados, as\\\\'i como de la\\nrelaci\\\\'on de dependencia entre los mismos.\",\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Hedging Conditional Value at Risk with Options',\n",
       "  'text': 'We present a method of hedging Conditional Value at Risk of a position in\\nstock using put options. The result leads to a linear programming problem that\\ncan be solved to optimise risk hedging.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Monetary Measures of Risk',\n",
       "  'text': 'This survey gives an introduction to monetary measures of risk as monotone\\nand cash additive functions on spaces of univariate random variables. Primal\\nand dual representation results as well as several examples are discussed. Principal ways to construct risk measures are given and extensions to more\\ngeneral situations indicated.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Spectral risk measures and uncertainty',\n",
       "  'text': 'Risk assessment under different possible scenarios is a source of uncertainty\\nthat may lead to concerning financial losses. We address this issue, first, by\\nadapting a robust framework to the class of spectral risk measures. Second, we\\npropose a Deviation-based approach to quantify uncertainty. Furthermore, the\\ntheory is illustrated with a practical case study from NASDAQ index.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Crowd Behaviour during High-Stress Evacuations in an Immersive Virtual\\n  Environment',\n",
       "  'text': 'Understanding the collective dynamics of crowd movements during stressful\\nemergency situations is central to reducing the risk of deadly crowd disasters. Yet, their systematic experimental study remains a challenging open problem due\\nto ethical and methodological constraints. In this paper, we demonstrate the\\nviability of shared 3D virtual environments as an experimental platform for\\nconducting crowd experiments with real people. In particular, we show that\\ncrowds of real human subjects moving and interacting in an immersive 3D virtual\\nenvironment exhibit typical patterns of real crowds as observed in real-life\\ncrowded situations. These include the manifestation of social conventions and\\nthe emergence of self-organized patterns during egress scenarios.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Crowd Behaviour during High-Stress Evacuations in an Immersive Virtual\\n  Environment',\n",
       "  'text': 'High-stress\\nevacuation experiments conducted in this virtual environment reveal movements\\ncharacterized by mass herding and dangerous overcrowding as they occur in crowd\\ndisasters. We describe the behavioral mechanisms at play under such extreme\\nconditions and identify critical zones where overcrowding may occur. Furthermore, we show that herding spontaneously emerges from a density effect\\nwithout the need to assume an increase of the individual tendency to imitate\\npeers. Our experiments reveal the promise of immersive virtual environments as\\nan ethical, cost-efficient, yet accurate platform for exploring crowd behaviour\\nin high-risk situations with real human subjects.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Of Disasters and Dragon Kings: A Statistical Analysis of Nuclear Power\\n  Incidents & Accidents',\n",
       "  'text': 'We provide, and perform a risk theoretic statistical analysis of, a dataset\\nthat is 75 percent larger than the previous best dataset on nuclear incidents\\nand accidents, comparing three measures of severity: INES (International\\nNuclear Event Scale), radiation released, and damage dollar losses. The annual\\nrate of nuclear accidents, with size above 20 Million US$, per plant, decreased\\nfrom the 1950s until dropping significantly after Chernobyl (April, 1986). The\\nrate is now roughly stable at 0.002 to 0.003, i.e., around 1 event per year\\nacross the current fleet.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Of Disasters and Dragon Kings: A Statistical Analysis of Nuclear Power\\n  Incidents & Accidents',\n",
       "  'text': 'The distribution of damage values changed after Three\\nMile Island (TMI; March, 1979), where moderate damages were suppressed but the\\ntail became very heavy, being described by a Pareto distribution with tail\\nindex 0.55. Further, there is a runaway disaster regime, associated with the\\n\"dragon-king\" phenomenon, amplifying the risk of extreme damage. In fact, the\\ndamage of the largest event (Fukushima; March, 2011) is equal to 60 percent of\\nthe total damage of all 174 accidents in our database since 1946.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Of Disasters and Dragon Kings: A Statistical Analysis of Nuclear Power\\n  Incidents & Accidents',\n",
       "  'text': 'In dollar\\nlosses we compute a 50% chance that (i) a Fukushima event (or larger) occurs in\\nthe next 50 years, (ii) a Chernobyl event (or larger) occurs in the next 27\\nyears and (iii) a TMI event (or larger) occurs in the next 10 years. Finally,\\nwe find that the INES scale is inconsistent. To be consistent with damage, the\\nFukushima disaster would need to have an INES level of 11, rather than the\\nmaximum of 7.',\n",
       "  'meta': {'_split_id': 2}},\n",
       " {'name': 'The multi-layer network nature of systemic risk and its implications for\\n  the costs of financial crises',\n",
       "  'text': 'The inability to see and quantify systemic financial risk comes at an immense\\nsocial cost. Systemic risk in the financial system arises to a large extent as\\na consequence of the interconnectedness of its institutions, which are linked\\nthrough networks of different types of financial contracts, such as credit,\\nderivatives, foreign exchange and securities. The interplay of the various\\nexposure networks can be represented as layers in a financial multi-layer\\nnetwork. In this work we quantify the daily contributions to systemic risk from\\nfour layers of the Mexican banking system from 2007-2013.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'The multi-layer network nature of systemic risk and its implications for\\n  the costs of financial crises',\n",
       "  'text': 'We show that focusing\\non a single layer underestimates the total systemic risk by up to 90%. By\\nassigning systemic risk levels to individual banks we study the systemic risk\\nprofile of the Mexican banking system on all market layers. This profile can be\\nused to quantify systemic risk on a national level in terms of nation-wide\\nexpected systemic losses. We show that market-based systemic risk indicators\\nsystematically underestimate expected systemic losses. We find that expected\\nsystemic losses are up to a factor four higher now than before the financial\\ncrisis of 2007-2008.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'The multi-layer network nature of systemic risk and its implications for\\n  the costs of financial crises',\n",
       "  'text': 'We find that systemic risk contributions of individual\\ntransactions can be up to a factor of thousand higher than the corresponding\\ncredit risk, which creates huge risks for the public. We find an intriguing\\nnon-linear effect whereby the sum of systemic risk of all layers underestimates\\nthe total risk. The method presented here is the first objective data driven\\nquantification of systemic risk on national scales that reveal its true levels.',\n",
       "  'meta': {'_split_id': 2}},\n",
       " {'name': 'Deep Replication of a Runoff Portfolio',\n",
       "  'text': 'To the best of our knowledge, the application of deep learning in the field\\nof quantitative risk management is still a relatively recent phenomenon. This\\narticle presents the key notions of Deep Asset Liability Management (Deep~ALM)\\nfor a technological transformation in the management of assets and liabilities\\nalong a whole term structure. The approach has a profound impact on a wide\\nrange of applications such as optimal decision making for treasurers, optimal\\nprocurement of commodities or the optimisation of hydroelectric power plants. As a by-product, intriguing aspects of goal-based investing or Asset Liability\\nManagement (ALM) in abstract terms concerning urgent challenges of our society\\nare expected alongside.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Deep Replication of a Runoff Portfolio',\n",
       "  'text': 'We illustrate the potential of the approach in a\\nstylised case.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': \"Risk reduction and Diversification within Markowitz's Mean-Variance\\n  Model: Theoretical Revisit\",\n",
       "  'text': 'The conventional wisdom of mean-variance (MV) portfolio theory asserts that\\nthe nature of the relationship between risk and diversification is a decreasing\\nasymptotic function, with the asymptote approximating the level of portfolio\\nsystematic risk or undiversifiable risk. This literature assumes that investors\\nhold an equally-weighted or a MV portfolio and quantify portfolio\\ndiversification using portfolio size. However, the equally-weighted portfolio\\nand portfolio size are MV optimal if and only if asset returns distribution is\\nexchangeable or investors have no useful information about asset expected\\nreturn and risk.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': \"Risk reduction and Diversification within Markowitz's Mean-Variance\\n  Model: Theoretical Revisit\",\n",
       "  'text': 'Moreover, the whole of literature, absolutely all of it,\\nfocuses only on risky assets, ignoring the role of the risk free asset in the\\nefficient diversification. Therefore, it becomes interesting and important to\\nanswer this question: how valid is this conventional wisdom when investors have\\nfull information about asset expected return and risk and asset returns\\ndistribution is not exchangeable in both the case where the risk free rate is\\navailable or not? Unfortunately, this question have never been addressed in the\\ncurrent literature. This paper fills the gap.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Risk Without Return',\n",
       "  'text': 'Risk-only investment strategies have been growing in popularity as\\ntraditional in- vestment strategies have fallen short of return targets over\\nthe last decade. However, risk-based investors should be aware of four things. First, theoretical considerations and empirical studies show that apparently\\ndictinct risk-based investment strategies are manifestations of a single\\neffect. Second, turnover and associated transaction costs can be a substantial\\ndrag on return. Third, capital diversification benefits may be reduced. Fourth,\\nthere is an apparent connection between performance and risk diversification.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Risk Without Return',\n",
       "  'text': 'To analyze risk diversification benefits in a consistent way, we introduce the\\nRisk Diversification Index (RDI) which measures risk concentrations and\\ncomplements the Herfindahl-Herschman Index (HHI) for capital concentrations.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Reward-risk momentum strategies using classical tempered stable\\n  distribution',\n",
       "  'text': 'We implement momentum strategies using reward-risk measures as ranking\\ncriteria based on classical tempered stable distribution. Performances and risk\\ncharacteristics for the alternative portfolios are obtained in various asset\\nclasses and markets. The reward-risk momentum strategies with lower volatility\\nlevels outperform the traditional momentum strategy regardless of asset class\\nand market. Additionally, the alternative portfolios are not only less riskier\\nin risk measures such as VaR, CVaR and maximum drawdown but also characterized\\nby thinner downside tails. Similar patterns in performance and risk profile are\\nalso found at the level of each ranking basket in the reward-risk portfolios.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Reward-risk momentum strategies using classical tempered stable\\n  distribution',\n",
       "  'text': 'Higher factor-neutral returns achieved by the reward-risk momentum strategies\\nare statistically significant and large portions of the performances are not\\nexplained by the Carhart four-factor model.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'The Temporal Dimension of Risk',\n",
       "  'text': 'Multi-period measures of risk account for the path that the value of an\\ninvestment portfolio takes. In the context of probabilistic risk measures, the\\nfocus has traditionally been on the magnitude of investment loss and not on the\\ndimension associated with the passage of time. In this paper, the concept of\\ntemporal path-dependent risk measure is mathematically formalized to capture\\nthe risk associated with the temporal dimension of a stochastic process. We\\ndiscuss the properties of temporal measures of risk and show that they can\\nnever be coherent.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'The Temporal Dimension of Risk',\n",
       "  'text': 'We then study the temporal dimension of investment drawdown,\\nits duration, which measures the length of excursions below a running maximum. Its properties in the context of risk measures are analyzed both theoretically\\nand empirically. In particular, we show that duration captures serial\\ncorrelation in the returns of two major asset classes. We conclude by\\ndiscussing the challenges of path-dependent temporal risk estimation in\\npractice.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Multifactor Risk Models and Heterotic CAPM',\n",
       "  'text': 'We give a complete algorithm and source code for constructing general\\nmultifactor risk models (for equities) via any combination of style factors,\\nprincipal components (betas) and/or industry factors. For short horizons we\\nemploy the Russian-doll risk model construction to obtain a nonsingular factor\\ncovariance matrix. This generalizes the heterotic risk model construction to\\ninclude arbitrary non-industry risk factors as well as industry risk factors\\nwith generic \"weights\". The aim of sharing our proprietary know-how with the\\ninvestment community is to encourage organic risk model building. The\\npresentation is intended to be essentially self-contained and pedagogical. So,\\nstop wasting money and complaining, start building risk models and enjoy!',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Risk and Return models for Equity Markets and Implied Equity Risk\\n  Premium',\n",
       "  'text': 'Equity risk premium is a central component of every risk and return model in\\nfinance and a key input to estimate costs of equity and capital in both\\ncorporate finance and valuation. An article by Damodaran examines three broad\\napproaches for estimating the equity risk premium. The first is survey based,\\nit consists in asking common investors or big players like pension fund\\nmanagers what they require as a premium to invest in equity. The second is to\\nlook at the premia earned historically by investing in stocks, as opposed to\\nrisk-free investments.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Risk and Return models for Equity Markets and Implied Equity Risk\\n  Premium',\n",
       "  'text': \"The third method tries to extrapolate a market-consensus\\non equity risk premium (Implied Equity Risk Premium) by analysing equity prices\\non the market today. After having introduced some basic concepts and models,\\nI'll briefly explain the pluses and minuses of the first two methods, and\\nanalyse more deeply the third. In the end I'll show the results of my\\nestimation of ERP on real data, using variants of the Implied ERP (third)\\nmethod.\",\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'calculation worst-case Value-at-Risk prediction using empirical data\\n  under model uncertainty',\n",
       "  'text': 'Quantification of risk positions under model uncertainty is of crucial\\nimportance from both viewpoints of external regulation and internal management. The concept of model uncertainty, sometimes also referred to as model\\nambiguity. Although we know the family of models, we cannot precisely decide\\nwhich one to use. Given the set $\\\\mathcal{P}$, the value of the risk measure\\n$\\\\rho$ varies in a range over the set of all possible models. The largest value\\nin such a range is referred to as a worst-case value, and the corresponding\\nmodel is called a worst scenario. Value-at-Risk(VaR) has become a very popular\\nrisk-measurement tool since it was first proposed.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'calculation worst-case Value-at-Risk prediction using empirical data\\n  under model uncertainty',\n",
       "  'text': 'Naturally, WVaR(worst-case\\nValue-at-Risk) attracts the attention of many researchers. Although many\\nliteratures investigated WVaR, the implications for empirical data analysis\\nremain rare. In this paper, we proposed a special model uncertainty market\\nmodel to simply the $\\\\mathcal{P}$ to a set contain finite number of probability\\ndistributions. The model has the structure of the two-layer mixed distribution\\nmodel. We used change point detection method to divide the returns series and\\nthen used EM algorithm to estimate the parameters. Finally, we calculated VaR,\\nWVaR(worst-case Value-at-Risk) and BVaR(best-case Value-at-Risk) for four\\nfinancial markets and then analyzed their different performance.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'What is the best risk measure in practice? A comparison of standard\\n  measures',\n",
       "  'text': 'Expected Shortfall (ES) has been widely accepted as a risk measure that is\\nconceptually superior to Value-at-Risk (VaR). At the same time, however, it has\\nbeen criticised for issues relating to backtesting. In particular, ES has been\\nfound not to be elicitable which means that backtesting for ES is less\\nstraightforward than, e.g., backtesting for VaR. Expectiles have been suggested\\nas potentially better alternatives to both ES and VaR. In this paper, we\\nrevisit commonly accepted desirable properties of risk measures like coherence,\\ncomonotonic additivity, robustness and elicitability. We check VaR, ES and\\nExpectiles with regard to whether or not they enjoy these properties, with\\nparticular emphasis on Expectiles.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'What is the best risk measure in practice? A comparison of standard\\n  measures',\n",
       "  'text': 'We also consider their impact on capital\\nallocation, an important issue in risk management. We find that, despite the\\ncaveats that apply to the estimation and backtesting of ES, it can be\\nconsidered a good risk measure. As a consequence, there is no sufficient\\nevidence to justify an all-inclusive replacement of ES by Expectiles in\\napplications. For backtesting ES, we propose an empirical approach that\\nconsists in replacing ES by a set of four quantiles, which should allow to make\\nuse of backtesting methods for VaR. Keywords: Backtesting; capital allocation; coherence; diversification;\\nelicitability; expected shortfall; expectile; forecasts; probability integral\\ntransform (PIT); risk measure; risk management; robustness; value-at-risk',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Representing, Simulating and Analysing Ho Chi Minh City Tsunami Plan by\\n  Means of Process Models',\n",
       "  'text': \"This paper considers the textual plan (guidelines) proposed by People's\\nCommittee of Ho Chi Minh City (Vietnam) to manage earthquake and tsunami, and\\ntry to represent it in a more formal way, in order to provide means to\\nsimulate, analyse and adapt it. We first present a state of the art about\\ncoordination models for disaster management with a focus on process oriented\\napproaches. We give an overview of the different dimensions of the textual\\ntsunami plan of Ho Chi Minh City and then the graphical representation of its\\nprocess with BPMN (Business Process Model and Notation).\",\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Representing, Simulating and Analysing Ho Chi Minh City Tsunami Plan by\\n  Means of Process Models',\n",
       "  'text': 'We finally show how to\\nexploit this process with workflow tools to simulate (YAWL tool) and analyse it\\n(ProM tool).',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Automated Management of Pothole related Disasters Using Image Processing\\n  and Geotagging',\n",
       "  'text': 'Potholes though seem inconsequential, may cause accidents resulting in loss\\nof human life. In this paper, we present an automated system to efficiently\\nmanage the potholes in a ward by deploying geotagging and image processing\\ntechniques that overcomes the drawbacks associated with the existing\\nsurvey-oriented systems. Image processing is used for identification of target\\npothole regions in the 2D images using edge detection and morphological image\\nprocessing operations. A method is developed to accurately estimate the\\ndimensions of the potholes from their images, analyze their area and depth,\\nestimate the quantity of filling material required and therefore enabling\\npothole attendance on a priority basis.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Automated Management of Pothole related Disasters Using Image Processing\\n  and Geotagging',\n",
       "  'text': 'This will further enable the government\\nofficial to have a fully automated system for effectively managing pothole\\nrelated disasters.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'On the Risk Management with Application of Econophysics Analysis in\\n  Central Banks and Financial Institutions',\n",
       "  'text': 'The purpose of this research article is to discover how the econophysics\\nanalysis can complement the econometrics models in application to the risk\\nmanagement in the central banks and financial institutions, operating within\\nthe nonlinear dynamical financial system. We consider the modern risk\\nmanagement models and show the appropriate techniques to calculate the various\\nexisting risks in the finances. We make a few comments on the possible\\nlimitations in the models of statistical modeling of volatility such as the\\nAutoregressive Conditional Heteroskedasticity (GARCH) model, because of the\\nnonlinearities appearance in the nonlinear dynamical financial systems.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'On the Risk Management with Application of Econophysics Analysis in\\n  Central Banks and Financial Institutions',\n",
       "  'text': 'We\\npropose that the various types of nonlinearities, which can originate in the\\nfinancial and economical systems, have to be taken to the detailed\\nconsideration during the Cost of Capital calculation in the finances and\\neconomics. We propose the new theory of nonlinear dynamic volatilities and the\\nnew nonlinear dynamic chaos (NDC) volatility model for the statistical modeling\\nof financial volatility with the aim to determine the Value at Risk.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'From Nobel Prize to Project Management: Getting Risks Right',\n",
       "  'text': 'A major source of risk in project management is inaccurate forecasts of\\nproject costs, demand, and other impacts. The paper presents a promising new\\napproach to mitigating such risk, based on theories of decision making under\\nuncertainty which won the 2002 Nobel prize in economics. First, the paper\\ndocuments inaccuracy and risk in project management. Second, it explains\\ninaccuracy in terms of optimism bias and strategic misrepresentation. Third,\\nthe theoretical basis is presented for a promising new method called \"reference\\nclass forecasting,\" which achieves accuracy by basing forecasts on actual\\nperformance in a reference class of comparable projects and thereby bypassing\\nboth optimism bias and strategic misrepresentation.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'From Nobel Prize to Project Management: Getting Risks Right',\n",
       "  'text': 'Fourth, the paper presents\\nthe first instance of practical reference class forecasting, which concerns\\ncost forecasts for large transportation infrastructure projects. Finally,\\npotentials for and barriers to reference class forecasting are assessed.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'On Game-Theoretic Risk Management (Part Three) - Modeling and\\n  Applications',\n",
       "  'text': 'The game-theoretic risk management framework put forth in the precursor\\nreports \"Towards a Theory of Games with Payoffs that are\\nProbability-Distributions\" (arXiv:1506.07368 [q-fin.EC]) and \"Algorithms to\\nCompute Nash-Equilibria in Games with Distributions as Payoffs\"\\n(arXiv:1511.08591v1 [q-fin.EC]) is herein concluded by discussing how to\\nintegrate the previously developed theory into risk management processes. To\\nthis end, we discuss how loss models (primarily but not exclusively\\nnon-parametric) can be constructed from data. Furthermore, hints are given on\\nhow a meaningful game theoretic model can be set up, and how it can be used in\\nvarious stages of the ISO 27000 risk management process.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'On Game-Theoretic Risk Management (Part Three) - Modeling and\\n  Applications',\n",
       "  'text': 'Examples related to\\nadvanced persistent threats and social engineering are given. We conclude by a\\ndiscussion on the meaning and practical use of (mixed) Nash equilibria\\nequilibria for risk management.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Portfolio Management Approach in Trade Credit Decision Making',\n",
       "  'text': 'The basic financial purpose of an enterprise is maximization of its value. Trade credit management should also contribute to realization of this\\nfundamental aim. Many of the current asset management models that are found in\\nfinancial management literature assume book profit maximization as the basic\\nfinancial purpose. These book profitbased models could be lacking in what\\nrelates to another aim (i.e., maximization of enterprise value). The enterprise\\nvalue maximization strategy is executed with a focus on risk and uncertainty. This article presents the consequences that can result from operating risk that\\nis related to purchasers using payment postponement for goods and/or services.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Portfolio Management Approach in Trade Credit Decision Making',\n",
       "  'text': 'The present article offers a method that uses portfolio management theory to\\ndetermine the level of accounts receivable in a firm. An increase in the level\\nof accounts receivables in a firm increases both net working capital and the\\ncosts of holding and managing accounts receivables. Both of these decrease the\\nvalue of the firm, but a liberal policy in accounts receivable coupled with the\\nportfolio management approach could increase the value. Efforts to assign ways\\nto manage these risks were also undertaken; among them, special attention was\\npaid to adapting assumptions from portfolio theory as well as gauging the\\npotential effect on the firm value.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'A new approach for scenario generation in Risk management',\n",
       "  'text': 'We provide a new dynamic approach to scenario generation for the purposes of\\nrisk management in the banking industry. We connect ideas from conventional\\ntechniques -- like historical and Monte Carlo simulation -- and we come up with\\na hybrid method that shares the advantages of standard procedures but\\neliminates several of their drawbacks. Instead of considering the static\\nproblem of constructing one or ten day ahead distributions for vectors of risk\\nfactors, we embed the problem into a dynamic framework, where any time horizon\\ncan be consistently simulated.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'A new approach for scenario generation in Risk management',\n",
       "  'text': 'Additionally, we use standard models from\\nmathematical finance for each risk factor, whence bridging the worlds of\\ntrading and risk management. Our approach is based on stochastic differential equations (SDEs), like the\\nHJM-equation or the Black-Scholes equation, governing the time evolution of\\nrisk factors, on an empirical calibration method to the market for the chosen\\nSDEs, and on an Euler scheme (or high-order schemes) for the numerical\\nevaluation of the respective SDEs.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'A new approach for scenario generation in Risk management',\n",
       "  'text': 'The empirical calibration procedure\\npresented in this paper can be seen as the SDE-counterpart of the so called\\nFiltered Historical Simulation method; the behavior of volatility stems in our\\ncase out of the assumptions on the underlying SDEs. Furthermore, we are able to\\neasily incorporate \"middle-size\" and \"large-size\" events within our framework\\nalways making a precise distinction between the information obtained from the\\nmarket and the one coming from the necessary a-priori intuition of the risk\\nmanager. Results of one concrete implementation are provided.',\n",
       "  'meta': {'_split_id': 2}},\n",
       " {'name': 'Optimizing S-shaped utility and implications for risk management',\n",
       "  'text': 'We consider market players with tail-risk-seeking behaviour as exemplified by\\nthe S-shaped utility introduced by Kahneman and Tversky. We argue that risk\\nmeasures such as value at risk (VaR) and expected shortfall (ES) are\\nineffective in constraining such players. We show that, in many standard market\\nmodels, product design aimed at utility maximization is not constrained at all\\nby VaR or ES bounds: the maximized utility corresponding to the optimal payoff\\nis the same with or without ES constraints.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Optimizing S-shaped utility and implications for risk management',\n",
       "  'text': 'By contrast we show that, in\\nreasonable markets, risk management constraints based on a second more\\nconventional concave utility function can reduce the maximum S-shaped utility\\nthat can be achieved by the investor, even if the constraining utility function\\nis only rather modestly concave. It follows that product designs leading to\\nunbounded S-shaped utilities will lead to unbounded negative expected\\nconstraining utilities when measured with such conventional utility functions. To prove these latter results we solve a general problem of optimizing an\\ninvestor expected utility under risk management constraints where both investor\\nand risk manager have conventional concave utility functions, but the investor\\nhas limited liability.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Optimizing S-shaped utility and implications for risk management',\n",
       "  'text': 'We illustrate our results throughout with the example of\\nthe Black--Scholes option market. These results are particularly important\\ngiven the historical role of VaR and that ES was endorsed by the Basel\\ncommittee in 2012--2013.',\n",
       "  'meta': {'_split_id': 2}},\n",
       " {'name': 'Parameter uncertainty for integrated risk capital calculations based on\\n  normally distributed subrisks',\n",
       "  'text': 'In this contribution we consider the overall risk given as the sum of random\\nsubrisks $\\\\mathbf{X}_j$ in the context of value-at-risk (VaR) based risk\\ncalculations. If we assume that the undertaking knows the parametric\\ndistribution family subrisk $\\\\mathbf{X}_j=\\\\mathbf{X}_j(\\\\theta_j)$, but does not\\nknow the true parameter vectors $\\\\theta_j$, the undertaking faces parameter\\nuncertainty. To assess the appropriateness of methods to model parameter\\nuncertainty for risk capital calculation we consider a criterion introduced in\\nthe recent literature.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Parameter uncertainty for integrated risk capital calculations based on\\n  normally distributed subrisks',\n",
       "  'text': 'According to this criterion, we demonstrate that, in\\ngeneral, appropriateness of a risk capital model for each subrisk does not\\nimply appropriateness of the model on the aggregate level of the overall\\nrisk.\\\\\\\\ For the case where the overall risk is given by the sum of normally\\ndistributed subrisks we prove a theoretical result leading to an appropriate\\nintegrated risk capital model taking parameter uncertainty into account. Based\\non the theorem we develop a method improving the approximation of the required\\nconfidence level simultaneously for both - on the level of each subrisk as well\\nas for the overall risk.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': \"The case of 'Less is more': Modelling risk-preference with Expected\\n  Downside Risk\",\n",
       "  'text': 'This paper discusses an alternative explanation for the empirical findings\\ncontradicting the positive relationship between risk (variance) and reward\\n(expected return). We show that these contradicting results might be due to the\\nfalse definition of risk-perception, which we correct by introducing Expected\\nDownside Risk (EDR). The EDR parameter, similar to the Expected Shortfall or\\nConditional Value-at-Risk, measures the tail risk, however, fits and better\\nexplains the utility perception of investors. Our results indicate that when\\nusing the EDR as risk measure, both the positive and negative relationship\\nbetween expected return and risk can be derived under standard conditions (e.g. expected utility theory and positive risk-aversion).',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': \"The case of 'Less is more': Modelling risk-preference with Expected\\n  Downside Risk\",\n",
       "  'text': 'Therefore, no alternative\\npsychological explanation or additional boundary condition on utility theory is\\nrequired to explain the phenomenon. Furthermore, we show empirically that it is\\na more precise linear predictor of expected return than volatility, both for\\nindividual assets and portfolios.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Solvency II, or How to Sweep the Downside Risk Under the Carpet',\n",
       "  'text': 'Under Solvency II the computation of capital requirements is based on value\\nat risk (V@R). V@R is a quantile-based risk measure and neglects extreme risks\\nin the tail. V@R belongs to the family of distortion risk measures. A serious\\ndeficiency of V@R is that firms can hide their total downside risk in corporate\\nnetworks, unless a consolidated solvency balance sheet is required for each\\neconomic scenario. In this case, they can largely reduce their total capital\\nrequirements via appropriate transfer agreements within a network structure\\nconsisting of sufficiently many entities and thereby circumvent capital\\nregulation.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Solvency II, or How to Sweep the Downside Risk Under the Carpet',\n",
       "  'text': 'We prove several versions of such a result for general distortion\\nrisk measures of V@R-type, explicitly construct suitable allocations of the\\nnetwork portfolio, and finally demonstrate how these findings can be extended\\nbeyond distortion risk measures. We also discuss why consolidation requirements\\ncannot completely eliminate this problem. Capital regulation should thus be\\nbased on coherent or convex risk measures like average value at risk or\\nexpectiles.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Multivariate Shortfall Risk Allocation and Systemic Risk',\n",
       "  'text': 'The ongoing concern about systemic risk since the outburst of the global\\nfinancial crisis has highlighted the need for risk measures at the level of\\nsets of interconnected financial components, such as portfolios, institutions\\nor members of clearing houses. The two main issues in systemic risk measurement\\nare the computation of an overall reserve level and its allocation to the\\ndifferent components according to their systemic relevance. We develop here a\\npragmatic approach to systemic risk measurement and allocation based on\\nmultivariate shortfall risk measures, where acceptable allocations are first\\ncomputed and then aggregated so as to minimize costs.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Multivariate Shortfall Risk Allocation and Systemic Risk',\n",
       "  'text': 'We analyze the\\nsensitivity of the risk allocations to various factors and highlight its\\nrelevance as an indicator of systemic risk. In particular, we study the\\ninterplay between the loss function and the dependence structure of the\\ncomponents. Moreover, we address the computational aspects of risk allocation. Finally, we apply this methodology to the allocation of the default fund of a\\nCCP on real data.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Corporate payments networks and credit risk rating',\n",
       "  'text': 'Aggregate and systemic risk in complex systems are emergent phenomena\\ndepending on two properties: the idiosyncratic risks of the elements and the\\ntopology of the network of interactions among them. While a significant\\nattention has been given to aggregate risk assessment and risk propagation once\\nthe above two properties are given, less is known about how the risk is\\ndistributed in the network and its relations with the topology. We study this\\nproblem by investigating a large proprietary dataset of payments among 2.4M\\nItalian firms, whose credit risk rating is known. We document significant\\ncorrelations between local topological properties of a node (firm) and its\\nrisk.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Corporate payments networks and credit risk rating',\n",
       "  'text': 'Moreover we show the existence of an homophily of risk, i.e. the tendency\\nof firms with similar risk profile to be statistically more connected among\\nthemselves. This effect is observed when considering both pairs of firms and\\ncommunities or hierarchies identified in the network. We leverage this\\nknowledge to show the predictability of the missing rating of a firm using only\\nthe network properties of the associated node.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'A Bonus-Malus Framework for Cyber Risk Insurance and Optimal\\n  Cybersecurity Provisioning',\n",
       "  'text': 'The cyber risk insurance market is at a nascent stage of its development,\\neven as the magnitude of cyber losses is significant and the rate of cyber risk\\nevents is increasing. Existing cyber risk insurance products as well as\\nacademic studies have been focusing on classifying cyber risk events and\\ndeveloping models of these events, but little attention has been paid to\\nproposing insurance risk transfer strategies that incentivize mitigation of\\ncyber loss through adjusting the premium of the risk transfer product. To\\naddress this important gap, we develop a Bonus-Malus model for cyber risk\\ninsurance.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'A Bonus-Malus Framework for Cyber Risk Insurance and Optimal\\n  Cybersecurity Provisioning',\n",
       "  'text': 'Specifically, we propose a mathematical model of cyber risk\\ninsurance and cybersecurity provisioning supported with an efficient numerical\\nalgorithm based on dynamic programming. Through a numerical experiment, we\\ndemonstrate how a properly designed cyber risk insurance contract with a\\nBonus-Malus system can resolve the issue of moral hazard and benefit the\\ninsurer.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Polish and Silesian Non-Profit Organizations Liquidity Strategies',\n",
       "  'text': 'The kind of realized mission inflows the sensitivity to risk. Among other\\nfactors, the risk results from decision about liquid assets investment level\\nand liquid assets financing. The higher the risk exposure, the higher the level\\nof liquid assets. If the specific risk exposure is smaller, the more aggressive\\ncould be the net liquid assets strategy. The organization choosing between\\nvarious solutions in liquid assets needs to decide what level of risk is\\nacceptable for her owners (or donors) and / or capital suppliers.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Polish and Silesian Non-Profit Organizations Liquidity Strategies',\n",
       "  'text': 'The paper\\nshows how, in authors opinion, decisions, about liquid assets management\\nstrategy inflow the risk of the organizations and its economical results during\\nrealization of main mission. Comparison of theoretical model with empirical\\ndata for over 450 Silesian nonprofit organization results suggests that\\nnonprofit organization managing teams choose more risky aggressive liquid\\nassets solutions than for-profit firms.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Model Risk Measurement under Wasserstein Distance',\n",
       "  'text': 'The paper proposes a new approach to model risk measurement based on the\\nWasserstein distance between two probability measures. It formulates the\\ntheoretical motivation resulting from the interpretation of fictitious\\nadversary of robust risk management. The proposed approach accounts for\\nequivalent and non-equivalent probability measures and incorporates the\\neconomic reality of the fictitious adversary. It provides practically feasible\\nresults that overcome the restriction of considering only models implying\\nprobability measures equivalent to the reference model. The Wasserstein\\napproach suits for various types of model risk problems, ranging from the\\nsingle-asset hedging risk problem to the multi-asset allocation problem.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Model Risk Measurement under Wasserstein Distance',\n",
       "  'text': 'The\\nrobust capital market line, accounting for the correlation risk, is not\\nachievable with other non-parametric approaches.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'The t copula with Multiple Parameters of Degrees of Freedom: Bivariate\\n  Characteristics and Application to Risk Management',\n",
       "  'text': 'The t copula is often used in risk management as it allows for modelling tail\\ndependence between risks and it is simple to simulate and calibrate. However,\\nthe use of a standard t copula is often criticized due to its restriction of\\nhaving a single parameter for the degrees of freedom (dof) that may limit its\\ncapability to model the tail dependence structure in a multivariate case. To\\novercome this problem, grouped t copula was proposed recently, where risks are\\ngrouped a priori in such a way that each group has a standard t copula with its\\nspecific dof parameter.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'The t copula with Multiple Parameters of Degrees of Freedom: Bivariate\\n  Characteristics and Application to Risk Management',\n",
       "  'text': \"In this paper we propose the use of a grouped t copula,\\nwhere each group consists of one risk factor only, so that a priori grouping is\\nnot required. The copula characteristics in the bivariate case are studied. We\\nexplain simulation and calibration procedures, including a simulation study on\\nfinite sample properties of the maximum likelihood estimators and Kendall's tau\\napproximation. This new copula can be significantly different from the standard\\nt copula in terms of risk measures such as tail dependence, value at risk and\\nexpected shortfall. Keywords: grouped t copula, tail dependence, risk management.\",\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Time Consistent Dynamic Risk Processes, Cadlag Modification',\n",
       "  'text': \"Working in a continuous time setting, we extend to the general case of\\ndynamic risk measures continuous from above the characterization of time\\nconsistency in terms of ``cocycle condition'' of the minimal penalty function. We prove also the supermartingale property for general time consistent dynamic\\nrisk measures. When the time consistent dynamic risk measure (continuous from\\nabove) is normalized and non degenerate, we prove, under a mild condition, that\\nthe dynamic risk process of any financial instrument has a cadlag modification. This condition is always satisfied in case of continuity from below.\",\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Risk Minimization through Portfolio Replication',\n",
       "  'text': 'We use a replica approach to deal with portfolio optimization problems. A\\ngiven risk measure is minimized using empirical estimates of asset values\\ncorrelations. We study the phase transition which happens when the time series\\nis too short with respect to the size of the portfolio. We also study the noise\\nsensitivity of portfolio allocation when this transition is approached. We\\nconsider explicitely the cases where the absolute deviation and the conditional\\nvalue-at-risk are chosen as a risk measure. We show how the replica method can\\nstudy a wide range of risk measures, and deal with various types of time series\\ncorrelations, including realistic ones with volatility clustering.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Maturity-independent risk measures',\n",
       "  'text': 'The new notion of maturity-independent risk measures is introduced and\\ncontrasted with the existing risk measurement concepts. It is shown, by means\\nof two examples, one set on a finite probability space and the other in a\\ndiffusion framework, that, surprisingly, some of the widely utilized risk\\nmeasures cannot be used to build maturity-independent counterparts. We\\nconstruct a large class of maturity-independent risk measures and give\\nrepresentative examples in both continuous- and discrete-time financial models.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'The Quantification of Operational Risk using Internal Data, Relevant\\n  External Data and Expert Opinions',\n",
       "  'text': \"To quantify an operational risk capital charge under Basel II, many banks\\nadopt a Loss Distribution Approach. Under this approach, quantification of the\\nfrequency and severity distributions of operational risk involves the bank's\\ninternal data, expert opinions and relevant external data. In this paper we\\nsuggest a new approach, based on a Bayesian inference method, that allows for a\\ncombination of these three sources of information to estimate the parameters of\\nthe risk frequency and severity distributions.\",\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Risk assessment for uncertain cash flows: Model ambiguity, discounting\\n  ambiguity, and the role of bubbles',\n",
       "  'text': 'We study the risk assessment of uncertain cash flows in terms of dynamic\\nconvex risk measures for processes as introduced in Cheridito, Delbaen, and\\nKupper (2006). These risk measures take into account not only the amounts but\\nalso the timing of a cash flow. We discuss their robust representation in terms\\nof suitably penalized probability measures on the optional sigma-field. This\\nyields an explicit analysis both of model and discounting ambiguity. We focus\\non supermartingale criteria for different notions of time consistency. In\\nparticular we show how bubbles may appear in the dynamic penalization, and how\\nthey cause a breakdown of asymptotic safety of the risk assessment procedure.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Spectral Risk Measures: Properties and Limitations',\n",
       "  'text': 'Spectral risk measures (SRMs) are risk measures that take account of user\\nriskaversion, but to date there has been little guidance on the choice of\\nutility function underlying them. This paper addresses this issue by examining\\nalternative approaches based on exponential and power utility functions. A\\nnumber of problems are identified with both types of spectral risk measure. The\\ngeneral lesson is that users of spectral risk measures must be careful to\\nselect utility functions that fit the features of the particular problems they\\nare dealing with, and should be especially careful when using power SRMs.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Distortion risk measures for sums of dependent losses',\n",
       "  'text': 'We discuss two distinct approaches, for distorting risk measures of sums of\\ndependent random variables, which preserve the property of coherence. The\\nfirst, based on distorted expectations, operates on the survival function of\\nthe sum. The second, simultaneously applies the distortion on the survival\\nfunction of the sum and the dependence structure of risks, represented by\\ncopulas. Our goal is to propose risk measures that take into account the\\nfluctuations of losses and possible correlations between risk components.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Risk Measures on $\\\\mathcal{P}(\\\\mathbb{R})$ and Value At Risk with\\n  Probability/Loss function',\n",
       "  'text': 'We propose a generalization of the classical notion of the $V@R_{\\\\lambda}$\\nthat takes into account not only the probability of the losses, but the balance\\nbetween such probability and the amount of the loss. This is obtained by\\ndefining a new class of law invariant risk measures based on an appropriate\\nfamily of acceptance sets. The $V@R_{\\\\lambda}$ and other known law invariant\\nrisk measures turn out to be special cases of our proposal. We further prove\\nthe dual representation of Risk Measures on $\\\\mathcal{P}(% \\\\mathbb{R}).$',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Iterated risk measures for risk-sensitive Markov decision processes with\\n  discounted cost',\n",
       "  'text': 'We demonstrate a limitation of discounted expected utility, a standard\\napproach for representing the preference to risk when future cost is\\ndiscounted. Specifically, we provide an example of the preference of a decision\\nmaker that appears to be rational but cannot be represented with any discounted\\nexpected utility. A straightforward modification to discounted expected utility\\nleads to inconsistent decision making over time. We will show that an iterated\\nrisk measure can represent the preference that cannot be represented by any\\ndiscounted expected utility and that the decisions based on the iterated risk\\nmeasure are consistent over time.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Involving copula functions in Conditional Tail Expectation',\n",
       "  'text': 'Our goal in this paper is to propose an alternative risk measure which takes\\ninto account the fluctuations of losses and possible correlations between\\nrandom variables. This new notion of risk measures, that we call Copula\\nConditional Tail Expectation describes the expected amount of risk that can be\\nexperienced given that a potential bivariate risk exceeds a bivariate threshold\\nvalue, and provides an important measure for right-tail risk. An application to\\nreal financial data is given.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Measuring Model Risk',\n",
       "  'text': 'We propose to interpret distribution model risk as sensitivity of expected\\nloss to changes in the risk factor distribution, and to measure the\\ndistribution model risk of a portfolio by the maximum expected loss over a set\\nof plausible distributions defined in terms of some divergence from an\\nestimated distribution. The divergence may be relative entropy, a Bregman\\ndistance, or an $f$-divergence. We give formulas for the calculation of\\ndistribution model risk and explicitly determine the worst case distribution\\nfrom the set of plausible distributions. We also give formulas for the\\nevaluation of divergence preferences describing ambiguity averse decision\\nmakers.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Capital allocation and risk appetite under Solvency II framework',\n",
       "  'text': 'The aim of this paper is to introduce a method for computing the allocated\\nSolvency II Capital Requirement (SCR) of each Risk which the company is exposed\\nto, taking in account for the diversification effect among different risks. The\\nmethod suggested is based on the Euler principle. We show that it has very\\nsuitable properties like coherence in the sense of Denault (2001) and RORAC\\ncompatibility, and practical implications for the companies that use the\\nstandard formula.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Capital allocation and risk appetite under Solvency II framework',\n",
       "  'text': \"Further, we show how this approach can be used to evaluate\\nthe underwriting and reinsurance policies and to define a measure of the\\nCompany's risk appetite, based on the capital at risk return.\",\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'An Approximate Solution Method for Large Risk-Averse Markov Decision\\n  Processes',\n",
       "  'text': 'Stochastic domains often involve risk-averse decision makers. While recent\\nwork has focused on how to model risk in Markov decision processes using risk\\nmeasures, it has not addressed the problem of solving large risk-averse\\nformulations. In this paper, we propose and analyze a new method for solving\\nlarge risk-averse MDPs with hybrid continuous-discrete state spaces and\\ncontinuous action spaces. The proposed method iteratively improves a bound on\\nthe value function using a linearity structure of the MDP. We demonstrate the\\nutility and properties of the method on a portfolio optimization problem.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'On the Capital Allocation Problem for a New Coherent Risk Measure in\\n  Collective Risk Theory',\n",
       "  'text': \"In this paper we introduce a new coherent cumulative risk measure on\\n$\\\\mathcal{R}_L^p$, the space of c\\\\`adl\\\\`ag processes having Laplace transform. This new coherent risk measure turns out to be tractable enough within a class\\nof models where the aggregate claims is driven by a spectrally positive L\\\\'evy\\nprocess. Moreover, we study the problem of capital allocation in an insurance\\ncontext and we show that the capital allocation problem for this risk measure\\nhas a unique solution determined by the Euler allocation method. Some examples\\nare provided.\",\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'On Optimal Reinsurance Policy with Distortion Risk Measures and Premiums',\n",
       "  'text': 'In this paper, we consider the problem of optimal reinsurance design, when\\nthe risk is measured by a distortion risk measure and the premium is given by a\\ndistortion risk premium. First, we show how the optimal reinsurance design for\\nthe ceding company, the reinsurance company and the social planner can be\\nformulated in the same way. Second, by introducing the marginal indemnification\\nfunctions, we characterize the optimal reinsurance contracts. We show that, for\\nan optimal policy, the associated marginal indemnification function only takes\\nthe values zero and one.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'On Optimal Reinsurance Policy with Distortion Risk Measures and Premiums',\n",
       "  'text': 'We will see how the roles of the market preferences\\nand premiums and that of the total risk are separated.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Systemic Risks in CCP Networks',\n",
       "  'text': 'We propose a model for the credit and liquidity risks faced by clearing\\nmembers of Central Counterparty Clearing houses (CCPs). This model aims to\\ncapture the features of: gap risk; feedback between clearing member default,\\nmarket volatility and margining requirements; the different risks faced by\\nvarious types of market participant and the changes in margining requirements a\\nclearing member faces as the system evolves. By considering the entire network\\nof CCPs and clearing members, we investigate the distribution of losses to\\ndefault fund contributions and contingent liquidity requirements for each\\nclearing member; further, we identify wrong-way risks between defaults of\\nclearing members and market turbulence.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Pricing Credit Default Swap Subject to Counterparty Risk and\\n  Collateralization',\n",
       "  'text': 'This article presents a new model for valuing a credit default swap (CDS)\\ncontract that is affected by multiple credit risks of the buyer, seller and\\nreference entity. We show that default dependency has a significant impact on\\nasset pricing. In fact, correlated default risk is one of the most pervasive\\nthreats in financial markets. We also show that a fully collateralized CDS is\\nnot equivalent to a risk-free one. In other words, full collateralization\\ncannot eliminate counterparty risk completely in the CDS market.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Risk measures for processes and BSDEs',\n",
       "  'text': 'The paper analyzes risk assessment for cash flows in continuous time using\\nthe notion of convex risk measures for processes. By combining a decomposition\\nresult for optional measures, and a dual representation of a convex risk\\nmeasure for bounded \\\\cd processes, we show that this framework provides a\\nsystematic approach to the both issues of model ambiguity, and uncertainty\\nabout the time value of money. We also establish a link between risk measures\\nfor processes and BSDEs.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Risk-Minimizing Hedging of Counterparty Risk',\n",
       "  'text': 'We study dynamic hedging of counterparty risk for a portfolio of credit\\nderivatives. Our empirically driven credit model consists of interacting\\ndefault intensities which ramp up and then decay after the occurrence of credit\\nevents. Using the Galtchouk-Kunita-Watanabe decomposition of the counterparty\\nrisk price payment stream, we recover a closed-form representation for the risk\\nminimizing strategy in terms of classical solutions to nonlinear recursive\\nsystems of Cauchy problems. We discuss applications of our framework to the\\nmost prominent class of credit derivatives, including credit swap and risky\\nbond portfolios, as well as first-to-default claims.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Pricing Financial Derivatives Subject to Counterparty Risk and Credit\\n  Value Adjustment',\n",
       "  'text': 'This article presents a generic model for pricing financial derivatives\\nsubject to counterparty credit risk. Both unilateral and bilateral types of\\ncredit risks are considered. Our study shows that credit risk should be modeled\\nas American style options in most cases, which require a backward induction\\nvaluation. To correct a common mistake in the literature, we emphasize that the\\nmarket value of a defaultable derivative is actually a risky value rather than\\na risk-free value. Credit value adjustment (CVA) is also elaborated. A\\npractical framework is developed for pricing defaultable derivatives and\\ncalculating their CVAs at a portfolio level.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Value-at-Risk substitute for non-ruin capital is fallacious and\\n  redundant',\n",
       "  'text': 'This seemed impossible to use a theoretically adequate but too sophisticated\\nrisk measure called non-ruin capital, whence its widespread (including\\nregulatory documents) replacement with an inadequate, but simple risk measure\\ncalled Value-at-Risk. Conflicting with the idea by Albert Einstein that\\n\"everything should be made as simple as possible, but not simpler\", this led to\\nfallacious, and even deceitful (but generally accepted) standards and\\nrecommendations. Arguing from the standpoint of mathematical theory of risk, we\\naim to break this impasse.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Dynamic Network Risk',\n",
       "  'text': 'This paper examines the pricing of short-term and long-term dynamic network\\nrisk in the cross-section of stock returns. Stocks with high sensitivities to\\ndynamic network risk earn lower returns. We rationalize our finding with\\neconomic theory that allows the stochastic discount factor to load on network\\nrisk through the precautionary savings channel. A one-standard deviation\\nincrease in long-term (short-term) network risk loadings associate with a 7.66%\\n(6.71%) drop in annualized expected returns.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Entropic Risk Constrained Soft-Robust Policy Optimization',\n",
       "  'text': 'Having a perfect model to compute the optimal policy is often infeasible in\\nreinforcement learning. It is important in high-stakes domains to quantify and\\nmanage risk induced by model uncertainties. Entropic risk measure is an\\nexponential utility-based convex risk measure that satisfies many reasonable\\nproperties. In this paper, we propose an entropic risk constrained policy\\ngradient and actor-critic algorithms that are risk-averse to the model\\nuncertainty. We demonstrate the usefulness of our algorithms on several problem\\ndomains.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Computation of bonus in multi-state life insurance',\n",
       "  'text': 'We consider computation of market values of bonus payments in multi-state\\nwith-profit life insurance. The bonus scheme consists of additional benefits\\nbought according to a dividend strategy that depends on the past realization of\\nfinancial risk, the current individual insurance risk, the number of additional\\nbenefits currently held, and so-called portfolio-wide means describing the\\nshape of the insurance business. We formulate numerical procedures that\\nefficiently combine simulation of financial risk with more analytical methods\\nfor the outstanding insurance risk. Special attention is given to the case\\nwhere the number of additional benefits bought only depends on the financial\\nrisk.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Assessing Systemic Risk in the Insurance Sector via Network Theory',\n",
       "  'text': 'We provide a framework for detecting relevant insurance companies in a\\nsystemic risk perspective. Among the alternative methodologies for measuring\\nsystemic risk, we propose a complex network approach where insurers are linked\\nto form a global interconnected system. We model the reciprocal influence\\nbetween insurers calibrating edge weights on the basis of specific risk\\nmeasures. Therefore, we provide a suitable network indicator, the Weighted\\nEffective Resistance Centrality, able to catch which is the effect of a\\nspecific vertex on the network robustness. By means of this indicator, we\\nassess the prominence of a company in spreading and receiving risk from the\\nothers.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Planning Optimal From the Firm Value Creation Perspective Levels of\\n  Operating Cash Investments',\n",
       "  'text': 'The basic financial purpose of corporation is creation of its value. Liquidity management should also contribute to realization of this fundamental\\naim. Many of the current asset management models that are found in financial\\nmanagement literature assume book profit maximization as the basic financial\\npurpose. These book profit based models could be lacking in what relates to\\nanother aim like maximization of enterprise value. The corporate value creation\\nstrategy is executed with a focus on risk and uncertainty. Firms hold cash for\\na variety of reasons. Generally, cash balances held in a firm can be called\\nconsidered, precautionary, speculative, transactional and intentional.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Planning Optimal From the Firm Value Creation Perspective Levels of\\n  Operating Cash Investments',\n",
       "  'text': 'The\\nfirst are the result of management anxieties. Managers fear the negative part\\nof the risk and hold cash to hedge against it. Second, cash balances are held\\nto use chances that are created by the positive part of the risk equation. Next, cash balances are the result of the operating needs of the firm. In this\\narticle, we analyze the relation between these types of cash balances and risk. This article presents the discussion about relations between firm net working\\ninvestment policy and as result operating cash balances and firm value.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Planning Optimal From the Firm Value Creation Perspective Levels of\\n  Operating Cash Investments',\n",
       "  'text': 'This\\narticle also contains propositions for marking levels of precautionary cash\\nbalances and speculative cash balances. Application of these propositions\\nshould help managers to make better decisions to maximize the value of a firm.',\n",
       "  'meta': {'_split_id': 2}},\n",
       " {'name': 'Inverse Optimization of Convex Risk Functions',\n",
       "  'text': \"The theory of convex risk functions has now been well established as the\\nbasis for identifying the families of risk functions that should be used in\\nrisk averse optimization problems. Despite its theoretical appeal, the\\nimplementation of a convex risk function remains difficult, as there is little\\nguidance regarding how a convex risk function should be chosen so that it also\\nwell represents one's own risk preferences. In this paper, we address this\\nissue through the lens of inverse optimization.\",\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Inverse Optimization of Convex Risk Functions',\n",
       "  'text': 'Specifically, given solution\\ndata from some (forward) risk-averse optimization problems we develop an\\ninverse optimization framework that generates a risk function that renders the\\nsolutions optimal for the forward problems. The framework incorporates the\\nwell-known properties of convex risk functions, namely, monotonicity,\\nconvexity, translation invariance, and law invariance, as the general\\ninformation about candidate risk functions, and also the feedbacks from\\nindividuals, which include an initial estimate of the risk function and\\npairwise comparisons among random losses, as the more specific information. Our\\nframework is particularly novel in that unlike classical inverse optimization,\\nno parametric assumption is made about the risk function, i.e. it is\\nnon-parametric.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Inverse Optimization of Convex Risk Functions',\n",
       "  'text': 'We show how the resulting inverse optimization problems can be\\nreformulated as convex programs and are polynomially solvable if the\\ncorresponding forward problems are polynomially solvable. We illustrate the\\nimputed risk functions in a portfolio selection problem and demonstrate their\\npractical value using real-life data.',\n",
       "  'meta': {'_split_id': 2}},\n",
       " {'name': 'An Autonomous Spectrum Management Scheme for Unmanned Aerial Vehicle\\n  Networks in Disaster Relief Operations',\n",
       "  'text': 'This paper studies the problem of spectrum shortage in an unmanned aerial\\nvehicle (UAV) network during critical missions such as wildfire monitoring,\\nsearch and rescue, and disaster monitoring. Such applications involve a high\\ndemand for high-throughput data transmissions such as real-time video-, image-,\\nand voice- streaming where the assigned spectrum to the UAV network may not be\\nadequate to provide the desired Quality of Service (QoS). In these scenarios,\\nthe aerial network can borrow an additional spectrum from the available\\nterrestrial networks in the trade of a relaying service for them.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'An Autonomous Spectrum Management Scheme for Unmanned Aerial Vehicle\\n  Networks in Disaster Relief Operations',\n",
       "  'text': 'We propose a\\nspectrum sharing model in which the UAVs are grouped into two classes of\\nrelaying UAVs that service the spectrum owner and the sensing UAVs that perform\\nthe disaster relief mission using the obtained spectrum. The operation of the\\nUAV network is managed by a hierarchical mechanism in which a central\\ncontroller assigns the tasks of the UAVs based on their resources and determine\\ntheir operation region based on the level of priority of impacted areas and\\nthen the UAVs autonomously fine-tune their position using a model-free\\nreinforcement learning algorithm to maximize the individual throughput and\\nprolong their lifetime.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'An Autonomous Spectrum Management Scheme for Unmanned Aerial Vehicle\\n  Networks in Disaster Relief Operations',\n",
       "  'text': 'We analyze the performance and the convergence for the\\nproposed method analytically and with extensive simulations in different\\nscenarios.',\n",
       "  'meta': {'_split_id': 2}},\n",
       " {'name': 'Quantifying mortality risk in small defined-benefit pension schemes',\n",
       "  'text': \"A risk of small defined-benefit pension schemes is that there are too few\\nmembers to eliminate idiosyncratic mortality risk, that is there are too few\\nmembers to effectively pool mortality risk. This means that when there are few\\nmembers in the scheme, there is an increased risk of the liability value\\ndeviating significantly from the expected liability value, as compared to a\\nlarge scheme. We quantify this risk through examining the coefficient of variation of a\\nscheme's liability value relative to its expected value.\",\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Quantifying mortality risk in small defined-benefit pension schemes',\n",
       "  'text': 'We examine how the\\ncoefficient of variation varies with the number of members and find that, even\\nwith a few hundred members in the scheme, idiosyncratic mortality risk may\\nstill be significant. Using a stochastic mortality model reduces the\\nidiosyncratic mortality risk but at the cost of increasing the overall\\nmortality risk in the scheme. Next we quantify the amount of the mortality risk concentrated in the\\nexecutive section of the scheme, where the executives receive a benefit that is\\nhigher than the non-executive benefit. We use the Euler capital allocation\\nprinciple to allocate the total standard deviation of the liability value\\nbetween the executive and non-executive sections.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Quantifying mortality risk in small defined-benefit pension schemes',\n",
       "  'text': \"We find that the proportion\\nof the standard deviation allocated to the executive section is higher than is\\nsuggested by an allocation based on the members' benefit amounts. While the\\nresults are sensitive to the choice of mortality model, they do suggest that\\nthe mortality risk of the scheme should be monitored and managed within the\\nsections of a scheme, and not only on a scheme-wide basis.\",\n",
       "  'meta': {'_split_id': 2}},\n",
       " {'name': 'Review Study For Inter-Operability Of Manet Protocols In Wireless Sensor\\n  Networks',\n",
       "  'text': 'Wireless Networks are most appealing in terms of deployment over a wide range\\nof applications. The key areas are disaster management, industrial unit\\nautomation and battlefield surveillance. The paper presents a study over\\ninter-operability of MANET (Mobile Ad-Hoc Network) protocols i.e DSDV, OLSR,\\nZRP, AODV over WSN (Wireless Sensor Network) [10]. The review here covers all\\nthe prevailing protocol solutions for WSN and deployment of MANET protocols\\nover them. The need of moving to MANET protocols lie in situation when we talk\\nabout mobile sensory nodes which are a compulsion when we talk about the above\\nmentioned three areas. However, the deployment may not be limited to these\\nonly.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Massive MIMO for Drone Communications: Case Studies and Future\\n  Directions',\n",
       "  'text': 'Unmanned aerial vehicles (UAVs), also known as drones, are proliferating. Applications, such as surveillance, disaster management, and drone racing,\\nplace high requirements on the communication with the drones in terms of\\nthroughput, reliability, and latency. The existing wireless technologies,\\nnotably Wi-Fi, that are currently used for drone connectivity are limited to\\nshort ranges and low-mobility situations. New, scalable technology is needed to\\nmeet future demands on long connectivity ranges, support for fast-moving\\ndrones, and the possibility to simultaneously communicate with entire swarms of\\ndrones. Massive multiple-input and multiple-output (MIMO), the main technology\\ncomponent of emerging 5G standards, has the potential to meet these\\nrequirements.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'PSIque: Next Sequence Prediction of Satellite Images using a\\n  Convolutional Sequence-to-Sequence Network',\n",
       "  'text': 'Predicting unseen weather phenomena is an important issue for disaster\\nmanagement. In this paper, we suggest a model for a convolutional\\nsequence-to-sequence autoencoder for predicting undiscovered weather situations\\nfrom previous satellite images. We also propose a symmetric skip connection\\nbetween encoder and decoder modules to produce more comprehensive image\\npredictions. To examine our model performance, we conducted experiments for\\neach suggested model to predict future satellite images from historical\\nsatellite images. A specific combination of skip connection and\\nsequence-to-sequence autoencoder was able to generate closest prediction from\\nthe ground truth image.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Towards better social crisis data with HERMES: Hybrid sensing for\\n  EmeRgency ManagEment System',\n",
       "  'text': 'People involved in mass emergencies increasingly publish information-rich\\ncontents in online social networks (OSNs), thus acting as a distributed and\\nresilient network of human sensors. In this work, we present HERMES, a system\\ndesigned to enrich the information spontaneously disclosed by OSN users in the\\naftermath of disasters. HERMES leverages a mixed data collection strategy,\\ncalled hybrid crowdsensing, and state-of-the-art AI techniques. Evaluated in\\nreal-world emergencies, HERMES proved to increase: (i) the amount of the\\navailable damage information; (ii) the density (up to 7x) and the variety (up\\nto 18x) of the retrieved geographic information; (iii) the geographic coverage\\n(up to 30%) and granularity.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Deep Learning Models for Predicting Wildfires from Historical\\n  Remote-Sensing Data',\n",
       "  'text': 'Identifying regions that have high likelihood for wildfires is a key\\ncomponent of land and forestry management and disaster preparedness. We create\\na data set by aggregating nearly a decade of remote-sensing data and historical\\nfire records to predict wildfires. This prediction problem is framed as three\\nmachine learning tasks. Results are compared and analyzed for four different\\ndeep learning models to estimate wildfire likelihood. The results demonstrate\\nthat deep learning models can successfully identify areas of high fire\\nlikelihood using aggregated data about vegetation, weather, and topography with\\nan AUC of 83%.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Cyber Risk in Health Facilities: A Systematic Literature Review',\n",
       "  'text': 'The current world challenges include issues such as infectious disease\\npandemics, environmental health risks, food safety, and crime prevention. Through this article, a special emphasis is given to one of the main challenges\\nin the healthcare sector during the COVID-19 pandemic, the cyber risk. Since\\nthe beginning of the Covid-19 pandemic, the World Health Organization has\\ndetected a dramatic increase in the number of cyber-attacks. For instance, in\\nItaly the COVID-19 emergency has heavily affected cybersecurity; from January\\nto April 2020, the total of attacks, accidents, and violations of privacy to\\nthe detriment of companies and individuals has doubled.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Cyber Risk in Health Facilities: A Systematic Literature Review',\n",
       "  'text': 'Using a systematic and\\nrigorous approach, this paper aims to analyze the literature on the cyber risk\\nin the healthcare sector to understand the real knowledge on this topic. The\\nfindings highlight the poor attention of the scientific community on this\\ntopic, except in the United States. The literature lacks research contributions\\nto support cyber risk management in subject areas such as Business, Management\\nand Accounting; Social Science; and Mathematics. This research outlines the\\nneed to empirically investigate the cyber risk, giving a practical solution to\\nhealth facilities. Keywords: cyber risk; cyber-attack; cybersecurity; computer\\nsecurity; COVID-19; coronavirus;information technology risk; risk management;\\nrisk assessment; health facilities; healthcare sector;systematic literature\\nreview; insurance',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'New copulas based on general partitions-of-unity and their applications\\n  to risk management',\n",
       "  'text': 'We construct new multivariate copulas on the basis of a generalized infinite\\npartition-of-unity approach. This approach allows - in contrast to finite\\npartition-of-unity copulas - for tail-dependence as well as for asymmetry. A\\npossibility of fitting such copulas to real data from quantitative risk\\nmanagement is also pointed out.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Data driven partition-of-unity copulas with applications to risk\\n  management',\n",
       "  'text': 'We present a constructive and self-contained approach to data driven general\\npartition-of-unity copulas that were recently introduced in the literature. In\\nparticular, we consider Bernstein-, negative binomial and Poisson copulas and\\npresent a solution to the problem of fitting such copulas to highly asymmetric\\ndata.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Game-theoretic approach to risk-sensitive benchmarked asset management',\n",
       "  'text': 'In this article we consider a game theoretic approach to the Risk-Sensitive\\nBenchmarked Asset Management problem (RSBAM) of Davis and Lleo \\\\cite{DL}. In\\nparticular, we consider a stochastic differential game between two players,\\nnamely, the investor who has a power utility while the second player represents\\nthe market which tries to minimize the expected payoff of the investor. The\\nmarket does this by modulating a stochastic benchmark that the investor needs\\nto outperform. We obtain an explicit expression for the optimal pair of\\nstrategies as for both the players.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Mathematical Analysis of Dynamic Risk Default in Microfinance',\n",
       "  'text': 'In this work we will develop a new approach to solve the non repayment\\nproblem in microfinance due to the problem of asymmetric information. This\\napproach is based on modeling and simulation of ordinary differential systems\\nwhere time remains a primordial component, they thus enable microfinance\\ninstitutions to manage their risk portfolios by a prediction of numbers of\\nsolvent and insolvent borrowers ever a period, in order to define or redefine\\nits development strategy, investment and management in an area, where the\\npopulation is often poor and in need a mechanism of financial inclusion.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Accelerator Disaster Scenarios, the Unabomber, and Scientific Risks',\n",
       "  'text': 'The possibility that experiments at high-energy accelerators could create new\\nforms of matter that would ultimately destroy the Earth has been considered\\nseveral times in the past quarter century. One consequence of the earliest of\\nthese disaster scenarios was that the authors of a 1993 article in \"Physics\\nToday\" who reviewed the experiments that had been carried out at the Bevalac at\\nLawrence Berkeley Laboratory were placed on the FBI\\'s Unabomber watch list.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Accelerator Disaster Scenarios, the Unabomber, and Scientific Risks',\n",
       "  'text': \"Later, concerns that experiments at the Relativistic Heavy Ion Collider at\\nBrookhaven National Laboratory might create mini black holes or nuggets of\\nstable strange quark matter resulted in a flurry of articles in the popular\\npress. I discuss this history, as well as Richard A. Posner's provocative\\nanalysis and recommendations on how to deal with such scientific risks. I\\nconclude that better communication between scientists and nonscientists would\\nserve to assuage unreasonable fears and focus attention on truly serious\\npotential threats to humankind.\",\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'A central limit theorem for functions of stationary max-stable random\\n  fields on $\\\\mathbb{R}^d$',\n",
       "  'text': 'Max-stable random fields are very appropriate for the statistical modelling\\nof spatial extremes. Hence, integrals of functions of max-stable random fields\\nover a given region can play a key role in the assessment of the risk of\\nnatural disasters, meaning that it is relevant to improve our understanding of\\ntheir probabilistic behaviour. For this purpose, in this paper, we propose a\\ngeneral central limit theorem for functions of stationary max-stable random\\nfields on $\\\\mathbb{R}^d$. Then, we show that appropriate functions of the\\nBrown-Resnick random field with a power variogram and of the Smith random field\\nsatisfy the central limit theorem.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'A central limit theorem for functions of stationary max-stable random\\n  fields on $\\\\mathbb{R}^d$',\n",
       "  'text': 'Another strong motivation for our work lies\\nin the fact that central limit theorems for random fields on $\\\\mathbb{R}^d$\\nhave been barely considered in the literature. As an application, we briefly\\nshow the usefulness of our results in a risk assessment context.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'An Alert-Generation Framework for Improving Resiliency in\\n  Human-Supervised, Multi-Agent Teams',\n",
       "  'text': \"Human-supervision in multi-agent teams is a critical requirement to ensure\\nthat the decision-maker's risk preferences are utilized to assign tasks to\\nrobots. In stressful complex missions that pose risk to human health and life,\\nsuch as humanitarian-assistance and disaster-relief missions, human mistakes or\\ndelays in tasking robots can adversely affect the mission. To assist human\\ndecision making in such missions, we present an alert-generation framework\\ncapable of detecting various modes of potential failure or performance\\ndegradation. We demonstrate that our framework, based on state machine\\nsimulation and formal methods, offers probabilistic modeling to estimate the\\nlikelihood of unfavorable events.\",\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'An Alert-Generation Framework for Improving Resiliency in\\n  Human-Supervised, Multi-Agent Teams',\n",
       "  'text': 'We introduce smart simulation that offers a\\ncomputationally-efficient way of detecting low-probability situations compared\\nto standard Monte-Carlo simulations. Moreover, for certain class of problems,\\nour inference-based method can provide guarantees on correctly detecting task\\nfailures.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'A multiagent urban traffic simulation. Part II: dealing with the\\n  extraordinary',\n",
       "  'text': 'In Probabilistic Risk Management, risk is characterized by two quantities:\\nthe magnitude (or severity) of the adverse consequences that can potentially\\nresult from the given activity or action, and by the likelihood of occurrence\\nof the given adverse consequences. But a risk seldom exists in isolation: chain\\nof consequences must be examined, as the outcome of one risk can increase the\\nlikelihood of other risks. Systemic theory must complement classic PRM. Indeed\\nthese chains are composed of many different elements, all of which may have a\\ncritical importance at many different levels.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'A multiagent urban traffic simulation. Part II: dealing with the\\n  extraordinary',\n",
       "  'text': 'Furthermore, when urban\\ncatastrophes are envisioned, space and time constraints are key determinants of\\nthe workings and dynamics of these chains of catastrophes: models must include\\na correct spatial topology of the studied risk. Finally, literature insists on\\nthe importance small events can have on the risk on a greater scale: urban\\nrisks management models belong to self-organized criticality theory. We chose\\nmultiagent systems to incorporate this property in our model: the behavior of\\nan agent can transform the dynamics of important groups of them.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Loss Distribution Approach for Operational Risk Capital Modelling under\\n  Basel II: Combining Different Data Sources for Risk Estimation',\n",
       "  'text': 'The management of operational risk in the banking industry has undergone\\nsignificant changes over the last decade due to substantial changes in\\noperational risk environment. Globalization, deregulation, the use of complex\\nfinancial products and changes in information technology have resulted in\\nexposure to new risks very different from market and credit risks. In response,\\nBasel Committee for banking Supervision has developed a regulatory framework,\\nreferred to as Basel II, that introduced operational risk category and\\ncorresponding capital requirements.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Loss Distribution Approach for Operational Risk Capital Modelling under\\n  Basel II: Combining Different Data Sources for Risk Estimation',\n",
       "  'text': 'Over the past five years, major banks in\\nmost parts of the world have received accreditation under the Basel II Advanced\\nMeasurement Approach (AMA) by adopting the loss distribution approach (LDA)\\ndespite there being a number of unresolved methodological challenges in its\\nimplementation. Different approaches and methods are still under hot debate. In\\nthis paper, we review methods proposed in the literature for combining\\ndifferent data sources (internal data, external data and scenario analysis)\\nwhich is one of the regulatory requirement for AMA.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Development and Experimentation of a Software Tool for Identifying High\\n  Risk Spreadsheets for Auditing',\n",
       "  'text': 'Heavy use of spreadsheets by organisations bears many potential risks such as\\nerrors, ambiguity, data loss, duplication, and fraud. In this paper these risks\\nare briefly outlined along with their available mitigation methods such as:\\ndocumentation, centralisation, auditing and user training. However, because of\\nthe large quantities of spreadsheets used in organisations, applying these\\nmethods on all spreadsheets is impossible. This fact is considered as a\\ndeficiency in these methods, a gap which is addressed in this paper. In this paper a new software tool for managing spreadsheets and identifying\\nthe risk levels they include is proposed, developed and tested.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Development and Experimentation of a Software Tool for Identifying High\\n  Risk Spreadsheets for Auditing',\n",
       "  'text': 'As an add-in\\nfor Microsoft Excel application, \"Risk Calculator\" can automatically collect\\nand record spreadsheet properties in an inventory database and assign risk\\nscores based on their importance, use and complexity. Consequently, auditing\\nprocesses can be targeted to high risk spreadsheets. Such a method saves time,\\neffort, and money.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Robustness in the Optimization of Risk Measures',\n",
       "  'text': 'We study issues of robustness in the context of Quantitative Risk Management\\nand Optimization. We develop a general methodology for determining whether a\\ngiven risk measurement related optimization problem is robust, which we call\\n\"robustness against optimization\". The new notion is studied for various\\nclasses of risk measures and expected utility and loss functions. Motivated by\\npractical issues from financial regulation, special attention is given to the\\ntwo most widely used risk measures in the industry, Value-at-Risk (VaR) and\\nExpected Shortfall (ES). We establish that for a class of general optimization\\nproblems, VaR leads to non-robust optimizers whereas convex risk measures\\ngenerally lead to robust ones.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Robustness in the Optimization of Risk Measures',\n",
       "  'text': 'Our results offer extra insight on the ongoing\\ndiscussion about the comparative advantages of VaR and ES in banking and\\ninsurance regulation. Our notion of robustness is conceptually different from\\nthe field of robust optimization, to which some interesting links are derived.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'The ineffectiveness of coherent risk measures',\n",
       "  'text': 'We show that coherent risk measures are ineffective in curbing the behaviour\\nof investors with limited liability or excessive tail-risk seeking behaviour if\\nthe market admits statistical arbitrage opportunities which we term\\n$\\\\rho$-arbitrage for a risk measure $\\\\rho$. We show how to determine\\nanalytically whether such $\\\\rho$-arbitrage portfolios exist in complete markets\\nand in the Markowitz model. We also consider realistic numerical examples of\\nincomplete markets and determine whether expected shortfall constraints are\\nineffective in these markets. We find that the answer depends heavily upon the\\nprobability model selected by the risk manager but that it is certainly\\npossible for expected shortfall constraints to be ineffective in realistic\\nmarkets.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'The ineffectiveness of coherent risk measures',\n",
       "  'text': 'Since value at risk constraints are weaker than expected shortfall\\nconstraints, our results can be applied to value at risk. By contrast, we show\\nthat reasonable expected utility constraints are effective in any\\narbitrage-free market.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'The Network Effect in Credit Concentration Risk',\n",
       "  'text': \"Measurement and management of credit concentration risk is critical for banks\\nand relevant for micro-prudential requirements. While several methods exist for\\nmeasuring credit concentration risk within institutions, the systemic effect of\\ndifferent institutions' exposures to the same counterparties has been less\\nexplored so far. In this paper, we propose a measure of the systemic credit\\nconcentration risk that arises because of common exposures between different\\ninstitutions within a financial system. This approach is based on a network\\nmodel that describes the effect of overlapping portfolios.\",\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'The Network Effect in Credit Concentration Risk',\n",
       "  'text': 'This network metric\\nis applied to synthetic and real world data to illustrate that the effect of\\ncommon exposures is not fully reflected in single portfolio concentration\\nmeasures. It also allows to quantify several aspects of the interplay between\\ninterconnectedness and credit risk. Using this network measure, we formulate an\\nanalytical approximation for the additional capital requirement corresponding\\nto the systemic risk arising from credit concentration interconnectedness. Our\\nmethodology also avoids double counting between the granularity adjustment and\\nthe common exposure adjustment.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'The Network Effect in Credit Concentration Risk',\n",
       "  'text': 'Although approximated, this common exposure\\nadjustment is able to capture, with only two parameters, an aspect of systemic\\nrisk that can extend single portfolios view to a system-wide one.',\n",
       "  'meta': {'_split_id': 2}},\n",
       " {'name': 'Introducing Variety in Risk Management',\n",
       "  'text': 'We review the recently introduced concept of variety of a financial portfolio\\nand we sketch its importance for risk control purposes. The empirical behaviour\\nof variety, correlation, exceedance correlation and asymmetry of the\\nprobability density function of daily returns is discussed. The results\\nobtained are compared with the ones of a one-factor model showing strengths and\\nlimitations of this model.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Global risk minimization in financial markets',\n",
       "  'text': 'Recurring international financial crises have adverse socioeconomic effects\\nand demand novel regulatory instruments or strategies for risk management and\\nmarket stabilization. However, the complex web of market interactions often\\nimpedes rational decisions that would absolutely minimize the risk. Here we\\nshow that, for any given expected return, investors can overcome this\\ncomplexity and globally minimize their financial risk in portfolio selection\\nmodels, which is mathematically equivalent to computing the ground state of\\nspin glass models in physics, provided the margin requirement remains below a\\ncritical, empirically measurable value. For markets with centrally regulated\\nmargin requirements, this result suggests a potentially stabilizing\\nintervention strategy.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Optimal consumption and investment with bounded downside risk measures\\n  for logarithmic utility functions',\n",
       "  'text': 'We investigate optimal consumption problems for a Black-Scholes market under\\nuniform restrictions on Value-at-Risk and Expected Shortfall for logarithmic\\nutility functions. We find the solutions in terms of a dynamic strategy in\\nexplicit form, which can be compared and interpreted. This paper continues our\\nprevious work, where we solved similar problems for power utility functions.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Saddlepoint methods in portfolio theory',\n",
       "  'text': 'We discuss the use of saddlepoint methods in the analysis of portfolios, with\\nparticular reference to credit portfolios. The objective is to proceed from a\\nmodel of the loss distribution, given through probabilities, correlations and\\nthe like, to an analytical approximation of the distribution. Once this is done\\nwe show how to derive the so-called risk contributions which are the\\nderivatives of risk measures, such as a given quantile (VaR) or expected\\nshortfall, to the allocations in the underlying assets. These show, informally,\\nwhere the risk is coming from, and also indicate how to go about optimising the\\nportfolio.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Hedging Against the Interest-rate Risk by Measuring the Yield-curve\\n  Movement',\n",
       "  'text': 'By adopting the polynomial interpolation method, we propose an approach to\\nhedge against the interest-rate risk of the default-free bonds by measuring the\\nnonparallel movement of the yield-curve, such as the translation, the rotation\\nand the twist. The empirical analysis shows that our hedging strategies are\\ncomparable to traditional duration-convexity strategy, or even better when we\\nhave more suitable hedging instruments on hand. The article shows that this\\nstrategy is flexible and robust to cope with the interest-rate risk and can\\nhelp fine-tune a position as time changes.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Optimal Investment and Risk Control Problem for an Insurer: Expected\\n  Utility Maximization',\n",
       "  'text': \"Motivated by the AIG bailout case in the financial crisis of 2007-2008, we\\nconsider an insurer who wants to maximize the expected utility of the terminal\\nwealth by selecting optimal investment and risk control strategies. The\\ninsurer's risk process is modelled by a jump-diffusion process and is\\nnegatively correlated with the capital gains in the financial market. We obtain\\nexplicit solution to optimal strategies for various utility functions.\",\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'The Credibility Theory applied to backtesting Counterparty Credit Risk',\n",
       "  'text': 'Credibility theory provides tools to obtain better estimates by combining\\nindividual data with sample information. We apply the Credibility theory to a\\nUniform distribution that is used in testing the reliability of forecasting an\\ninterest rate for long term horizons. Such empirical exercise is asked by\\nRegulators (CRR, 2013) in validating an Internal Model Method for Counterparty\\nCredit Risk. The main results is that risk managers consider more reliable the\\noutput of a test with limited sample size when the Credibility is applied to\\ndefine a confidence interval.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Value-at-Risk Prediction in R with the GAS Package',\n",
       "  'text': 'GAS models have been recently proposed in time-series econometrics as\\nvaluable tools for signal extraction and prediction. This paper details how\\nfinancial risk managers can use GAS models for Value-at-Risk (VaR) prediction\\nusing the novel GAS package for R. Details and code snippets for prediction,\\ncomparison and backtesting with GAS models are presented. An empirical\\napplication considering Dow Jones Index constituents investigates the VaR\\nforecasting performance of GAS models.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Analytical Framework for Credit Portfolios',\n",
       "  'text': 'Analytical, free of time consuming Monte Carlo simulations, framework for\\ncredit portfolio systematic risk metrics calculations is presented. Techniques\\nare described that allow calculation of portfolio-level systematic risk\\nmeasures (standard deviation, VaR and Expected Shortfall) as well as allocation\\nof risk down to individual transactions. The underlying model is the industry\\nstandard multi-factor Merton-type model with arbitrary valuation function at\\nhorizon (in contrast to the simplistic default-only case). High accuracy of the\\nproposed analytical technique is demonstrated by benchmarking against Monte\\nCarlo simulations.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Alert-BDI: BDI Model with Adaptive Alertness through Situational\\n  Awareness',\n",
       "  'text': 'In this paper, we address the problems faced by a group of agents that\\npossess situational awareness, but lack a security mechanism, by the\\nintroduction of a adaptive risk management system. The Belief-Desire-Intention\\n(BDI) architecture lacks a framework that would facilitate an adaptive risk\\nmanagement system that uses the situational awareness of the agents. We extend\\nthe BDI architecture with the concept of adaptive alertness. Agents can modify\\ntheir level of alertness by monitoring the risks faced by them and by their\\npeers. Alert-BDI enables the agents to detect and assess the risks faced by\\nthem in an efficient manner, thereby increasing operational efficiency and\\nresistance against attacks.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Systemic risk through contagion in a core-periphery structured banking\\n  network',\n",
       "  'text': 'We contribute to the understanding of how systemic risk arises in a network\\nof credit-interlinked agents. Motivated by empirical studies we formulate a\\nnetwork model which, despite its simplicity, depicts the nature of interbank\\nmarkets better than a homogeneous model. The components of a vector\\nOrnstein-Uhlenbeck process living on the vertices of the network describe the\\nfinancial robustnesses of the agents. For this system, we prove a LLN for\\ngrowing network size leading to a propagation of chaos result. We state\\nproperties, which arise from such a structure, and examine the effect of\\ninhomogeneity on several risk management issues and the possibility of\\ncontagion.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Asymptotic Analysis for Optimal Dividends in a Dual Risk Model',\n",
       "  'text': 'The dual risk model is a popular model in finance and insurance, which is\\noften used to model the wealth process of a venture capital or high tech\\ncompany. Optimal dividends have been extensively studied in the literature for\\nthe dual risk model. It is well known that the value function of this optimal\\ncontrol problem does not yield closed-form solutions except in some special\\ncases. In this paper, we study the asymptotics of the optimal dividends problem\\nwhen the parameters of the model go to either zero or infinity. Our results\\nprovide insights to the optimal strategies and the optimal values when the\\nparameters are extreme.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Asset Allocation Strategies Based on Penalized Quantile Regression',\n",
       "  'text': 'It is well known that quantile regression model minimizes the portfolio\\nextreme risk, whenever the attention is placed on the estimation of the\\nresponse variable left quantiles. We show that, by considering the entire\\nconditional distribution of the dependent variable, it is possible to optimize\\ndifferent risk and performance indicators. In particular, we introduce a\\nrisk-adjusted profitability measure, useful in evaluating financial portfolios\\nunder a pessimistic perspective, since the reward contribution is net of the\\nmost favorable outcomes. Moreover, as we consider large portfolios, we also\\ncope with the dimensionality issue by introducing an l1-norm penalty on the\\nassets weights.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'A simple model on streamflow management with a dynamic risk measure',\n",
       "  'text': 'We present an exactly-solvable risk-minimizing stochastic differential game\\nfor flood management in rivers. The streamflow dynamics follow stochastic\\ndifferential equations driven by a Levy process. An entropic dynamic risk\\nmeasure is employed to evaluate a flood risk under model uncertainty. The\\nproblem is solved via a Hamilton-Jacobi-Bellman-Isaacs equation. We explicitly\\nderive an optimal flood mitigation policy along with its existence criteria and\\nthe worst-case probability measure. A backward stochastic differential\\nrepresentation as an alternative formulation is also presented.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Extreme Measures of Agricultural Financial Risk',\n",
       "  'text': 'Risk is an inherent feature of agricultural production and marketing and\\naccurate measurement of it helps inform more efficient use of resources. This\\npaper examines three tail quantile-based risk measures applied to the\\nestimation of extreme agricultural financial risk for corn and soybean\\nproduction in the US: Value at Risk (VaR), Expected Shortfall (ES) and Spectral\\nRisk Measures (SRMs). We use Extreme Value Theory (EVT) to model the tail\\nreturns and present results for these three different risk measures using\\nagricultural futures market data.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Extreme Measures of Agricultural Financial Risk',\n",
       "  'text': 'We compare the estimated risk measures in\\nterms of their size and precision, and find that they are all considerably\\nhigher than normal estimates; they are also quite uncertain, and become more\\nuncertain as the risks involved become more extreme.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Systemic risk measures with markets volatility',\n",
       "  'text': 'As systemic risk has become a hot topic in the financial markets, how to\\nmeasure, allocate and regulate the systemic risk are becoming especially\\nimportant. However, the financial markets are becoming more and more\\ncomplicate, which makes the usual study of systemic risk to be restricted. In\\nthis paper, we will study the systemic risk measures on a special space\\n$L^{p(\\\\cdot)}$ where the variable exponent $p(\\\\cdot)$ is no longer a given real\\nnumber like the space $L^{p}$, but a random variable, which reflects the\\npossible volatility of the financial markets. Finally, the dual representation\\nfor this new systemic risk measures will be studied.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Systemic risk measures with markets volatility',\n",
       "  'text': 'Our results show that\\nevery this new systemic risk measure can be decomposed into a convex certain\\nfunction and a simple-systemic risk measure, which provides a new ideas for\\ndealing with the systemic risk.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Modelling Extremal Dependence for Operational Risk by a Bipartite Graph',\n",
       "  'text': 'We introduce a statistical model for operational losses based on heavy-tailed\\ndistributions and bipartite graphs, which captures the event type and business\\nline structure of operational risk data. The model explicitly takes into\\naccount the Pareto tails of losses and the heterogeneous dependence structures\\nbetween them. We then derive estimators for individual as well as aggregated\\ntail risk, measured in terms of Value-at-Risk and Conditional-Tail-Expectation\\nfor very high confidence levels, and provide also an asymptotically full\\ncapital allocation method. Estimation methods for such tail risk measures and\\ncapital allocations are also proposed and tested on simulated data.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Modelling Extremal Dependence for Operational Risk by a Bipartite Graph',\n",
       "  'text': 'Finally, by\\nhaving access to real-world operational risk losses from the Italian banking\\nsystem, we show that even with a small number of observations, the proposed\\nestimation methods produce reliable estimates, and that quantifying dependence\\nby means of the empirical network has a big impact on estimates at both\\nindividual and aggregate level, as well as for capital allocations.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'A Two-Population Mortality Model to Assess Longevity Basis Risk',\n",
       "  'text': 'Index-based hedging solutions are used to transfer the longevity risk to the\\ncapital markets. However, mismatches between the liability of the hedger and\\nthe hedging instrument cause longevity basis risk. Therefore, an appropriate\\ntwo-population model to measure and assess the longevity basis risk is\\nrequired. In this paper, we aim to construct a two-population mortality model\\nto provide an effective hedge against the longevity basis risk. The reference\\npopulation is modelled by using the Lee-Carter model with the renewal process\\nand exponential jumps proposed by \\\\\"Ozen and \\\\c{S}ahin (2020) and the dynamics\\nof the book population are specified.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'A Two-Population Mortality Model to Assess Longevity Basis Risk',\n",
       "  'text': 'The analysis based on the UK mortality\\ndata indicates that the proposed model for the reference population and the\\ncommon age effect model for the book population provide a better fit compared\\nto the other models considered in the paper. Different two-population models\\nare used to investigate the impact of the sampling risk on the index-based\\nhedge as well as to analyse the risk reduction regarding hedge effectiveness. The results show that the proposed model provides a significant risk reduction\\nwhen mortality jumps and the sampling risk are taken into account.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Measures of Systemic Risk',\n",
       "  'text': 'Systemic risk refers to the risk that the financial system is susceptible to\\nfailures due to the characteristics of the system itself. The tremendous cost\\nof systemic risk requires the design and implementation of tools for the\\nefficient macroprudential regulation of financial institutions. The current\\npaper proposes a novel approach to measuring systemic risk. Key to our construction is a rigorous derivation of systemic risk measures\\nfrom the structure of the underlying system and the objectives of a financial\\nregulator. The suggested systemic risk measures express systemic risk in terms\\nof capital endowments of the financial firms.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Measures of Systemic Risk',\n",
       "  'text': 'Their definition requires two\\ningredients: a cash flow or value model that assigns to the capital allocations\\nof the entities in the system a relevant stochastic outcome; and an\\nacceptability criterion, i.e. a set of random outcomes that are acceptable to a\\nregulatory authority. Systemic risk is measured by the set of allocations of\\nadditional capital that lead to acceptable outcomes. We explain the conceptual\\nframework and the definition of systemic risk measures, provide an algorithm\\nfor their computation, and illustrate their application in numerical case\\nstudies.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Measures of Systemic Risk',\n",
       "  'text': 'Many systemic risk measures in the literature can be viewed as the minimal\\namount of capital that is needed to make the system acceptable after\\naggregating individual risks, hence quantify the costs of a bail-out. In\\ncontrast, our approach emphasizes operational systemic risk measures that\\ninclude both ex post bailout costs as well as ex ante capital requirements and\\nmay be used to prevent systemic crises.',\n",
       "  'meta': {'_split_id': 2}},\n",
       " {'name': 'Collective Decision Dynamics in Group Evacuation: Modeling Tradeoffs and\\n  Optimal Behavior',\n",
       "  'text': 'Quantifying uncertainties in collective human behavior and decision making is\\ncrucial for ensuring public health and safety, enabling effective disaster\\nresponse, informing the design of transportation and communication networks,\\nand guiding the development of new technologies. However, modeling and\\npredicting such behavior is notoriously difficult, due to the influence of a\\nvariety of complex factors such as the availability and uncertainty of\\ninformation, the interaction and influence of social groups and networks, the\\ndegree of risk or time pressure involved in a situation, and differences in\\nindividual personalities and preferences.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Collective Decision Dynamics in Group Evacuation: Modeling Tradeoffs and\\n  Optimal Behavior',\n",
       "  'text': 'Here, we develop a stochastic model\\nof human decision making to describe the empirical behavior of subjects in a\\ncontrolled experiment simulating a natural disaster scenario. We compare the\\nobserved behavior to that of statistically optimal Bayesian decision makers,\\nquantifying the extent to which human decisions are optimal and identifying the\\nconditions in which sub-optimal decisions are made. Finally, we investigate how\\nhuman evacuation strategies change when decisions are made in groups under a\\nvariety of different rules, and whether these group strategy adjustments are\\noptimal or beneficial.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'A Coalition Formation Algorithm for Multi-Robot Task Allocation in\\n  Large-Scale Natural Disasters',\n",
       "  'text': 'In large-scale natural disasters, humans are likely to fail when they attempt\\nto reach high-risk sites or act in search and rescue operations. Robots,\\nhowever, outdo their counterparts in surviving the hazards and handling the\\nsearch and rescue missions due to their multiple and diverse sensing and\\nactuation capabilities. The dynamic formation of optimal coalition of these\\nheterogeneous robots for cost efficiency is very challenging and research in\\nthe area is gaining more and more attention. In this paper, we propose a novel\\nheuristic. Since the population of robots in large-scale disaster settings is\\nvery large, we rely on Quantum Multi-Objective Particle Swarm Optimization\\n(QMOPSO).',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'A Coalition Formation Algorithm for Multi-Robot Task Allocation in\\n  Large-Scale Natural Disasters',\n",
       "  'text': 'The problem is modeled as a multi-objective optimization problem. Simulations with different test cases and metrics, and comparison with other\\nalgorithms such as NSGA-II and SPEA-II are carried out. The experimental\\nresults show that the proposed algorithm outperforms the existing algorithms\\nnot only in terms of convergence but also in terms of diversity and processing\\ntime.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Risk measures and Margining control',\n",
       "  'text': 'This document constitutes the final report of the contractual activity\\nbetween Directa SIM and Dipartimento di Automatica e Informatica, Politecnico\\ndi Torino, on the research topic titled \"quantificazione del rischio di un\\nportafoglio di strumenti finanziari per trading online su device fissi e\\nmobili.\"',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Russian interbank networks: main characteristics and stability with\\n  respect to contagion',\n",
       "  'text': 'Systemic risks characterizing the Russian overnight interbank market from the\\nnetwork point of view are analyzed.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Overspend? Late? Failure? What the Data Say About IT Project Risk in the\\n  Public Sector',\n",
       "  'text': \"Implementing large-scale information and communication technology (IT)\\nprojects carries large risks and easily might disrupt operations, waste\\ntaxpayers' money, and create negative publicity. Because of the high risks it\\nis important that government leaders manage the attendant risks. We analysed a\\nsample of 1,355 public sector IT projects. The sample included large-scale\\nprojects, on average the actual expenditure was $130 million and the average\\nduration was 35 months. Our findings showed that the typical project had no\\ncost overruns and took on average 24% longer than initially expected.\",\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Overspend? Late? Failure? What the Data Say About IT Project Risk in the\\n  Public Sector',\n",
       "  'text': \"However,\\ncomparing the risk distribution with the normative model of a thin-tailed\\ndistribution, projects' actual costs should fall within -30% and +25% of the\\nbudget in nearly 99 out of 100 projects. The data showed, however, that a\\nstaggering 18% of all projects are outliers with cost overruns >25%. Tests\\nshowed that the risk of outliers is even higher for standard software (24%) as\\nwell as in certain project types, e.g., data management (41%), office\\nmanagement (23%), eGovernment (21%) and management information systems (20%). Analysis showed also that projects duration adds risk: every additional year of\\nproject duration increases the average cost risk by 4.2 percentage points.\",\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Overspend? Late? Failure? What the Data Say About IT Project Risk in the\\n  Public Sector',\n",
       "  'text': 'Lastly, we suggest four solutions that public sector organization can take: (1)\\nbenchmark your organization to know where you are, (2) de-bias your IT project\\ndecision-making, (3) reduce the complexities of your IT projects, and (4)\\ndevelop Masterbuilders to learn from the best in the field.',\n",
       "  'meta': {'_split_id': 2}},\n",
       " {'name': 'Monotone Sharpe ratios and related measures of investment performance',\n",
       "  'text': 'We introduce a new measure of performance of investment strategies, the\\nmonotone Sharpe ratio. We study its properties, establish a connection with\\ncoherent risk measures, and obtain an efficient representation for using in\\napplications.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'An Influence Diagram-Based Approach for Estimating Staff Training in\\n  Software Industry',\n",
       "  'text': 'The successful completion of a software development process depends on the\\nanalytical capability and foresightedness of the project manager. For the\\nproject manager, the main intriguing task is to manage the risk factors as they\\nadversely influence the completion deadline. One such key risk factor is staff\\ntraining. The risk of this factor can be avoided by pre-judging the amount of\\ntraining required by the staff. So, a procedure is required to help the project\\nmanager make this decision. This paper presents a system that uses influence\\ndiagrams to implement the risk model to aid decision making.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'An Influence Diagram-Based Approach for Estimating Staff Training in\\n  Software Industry',\n",
       "  'text': 'The system also\\nconsiders the cost of conducting the training, based on various risk factors\\nsuch as, (i) Lack of experience with project software; (ii) Newly appointed\\nstaff; (iii) Staff not well versed with the required quality standards; and\\n(iv) Lack of experience with project environment. The system provides estimated\\nrequirement details for staff training at the beginning of a software\\ndevelopment project.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Defending the future: An MSc module in End User Computing Risk\\n  Management',\n",
       "  'text': 'This paper describes the rationale, curriculum and subject matter of a new\\nMSc module being taught on an MSc Finance and Information Management course at\\nthe University of Wales Institute in Cardiff. Academic research on spreadsheet\\nrisks now has some penetration in academic literature and there is a growing\\nbody of knowledge on the subjects of spreadsheet error, human factors,\\nspreadsheet engineering, \"best practice\", spreadsheet risk management and\\nvarious techniques used to mitigate spreadsheet errors.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Defending the future: An MSc module in End User Computing Risk\\n  Management',\n",
       "  'text': 'This new MSc module in\\nEnd User Computing Risk Management is an attempt to pull all of this research\\nand practitioner experience together to arm the next generation of finance\\nspreadsheet champions with the relevant knowledge, techniques and critical\\nperspective on an emerging discipline.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'The Zeeman Effect in Finance: Libor Spectroscopy and Basis Risk\\n  Management',\n",
       "  'text': 'Once upon a time there was a classical financial world in which all the\\nLibors were equal. Standard textbooks taught that simple relations held, such\\nthat, for example, a 6 months Libor Deposit was replicable with a 3 months\\nLibor Deposits plus a 3x6 months Forward Rate Agreement (FRA), and that Libor\\nwas a good proxy of the risk free rate required as basic building block of\\nno-arbitrage pricing theory. Nowadays, in the modern financial world after the\\ncredit crunch, some Libors are more equal than others, depending on their rate\\ntenor, and classical formulas are history.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'The Zeeman Effect in Finance: Libor Spectroscopy and Basis Risk\\n  Management',\n",
       "  'text': 'Banks are not anymore too \"big to\\nfail\", Libors are fixed by panels of risky banks, and they are risky rates\\nthemselves. These simple empirical facts carry very important consequences in\\nderivative\\'s trading and risk management, such as, for example, basis risk,\\ncollateralization and regulatory pressure in favour of Central Counterparties. Something that should be carefully considered by anyone managing even a single\\nplain vanilla Swap. In this qualitative note we review the problem trying to\\nshed some light on this modern animal farm, recurring to an analogy with\\nquantum physics, the Zeeman effect.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Pricing and Risk Management with High-Dimensional Quasi Monte Carlo and\\n  Global Sensitivity Analysis',\n",
       "  'text': \"We review and apply Quasi Monte Carlo (QMC) and Global Sensitivity Analysis\\n(GSA) techniques to pricing and risk management (greeks) of representative\\nfinancial instruments of increasing complexity. We compare QMC vs standard\\nMonte Carlo (MC) results in great detail, using high-dimensional Sobol' low\\ndiscrepancy sequences, different discretization methods, and specific analyses\\nof convergence, performance, speed up, stability, and error optimization for\\nfinite differences greeks. We find that our QMC outperforms MC in most cases,\\nincluding the highest-dimensional simulations and greeks calculations, showing\\nfaster and more stable convergence to exact or almost exact results.\",\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Pricing and Risk Management with High-Dimensional Quasi Monte Carlo and\\n  Global Sensitivity Analysis',\n",
       "  'text': 'Using GSA,\\nwe are able to fully explain our findings in terms of reduced effective\\ndimension of our QMC simulation, allowed in most cases, but not always, by\\nBrownian bridge discretization. We conclude that, beyond pricing, QMC is a very\\npromising technique also for computing risk figures, greeks in particular, as\\nit allows to reduce the computational effort of high-dimensional Monte Carlo\\nsimulations typical of modern risk management.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Applying economic measures to lapse risk management with machine\\n  learning approaches',\n",
       "  'text': \"Modeling policyholders lapse behaviors is important to a life insurer since\\nlapses affect pricing, reserving, profitability, liquidity, risk management, as\\nwell as the solvency of the insurer. Lapse risk is indeed the most significant\\nlife underwriting risk according to European Insurance and Occupational\\nPensions Authority's Quantitative Impact Study QIS5. In this paper, we\\nintroduce two advanced machine learning algorithms for lapse modeling. Then we\\nevaluate the performance of different algorithms by means of classical\\nstatistical accuracy and profitability measure. Moreover, we adopt an\\ninnovative point of view on the lapse prediction problem that comes from churn\\nmanagement.\",\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Applying economic measures to lapse risk management with machine\\n  learning approaches',\n",
       "  'text': 'We transform the classification problem into a regression question\\nand then perform optimization, which is new for lapse risk management. We apply\\ndifferent algorithms to a large real-world insurance dataset. Our results show\\nthat XGBoost and SVM outperform CART and logistic regression, especially in\\nterms of the economic validation metric. The optimization after transformation\\nbrings out significant and consistent increases in economic gains.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'A Triptych Approach for Reverse Stress Testing of Complex Portfolios',\n",
       "  'text': 'The quest for diversification has led to an increasing number of complex\\nfunds with a high number of strategies and non-linear payoffs. The new\\ngeneration of Alternative Risk Premia (ARP) funds are an example that has been\\nvery popular in recent years. For complex funds like these, a Reverse Stress\\nTest (RST) is regarded by the industry and regulators as a better\\nforward-looking risk measure than a Value-at-Risk (VaR). We present an Extended\\nRST (ERST) triptych approach with three variables: level of plausibility, level\\nof loss and scenario. In our approach, any two of these variables can be\\nderived by providing the third as the input.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'A Triptych Approach for Reverse Stress Testing of Complex Portfolios',\n",
       "  'text': 'We advocate and demonstrate that\\nERST is a powerful tool for both simple linear and complex portfolios and for\\nboth risk management as well as day-to-day portfolio management decisions. An\\nupdated new version of the Levenberg - Marquardt optimization algorithm is\\nintroduced to derive ERST in certain complex cases.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'The PCL Framework: A strategic approach to comprehensive risk management\\n  in response to climate change impacts',\n",
       "  'text': 'The PCL framework provides a comprehensive climate risk management approach\\ngrounded in the assessment of societal values of financial and non-financial\\nloss tolerability. The framework optimizes response action across three main\\nclusters, namely preemptive adaptation (P) or risk reduction, contingent\\narrangements (C), and loss acceptance (L); without a predetermined hierarchy\\nacross them. The PCL Framework aims at including the three clusters of outlay\\nwithin a single continuum, and with the main policy outcome being a balanced\\nportfolio of actions across the three clusters by way of an optimization\\nmodule, such that the aggregate outlay is optimized in the long-term.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'The PCL Framework: A strategic approach to comprehensive risk management\\n  in response to climate change impacts',\n",
       "  'text': 'It is\\nproposed that the approach be applied separately for each hazard to which the\\ntarget community is exposed. While it is currently applied to climate-related\\nrisk management, the methodology can be repurposed for use in other contexts\\nwhere societal buy-in is central.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': \"Four Points Beginner Risk Managers Should Learn from Jeff Holman's\\n  Mistakes in the Discussion of Antifragile\",\n",
       "  'text': \"Using Jeff Holman's comments in Quantitative Finance to illustrate 4 critical\\nerrors students should learn to avoid: 1) Mistaking tails (4th moment) for\\nvolatility (2nd moment), 2) Missing Jensen's Inequality, 3) Analyzing the\\nhedging wihout the underlying, 4) The necessity of a numeraire in finance.\",\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Adaptive Financial Fraud Detection in Imbalanced Data with Time-Varying\\n  Poisson Processes',\n",
       "  'text': 'This paper discusses financial fraud detection in imbalanced dataset using\\nhomogeneous and non-homogeneous Poisson processes. The probability of\\npredicting fraud on the financial transaction is derived. Applying our\\nmethodology to the financial dataset shows a better predicting power than a\\nbaseline approach, especially in the case of higher imbalanced data.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Analytical scores for stress scenarios',\n",
       "  'text': 'In this work, inspired by the Archer-Mouy-Selmi approach, we present two\\nmethodologies for scoring the stress test scenarios used by CCPs for sizing\\ntheir Default Funds. These methodologies can be used by risk managers to\\ncompare different sets of scenarios and could be particularly useful when\\nevaluating the relevance of adding new scenarios to a pre-existing set.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Tackling Climate Change with Machine Learning',\n",
       "  'text': 'Climate change is one of the greatest challenges facing humanity, and we, as\\nmachine learning experts, may wonder how we can help. Here we describe how\\nmachine learning can be a powerful tool in reducing greenhouse gas emissions\\nand helping society adapt to a changing climate. From smart grids to disaster\\nmanagement, we identify high impact problems where existing gaps can be filled\\nby machine learning, in collaboration with other fields. Our recommendations\\nencompass exciting research questions as well as promising business\\nopportunities. We call on the machine learning community to join the global\\neffort against climate change.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'The Asset Liability Management problem of a nuclear operator : a\\n  numerical stochastic optimization approach',\n",
       "  'text': 'We numerically study an Asset Liability Management problem linked to the\\ndecommissioning of French nuclear power plants. We link the risk aversion of\\npractitioners to an optimization problem. Using different price models we show\\nthat the optimal solution is linked to a de-risking management strategy similar\\nto a concave strategy and we propose an effective heuristic to simulate the\\nunderlying optimal strategy. Besides we show that the strategy is stable with\\nrespect to the main parameters involved in the liability problem.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Decoding Stock Market with Quant Alphas',\n",
       "  'text': 'We give an explicit algorithm and source code for extracting expected returns\\nfor stocks from expected returns for alphas. Our algorithm altogether bypasses\\ncombining alphas with weights into \"alpha combos\". Simply put, we have\\ndeveloped a new method for trading alphas which does not involve combining\\nthem. This yields substantial cost savings as alpha combos cost hedge funds\\naround 3% of the P&L, while alphas themselves cost around 10%. Also, the extra\\nlayer of alpha combos, which our new method avoids, adds noise and\\nsuboptimality. We also arrive at our algorithm independently by explicitly\\nconstructing alpha risk models based on position data.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Downside Risk analysis applied to Hedge Funds universe',\n",
       "  'text': \"Hedge Funds are considered as one of the portfolio management sectors which\\nshows a fastest growing for the past decade. An optimal Hedge Fund management\\nrequires an appropriate risk metrics. The classic CAPM theory and its Ratio\\nSharpe fail to capture some crucial aspects due to the strong non-Gaussian\\ncharacter of Hedge Funds statistics. A possible way out to this problem while\\nkeeping the CAPM simplicity is the so-called Downside Risk analysis. One\\nimportant benefit lies in distinguishing between good and bad returns, that is:\\nreturns greater or lower than investor's goal. We revisit most popular Downside\\nRisk indicators and provide new analytical results on them.\",\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Downside Risk analysis applied to Hedge Funds universe',\n",
       "  'text': 'We compute these\\nmeasures by taking the Credit Suisse/Tremont Investable Hedge Fund Index Data\\nand with the Gaussian case as a benchmark. In this way an unusual transversal\\nlecture of the existing Downside Risk measures is provided.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Theoretical Sensitivity Analysis for Quantitative Operational Risk\\n  Management',\n",
       "  'text': 'We study the asymptotic behavior of the difference between the values at risk\\nVaR(L) and VaR(L+S) for heavy tailed random variables L and S for application\\nin sensitivity analysis of quantitative operational risk management within the\\nframework of the advanced measurement approach of Basel II (and III). Here L\\ndescribes the loss amount of the present risk profile and S describes the loss\\namount caused by an additional loss factor.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Theoretical Sensitivity Analysis for Quantitative Operational Risk\\n  Management',\n",
       "  'text': 'We obtain different types of\\nresults according to the relative magnitudes of the thicknesses of the tails of\\nL and S. In particular, if the tail of S is sufficiently thinner than the tail\\nof L, then the difference between prior and posterior risk amounts VaR(L+S) -\\nVaR(L) is asymptotically equivalent to the expectation (expected loss) of S.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'A unified approach to pricing and risk management of equity and credit\\n  risk',\n",
       "  'text': 'We propose a unified framework for equity and credit risk modeling, where the\\ndefault time is a doubly stochastic random time with intensity driven by an\\nunderlying affine factor process. This approach allows for flexible\\ninteractions between the defaultable stock price, its stochastic volatility and\\nthe default intensity, while maintaining full analytical tractability. We\\ncharacterise all risk-neutral measures which preserve the affine structure of\\nthe model and show that risk management as well as pricing problems can be\\ndealt with efficiently by shifting to suitable survival measures. As an\\nexample, we consider a jump-to-default extension of the Heston stochastic\\nvolatility model.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'On Evidence-based Risk Management in Requirements Engineering',\n",
       "  'text': 'Background: The sensitivity of Requirements Engineering (RE) to the context\\nmakes it difficult to efficiently control problems therein, thus, hampering an\\neffective risk management devoted to allow for early corrective or even\\npreventive measures. Problem: There is still little empirical knowledge about\\ncontext-specific RE phenomena which would be necessary for an effective\\ncontext- sensitive risk management in RE. Goal: We propose and validate an\\nevidence-based approach to assess risks in RE using cross-company data about\\nproblems, causes and effects. Research Method: We use survey data from 228\\ncompanies and build a probabilistic network that supports the forecast of\\ncontext-specific RE phenomena.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'On Evidence-based Risk Management in Requirements Engineering',\n",
       "  'text': 'We implement this approach using spreadsheets to\\nsupport a light-weight risk assessment. Results: Our results from an initial\\nvalidation in 6 companies strengthen our confidence that the approach increases\\nthe awareness for individual risk factors in RE, and the feedback further\\nallows for disseminating our approach into practice.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Risks for Academic Research Projects, An Empirical Study of Perceived\\n  Negative Risks and Possible Responses',\n",
       "  'text': \"Academic research projects receive hundreds of billions of dollars of\\ngovernment investment each year. They complement business research projects by\\nfocusing on the generation of new foundational knowledge and addressing\\nsocietal challenges. Despite the importance of academic research, the\\nmanagement of it is often undisciplined and ad hoc. It has been postulated that\\nthe inherent uncertainty and complexity of academic research projects make them\\nchallenging to manage. However, based on this study's analysis of input and\\nvoting from more than 500 academic research team members in facilitated risk\\nmanagement sessions, the most important perceived risks are general, as opposed\\nto being research specific.\",\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Risks for Academic Research Projects, An Empirical Study of Perceived\\n  Negative Risks and Possible Responses',\n",
       "  'text': \"Overall participants' top risks related to funding,\\nteam instability, unreliable partners, study participant recruitment, and data\\naccess. Many of these risks would require system- or organization-level\\nresponses that are beyond the scope of individual academic research teams.\",\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'PS2: Managing the next step in the Pan-STARRS wide field survey system',\n",
       "  'text': 'The Panoramic Survey Telescope and Rapid Response System (Pan-STARRS) is\\nunique among the existing or planned major ground-based optical survey systems\\nas the only \"distributed aperture\" system. The concept of increasing system\\n\\\\\\'etendue by replicating small telescopes and digital cameras presents both\\nmanagement opportunities and challenges. The focus in this paper is on\\nmanagement lessons learned from PS1, and how those have been used to form the\\nmanagement plan for PS2. The management plan components emphasized here include\\ntechnical development, financial and schedule planning, and critical path and\\nrisk management. Finally, the status and schedule for PS2 are presented.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Fostering Project Scheduling and Controlling Risk Management',\n",
       "  'text': 'Deployment of emerging technologies and rapid change in industries has\\ncreated a lot of risk for initiating the new projects. Many techniques and\\nsuggestions have been introduced but still lack the gap from various\\nprospective. This paper proposes a reliable project scheduling approach. The\\nobjectives of project scheduling approach are to focus on critical chain\\nschedule and risk management. Several risks and reservations exist in projects. These critical reservations may not only foil the projects to be finished\\nwithin time limit and budget, but also degrades the quality, and operational\\nprocess. In the proposed approach, the potential risks of project are\\ncritically analyzed.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Fostering Project Scheduling and Controlling Risk Management',\n",
       "  'text': 'To overcome these potential risks, fuzzy failure mode and\\neffect analysis (FMEA) is introduced. In addition, several affects of each risk\\nagainst each activity are evaluated. We use Monte Carlo simulation that helps\\nto calculate the total time of project. Our approach helps to control risk\\nmitigation that is determined using event tree analysis and fault tree\\nanalysis. We also implement distribute critical chain schedule for reliable\\nscheduling that makes the project to be implemented within defined plan and\\nschedule. Finally, adaptive procedure with density (APD) is deployed to get\\nreasonable feeding buffer time and project buffer time.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'A simulation of the insurance industry: The problem of risk model\\n  homogeneity',\n",
       "  'text': 'We develop an agent-based simulation of the catastrophe insurance and\\nreinsurance industry and use it to study the problem of risk model homogeneity. The model simulates the balance sheets of insurance firms, who collect premiums\\nfrom clients in return for ensuring them against intermittent, heavy-tailed\\nrisks. Firms manage their capital and pay dividends to their investors, and use\\neither reinsurance contracts or cat bonds to hedge their tail risk. The model\\ngenerates plausible time series of profits and losses and recovers stylized\\nfacts, such as the insurance cycle and the emergence of asymmetric, long tailed\\nfirm size distributions.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'A simulation of the insurance industry: The problem of risk model\\n  homogeneity',\n",
       "  'text': 'We use the model to investigate the problem of risk\\nmodel homogeneity. Under Solvency II, insurance companies are required to use\\nonly certified risk models. This has led to a situation in which only a few\\nfirms provide risk models, creating a systemic fragility to the errors in these\\nmodels. We demonstrate that using too few models increases the risk of\\nnonpayment and default while lowering profits for the industry as a whole. The\\npresence of the reinsurance industry ameliorates the problem but does not\\nremove it. Our results suggest that it would be valuable for regulators to\\nincentivize model diversity.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'A simulation of the insurance industry: The problem of risk model\\n  homogeneity',\n",
       "  'text': 'The framework we develop here provides a first\\nstep toward a simulation model of the insurance industry for testing policies\\nand strategies for better capital management.',\n",
       "  'meta': {'_split_id': 2}},\n",
       " {'name': 'Distributed simulation of city inundation by coupled surface and\\n  subsurface porous flow for urban flood decision support system',\n",
       "  'text': 'We present a decision support system for flood early warning and disaster\\nmanagement. It includes the models for data-driven meteorological predictions,\\nfor simulation of atmospheric pressure, wind, long sea waves and seiches; a\\nmodule for optimization of flood barrier gates operation; models for stability\\nassessment of levees and embankments, for simulation of city inundation\\ndynamics and citizens evacuation scenarios. The novelty of this paper is a\\ncoupled distributed simulation of surface and subsurface flows that can predict\\ninundation of low-lying inland zones far from the submerged waterfront areas,\\nas observed in St. Petersburg city during the floods.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Distributed simulation of city inundation by coupled surface and\\n  subsurface porous flow for urban flood decision support system',\n",
       "  'text': 'All the models are\\nwrapped as software services in the CLAVIRE platform for urgent computing,\\nwhich provides workflow management and resource orchestration.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Designing Emergency Response Pipelines : Lessons and Challenges',\n",
       "  'text': 'Emergency response to incidents such as accidents, crimes, and fires is a\\nmajor problem faced by communities. Emergency response management comprises of\\nseveral stages and sub-problems like forecasting, resource allocation, and\\ndispatch. The design of principled approaches to tackle each problem is\\nnecessary to create efficient emergency response management (ERM) pipelines. Over the last six years, we have worked with several first responder\\norganizations to design ERM pipelines. In this paper, we highlight some of the\\nchallenges that we have identified and lessons that we have learned through our\\nexperience in this domain.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Designing Emergency Response Pipelines : Lessons and Challenges',\n",
       "  'text': 'Such challenges are particularly relevant for\\npractitioners and researchers, and are important considerations even in the\\ndesign of response strategies to mitigate disasters like floods and\\nearthquakes.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Complexity in the Context of Systems Approach to Project Management',\n",
       "  'text': 'Complexity is an inherent attribute of any project. The purpose of defining\\nand documenting complexity is to have an early warning tool allowing a project\\nteam to focus on certain areas and aspects of the project in order to prevent\\nand alleviate future risks and issues caused by this complexity. The main\\ncontribution of this paper is to present a systematic view of complexity in\\nproject management by identifying its key attributes and classifying complexity\\nby these attributes. A \"complexity taxonomy\", based on a survey of the existing\\ncomplexity literature, is developed and discussed including the product,\\nproject, and external environment dimensions.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Complexity in the Context of Systems Approach to Project Management',\n",
       "  'text': 'We show how complexity types are\\ndescribed through simple real life examples and business cases. Then we develop\\na framework (tool) for applying the notion of complexity as an early warning\\ntool for a project manager in order to timely foresee future risks and\\nproblems. The paper is intended for researchers in complexity, project\\nmanagement, information systems, technology solutions and business management,\\nand also for information specialists, project managers, program managers,\\nfinancial staff and technology directors.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'National Infrastructure Contingencies: Survey of Wireless Technology\\n  Support',\n",
       "  'text': 'In modern society, the flow of information has become the lifeblood of\\ncommerce and social interaction. This movement of data supports most aspects of\\nthe United States economy in particular, as well as, serving as the vehicle\\nupon which governmental agencies react to social conditions. In addition, it is\\nunderstood that the continuance of efficient and reliable data communications\\nduring times of national or regional disaster remains a priority in the United\\nStates. The coordination of emergency response and area revitalization /\\nrehabilitation efforts between local, state, and federal emergency response is\\nincreasingly necessary as agencies strive to work more seamlessly between the\\naffected organizations.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'National Infrastructure Contingencies: Survey of Wireless Technology\\n  Support',\n",
       "  'text': 'Additionally, international support is often made\\navailable to react to such adverse conditions as wildfire suppression scenarios\\nand therefore require the efficient management of workforce and associated\\nlogistics support. It is through the examination of the issues related to un-tethered data\\ntransmission during infrastructure contingencies that responders may best\\ntailor a unified approach to the rapid recovery after disasters occur.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Robots as-a-Service in Cloud Computing: Search and Rescue in Large-scale\\n  Disasters Case Study',\n",
       "  'text': 'Internet of Things (IoT) is expected to enable a myriad of applications by\\ninterconnecting objects - such as sensors and robots - over the Internet. IoT\\napplications range from healthcare to autonomous vehicles and include disaster\\nmanagement. Enabling these applications in cloud environments requires the\\ndesign of appropriate IoT Infrastructure-as-a-Service (IoT IaaS) to ease the\\nprovisioning of the IoT objects as cloud services. This paper discusses a case\\nstudy on search and rescue IoT applications in large-scale disaster scenarios. It proposes an IoT IaaS architecture that virtualizes robots (IaaS for robots)\\nand provides them to the upstream applications as-a-Service. Node- and\\nNetwork-level robots virtualization are supported.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Robots as-a-Service in Cloud Computing: Search and Rescue in Large-scale\\n  Disasters Case Study',\n",
       "  'text': 'The proposed architecture\\nmeets a set of identified requirements, such as the need for a unified\\ndescription model for heterogeneous robots, publication/discovery mechanism,\\nand federation with other IaaS for robots when needed. A validating proof of\\nconcept is built and experiments are made to evaluate its performance. Lessons\\nlearned and prospective research directions are discussed.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Deep Learning Benchmarks and Datasets for Social Media Image\\n  Classification for Disaster Response',\n",
       "  'text': 'During a disaster event, images shared on social media helps crisis managers\\ngain situational awareness and assess incurred damages, among other response\\ntasks. Recent advances in computer vision and deep neural networks have enabled\\nthe development of models for real-time image classification for a number of\\ntasks, including detecting crisis incidents, filtering irrelevant images,\\nclassifying images into specific humanitarian categories, and assessing the\\nseverity of damage. Despite several efforts, past works mainly suffer from\\nlimited resources (i.e., labeled images) available to train more robust deep\\nlearning models. In this study, we propose new datasets for disaster type\\ndetection, and informativeness classification, and damage severity assessment.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Deep Learning Benchmarks and Datasets for Social Media Image\\n  Classification for Disaster Response',\n",
       "  'text': 'Moreover, we relabel existing publicly available datasets for new tasks. We\\nidentify exact- and near-duplicates to form non-overlapping data splits, and\\nfinally consolidate them to create larger datasets. In our extensive\\nexperiments, we benchmark several state-of-the-art deep learning models and\\nachieve promising results. We release our datasets and models publicly, aiming\\nto provide proper baselines as well as to spur further research in the crisis\\ninformatics community.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Discussion of \"2004 IMS Medallion Lecture: Local Rademacher complexities\\n  and oracle inequalities in risk minimization\" by V. Koltchinskii',\n",
       "  'text': 'Discussion of \"2004 IMS Medallion Lecture: Local Rademacher complexities and\\noracle inequalities in risk minimization\" by V. Koltchinskii [arXiv:0708.0083]',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': \"Discussion of ``2004 IMS Medallion Lecture: Local Rademacher\\n  complexities and oracle inequalities in risk minimization'' by V.\\n  Koltchinskii\",\n",
       "  'text': \"Discussion of ``2004 IMS Medallion Lecture: Local Rademacher complexities and\\noracle inequalities in risk minimization'' by V. Koltchinskii [arXiv:0708.0083]\",\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': \"Discussion of ``2004 IMS Medallion Lecture: Local Rademacher\\n  complexities and oracle inequalities in risk minimization'' by V.\\n  Koltchinskii\",\n",
       "  'text': \"Discussion of ``2004 IMS Medallion Lecture: Local Rademacher complexities and\\noracle inequalities in risk minimization'' by V. Koltchinskii [arXiv:0708.0083]\",\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': \"Discussion of ``2004 IMS Medallion Lecture: Local Rademacher\\n  complexities and oracle inequalities in risk minimization'' by V.\\n  Koltchinskii\",\n",
       "  'text': \"Discussion of ``2004 IMS Medallion Lecture: Local Rademacher complexities and\\noracle inequalities in risk minimization'' by V. Koltchinskii [arXiv:0708.0083]\",\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': \"Discussion of ``2004 IMS Medallion Lecture: Local Rademacher\\n  complexities and oracle inequalities in risk minimization'' by V.\\n  Koltchinskii\",\n",
       "  'text': \"Discussion of ``2004 IMS Medallion Lecture: Local Rademacher complexities and\\noracle inequalities in risk minimization'' by V. Koltchinskii [arXiv:0708.0083]\",\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Inf-convolution of G-expectations',\n",
       "  'text': 'In this paper we will discuss the optimal risk transfer problems when risk\\nmeasures are generated by G-expectations, and we present the relationship\\nbetween inf-convolution of G-expectations and the inf-convolution of drivers G.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Spin Glass Model of Operational Risk',\n",
       "  'text': 'We analyze operational risk in terms of a spin glass model. Several regimes\\nare investigated, as a functions of the parameters that characterize the\\ndynamics. The system is found to be robust against variations of these\\nparameters. We unveil the presence of limit cycles and scrutinize the features\\nof the asymptotic state.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Implied correlation from VaR',\n",
       "  'text': 'Value at risk (VaR) is a risk measure that has been widely implemented by\\nfinancial institutions. This paper measures the correlation among asset price\\nchanges implied from VaR calculation. Empirical results using US and UK equity\\nindexes show that implied correlation is not constant but tends to be higher\\nfor events in the left tails (crashes) than in the right tails (booms).',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Complete duality for quasiconvex dynamic risk measures on modules of the\\n  $L^{p}$-type',\n",
       "  'text': 'In the conditional setting we provide a complete duality between quasiconvex\\nrisk measures defined on $L^{0}$ modules of the $L^{p}$ type and the\\nappropriate class of dual functions. This is based on a general result which\\nextends the usual Penot-Volle representation for quasiconvex real valued maps.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Risk Measures in a Regime Switching Model Capturing Stylized Facts',\n",
       "  'text': 'We pick up the regime switching model for asset returns introduced by Rogers\\nand Zhang. The calibration involves various markets including implied\\nvolatility in order to gain additional predictive power. We focus on the\\ncalculation of risk measures by Fourier methods that have successfully been\\napplied to option pricing and analyze the accuracy of the results.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Concave risk measures in international capital regulation',\n",
       "  'text': 'We show that some specific market risk measures implied by current\\ninternational capital regulation (the Basel Accords and the Capital Adequacy\\nDirective of the European Union) violate the obvious requirement of convexity\\nin some regions in the space of portfolio weights.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Analytical models of operational risk and new results on the correlation\\n  problem',\n",
       "  'text': 'We propose a portfolio approach for operational risk quantification based on\\na class of analytical models from which we derive new results on the\\ncorrelation problem. In particular, we show that uniform correlation is a\\nrobust assumption for measuring capital charges in these models.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Vector-Valued Multivariate Conditional Value-at-Risk',\n",
       "  'text': 'In this study, we propose a new definition of multivariate conditional\\nvalue-at-risk (MCVaR) as a set of vectors for discrete probability spaces. We\\nexplore the properties of the vector-valued MCVaR (VMCVaR) and show the\\nadvantages of VMCVaR over the existing definitions given for continuous random\\nvariables when adapted to the discrete case.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Feasibility Study on Disaster Management with Hybrid Network of LTE and\\n  Satellite Links',\n",
       "  'text': 'We are highly vulnerable to either natural or artificial catastrophes and\\ntherefore, Public Protection and Disaster Relief (PPDR) operators need reliable\\nwireless communications for successful operations especially in critical rescue\\nmissions. PPDR dedicated or commercial terrestrial networks have always been\\nused which at most times lead to unsuccessful operations. This is due to the\\nfact these networks are all infrastructure-based which can be destroyed, fail\\nto deliver the required service or the networks are not able to support and\\nsustain the sudden traffic surge.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Feasibility Study on Disaster Management with Hybrid Network of LTE and\\n  Satellite Links',\n",
       "  'text': 'Long-Term Evolution (LTE) is earmarked as the\\nfuture candidate technology for PPDR purpose and so much have been put into it\\nin terms of research, perhaps suitable architecture that will meet\\nmission-critical requirements can be developed. This can only work if\\nterrestrial networks will always be available. Unfortunately, in worst case\\nscenarios, infrastructures might get damaged totally or might be destroyed by\\nsubsequent disasters. As a result, adequate guarantees can only be possible in\\nthe hypothesis of very high financial involvement. Fortunately, considering\\navailability, coverage ubiquity and reliability, satellite technologies have\\nlately proven good.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Feasibility Study on Disaster Management with Hybrid Network of LTE and\\n  Satellite Links',\n",
       "  'text': 'So, to maximize the high channel performance of terrestrial\\nnetworks and the availability and reliability of non-terrestrial networks, the\\nsolution lies in a hybrid system. It is on this ground that this work deals\\nwith the integration of LTE and satellite networks in both infrastructure-based\\nand infrastructure-less topologies for PPDR purpose. It is aim at providing\\npeople trapped in disaster and field operators with a transparent accessibility\\nand guaranteed coverage even when infrastructures are damaged. The requirements\\nare defined and the model simulated. The network is able to provide network\\ncoverage, enhanced capacity and promised greater resilience.',\n",
       "  'meta': {'_split_id': 2}},\n",
       " {'name': 'RDSP: Rapidly Deployable Wireless Ad Hoc System for Post-Disaster\\n  Management',\n",
       "  'text': 'In post-disaster scenarios, such as after floods, earthquakes, and in war\\nzones, the cellular communication infrastructure may be destroyed or seriously\\ndisrupted. In such emergency scenarios, it becomes very important for first aid\\nresponders to communicate with other rescue teams in order to provide feedback\\nto both the central office and the disaster survivors. To address this issue,\\nrapidly deployable systems are required to re-establish connectivity and assist\\nusers and first responders in the region of incident. In this work, we describe\\nthe design, implementation, and evaluation of a rapidly deployable system for\\nfirst response applications in post-disaster situations, named RDSP.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'RDSP: Rapidly Deployable Wireless Ad Hoc System for Post-Disaster\\n  Management',\n",
       "  'text': 'The\\nproposed system helps early rescue responders and victims by sharing their\\nlocation information to remotely located servers by utilizing a novel routing\\nscheme. This novel routing scheme consists of the Dynamic ID Assignment (DIA)\\nalgorithm and the Minimum Maximum Neighbor (MMN) algorithm. The DIA algorithm\\nis used by relay devices to dynamically select their IDs on the basis of all\\nthe available IDs of networks. Whereas, the MMN algorithm is used by the client\\nand relay devices to dynamically select their next neighbor relays for the\\ntransmission of messages.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'RDSP: Rapidly Deployable Wireless Ad Hoc System for Post-Disaster\\n  Management',\n",
       "  'text': \"The RDSP contains three devices; the client device\\nsends the victim's location information to the server, the relay device relays\\ninformation between client and server device, the server device receives\\nmessages from the client device to alert the rescue team. We deployed and\\nevaluated our system in the outdoor environment of the university campus. The\\nexperimental results show that the RDSP system reduces the message delivery\\ndelay and improves the message delivery ratio with lower communication\\noverhead.\",\n",
       "  'meta': {'_split_id': 2}},\n",
       " {'name': 'Utilizing Microblogs for Assisting Post-Disaster Relief Operations via\\n  Matching Resource Needs and Availabilities',\n",
       "  'text': 'During a disaster event, two types of information that are especially useful\\nfor coordinating relief operations are needs and availabilities of resources\\n(e.g., food, water, medicines) in the affected region. Information posted on\\nmicroblogging sites is increasingly being used for assisting post-disaster\\nrelief operations. In this context, two practical challenges are (i)~to\\nidentify tweets that inform about resource needs and availabilities (termed as\\nneed-tweets and availability-tweets respectively), and (ii)~to automatically\\nmatch needs with appropriate availabilities. While several works have addressed\\nthe first problem, there has been little work on automatically matching needs\\nwith availabilities.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Utilizing Microblogs for Assisting Post-Disaster Relief Operations via\\n  Matching Resource Needs and Availabilities',\n",
       "  'text': 'The few prior works that attempted matching only\\nconsidered the resources, and no attempt has been made to understand other\\naspects of needs/availabilities that are essential for matching in practice. In\\nthis work, we develop a methodology for understanding five important aspects of\\nneed-tweets and availability-tweets, including what resource and what quantity\\nis needed/available, the geographical location of the need/availability, and\\nwho needs / is providing the resource. Understanding these aspects helps us to\\naddress the need-availability matching problem considering not only the\\nresources, but also other factors such as the geographical proximity between\\nthe need and the availability.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Utilizing Microblogs for Assisting Post-Disaster Relief Operations via\\n  Matching Resource Needs and Availabilities',\n",
       "  'text': 'To our knowledge, this study is the first\\nattempt to develop methods for understanding the semantics of need-tweets and\\navailability-tweets. We also develop a novel methodology for matching\\nneed-tweets with availability-tweets, considering both resource similarity and\\ngeographical proximity. Experiments on two datasets corresponding to two\\ndisaster events, demonstrate that our proposed methods perform substantially\\nbetter matching than those in prior works.',\n",
       "  'meta': {'_split_id': 2}},\n",
       " {'name': 'Improving Emergency Response during Hurricane Season using Computer\\n  Vision',\n",
       "  'text': 'We have developed a framework for crisis response and management that\\nincorporates the latest technologies in computer vision (CV), inland flood\\nprediction, damage assessment and data visualization. The framework uses data\\ncollected before, during, and after the crisis to enable rapid and informed\\ndecision making during all phases of disaster response. Our computer-vision\\nmodel analyzes spaceborne and airborne imagery to detect relevant features\\nduring and after a natural disaster and creates metadata that is transformed\\ninto actionable information through web-accessible mapping tools. In\\nparticular, we have designed an ensemble of models to identify features\\nincluding water, roads, buildings, and vegetation from the imagery.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Improving Emergency Response during Hurricane Season using Computer\\n  Vision',\n",
       "  'text': \"We have\\ninvestigated techniques to bootstrap and reduce dependency on large data\\nannotation efforts by adding use of open source labels including OpenStreetMaps\\nand adding complementary data sources including Height Above Nearest Drainage\\n(HAND) as a side channel to the network's input to encourage it to learn other\\nfeatures orthogonal to visual characteristics. Modeling efforts include\\nmodification of connected U-Nets for (1) semantic segmentation, (2) flood line\\ndetection, and (3) for damage assessment. In particular for the case of damage\\nassessment, we added a second encoder to U-Net so that it could learn pre-event\\nand post-event image features simultaneously.\",\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Improving Emergency Response during Hurricane Season using Computer\\n  Vision',\n",
       "  'text': \"Through this method, the network\\nis able to learn the difference between the pre- and post-disaster images, and\\ntherefore more effectively classify the level of damage. We have validated our\\napproaches using publicly available data from the National Oceanic and\\nAtmospheric Administration (NOAA)'s Remote Sensing Division, which displays the\\ncity and street-level details as mosaic tile images as well as data released as\\npart of the Xview2 challenge.\",\n",
       "  'meta': {'_split_id': 2}},\n",
       " {'name': 'Monitoring the Impacts of a Tailings Dam Failure Using Satellite Images',\n",
       "  'text': \"Monitoring dam failures using satellite images provides first responders with\\nefficient management of early interventions. It is also equally important to\\nmonitor spatial and temporal changes in the inundation area to track the\\npost-disaster recovery. On January 25th, 2019, the tailings dam of the\\nC\\\\'orrego do Feij\\\\~ao iron ore mine, located in Brumadinho, Brazil, collapsed. This disaster caused more than 230 fatalities and 30 missing people leading to\\ndamage on the order of multiple billions of dollars. This study uses Sentinel-2\\nsatellite images to map the inundation area and assess and delineate the land\\nuse and land cover impacted by the dam failure.\",\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Monitoring the Impacts of a Tailings Dam Failure Using Satellite Images',\n",
       "  'text': 'The images correspond to data\\ncaptures from January 22nd (3 days before), and February 02 (7 days after the\\ncollapse). Satellite images of the region were classified for before and\\naftermath of the disaster implementing a machine learning algorithm. In order\\nto have sufficient land cover types to validate the quality and accuracy of the\\nalgorithm, 7 classes were defined: mine, forest, build-up, river, agricultural,\\nclear water, and grassland. The developed classification algorithm yielded a\\nhigh accuracy (99%) for the image before the collapse.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Monitoring the Impacts of a Tailings Dam Failure Using Satellite Images',\n",
       "  'text': 'This paper determines\\nland cover impact using two different models, 1) by using the trained network\\nin the \"after\" image, and 2) by creating a second network, trained in a subset\\nof points of the \"after\" image, and then comparing the land cover results of\\nthe two trained networks. In the first model, applying the trained network to\\nthe \"after\" image, the accuracy is still high (86%), but lower than using the\\nsecond model (98%).',\n",
       "  'meta': {'_split_id': 2}},\n",
       " {'name': 'Monitoring the Impacts of a Tailings Dam Failure Using Satellite Images',\n",
       "  'text': 'This strategy can be applied at a low cost for monitoring\\nand assessment by using openly available satellite information and, in case of\\ndam collapse or with a larger budget, higher resolution and faster data can be\\nobtained by fly-overs on the area of concern.',\n",
       "  'meta': {'_split_id': 3}},\n",
       " {'name': 'The Network of Counterparty Risk: Analysing Correlations in OTC\\n  Derivatives',\n",
       "  'text': 'Counterparty risk denotes the risk that a party defaults in a bilateral\\ncontract. This risk not only depends on the two parties involved, but also on\\nthe risk from various other contracts each of these parties holds. In rather\\ninformal markets, such as the OTC (over-the-counter) derivative market,\\ninstitutions only report their aggregated quarterly risk exposure, but no\\ndetails about their counterparties. Hence, little is known about the\\ndiversification of counterparty risk. In this paper, we reconstruct the\\nweighted and time-dependent network of counterparty risk in the OTC derivatives\\nmarket of the United States between 1998 and 2012.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'The Network of Counterparty Risk: Analysing Correlations in OTC\\n  Derivatives',\n",
       "  'text': 'To proxy unknown bilateral\\nexposures, we first study the co-occurrence patterns of institutions based on\\ntheir quarterly activity and ranking in the official report. The network\\nobtained this way is further analysed by a weighted k-core decomposition, to\\nreveal a core-periphery structure. This allows us to compare the activity-based\\nranking with a topology-based ranking, to identify the most important\\ninstitutions and their mutual dependencies. We also analyse correlations in\\nthese activities, to show strong similarities in the behavior of the core\\ninstitutions. Our analysis clearly demonstrates the clustering of counterparty\\nrisk in a small set of about a dozen US banks.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'The Network of Counterparty Risk: Analysing Correlations in OTC\\n  Derivatives',\n",
       "  'text': 'This not only increases the\\ndefault risk of the central institutions, but also the default risk of\\nperipheral institutions which have contracts with the central ones. Hence, all\\ninstitutions indirectly have to bear (part of) the counterparty risk of all\\nothers, which needs to be better reflected in the price of OTC derivatives.',\n",
       "  'meta': {'_split_id': 2}},\n",
       " {'name': 'News-sentiment networks as a risk indicator',\n",
       "  'text': 'To understand the relationship between news sentiment and company stock price\\nmovements, and to better understand connectivity among companies, we define an\\nalgorithm for measuring sentiment-based network risk. The algorithm ranks\\ncompanies in networks of co-occurrences, and measures sentiment-based risk, by\\ncalculating both individual risks and aggregated network risks. We extract\\nrelative sentiment for companies to get a measure of individual company risk,\\nand input it into our risk model together with co-occurrences of companies\\nextracted from news on a quarterly basis.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'News-sentiment networks as a risk indicator',\n",
       "  'text': 'We can show that the highest\\nquarterly risk value outputted by our risk model, is correlated to a higher\\nchance of stock price decline, up to 70 days after a risk measurement. Our\\nresults show that the highest difference in the probability of stock price\\ndecline, compared to the benchmark containing all risk values for the same\\nperiod, is during the interval from 21 to 30 days after a quarterly\\nmeasurement. The highest average probability of company stock price decline, is\\nfound at a delay of 28 days, after a company has reached its maximum risk\\nvalue.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'News-sentiment networks as a risk indicator',\n",
       "  'text': 'The highest probability differences for a daily decline were calculated\\nto be 13 percentage points.',\n",
       "  'meta': {'_split_id': 2}},\n",
       " {'name': 'A Multi-Agent-Based Rolling Optimization Method for Restoration\\n  Scheduling of Electrical Distribution Systems with Distributed Generation',\n",
       "  'text': 'Resilience against major disasters is the most essential characteristic of\\nfuture electrical distribution systems (EDS). A multi-agent-based rolling\\noptimization method for EDS restoration scheduling is proposed in this paper. When a blackout occurs, considering the risk of losing the centralized\\nauthority due to the failure of the common core communication network, the\\nagents available after disasters or cyber-attacks identify the\\ncommunication-connected parts (CCPs) in the EDS with distributed communication. A multi-time interval optimization model is formulated and solved by the agents\\nfor the restoration scheduling of a CCP. A rolling optimization process for the\\nentire EDS restoration is proposed.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'A Multi-Agent-Based Rolling Optimization Method for Restoration\\n  Scheduling of Electrical Distribution Systems with Distributed Generation',\n",
       "  'text': 'During the scheduling/rescheduling in the\\nrolling process, the CCPs in the EDS are reidentified and the restoration\\nschedules for the CCPs are updated. Through decentralized decision-making and\\nrolling optimization, EDS restoration scheduling can automatically start and\\nperiodically update itself, providing effective solutions for EDS restoration\\nscheduling in a blackout event. A modified IEEE 123-bus EDS is utilized to\\ndemonstrate the effectiveness of the proposed method.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Natural Gas Maximal Load Delivery for Multi-contingency Analysis',\n",
       "  'text': 'As the use of renewable generation has increased, electric power systems have\\nbecome increasingly reliant on natural gas-fired power plants as fast ramping\\nsources for meeting fluctuating bulk power demands. This dependence has\\nintroduced new vulnerabilities to the power grid, including disruptions to gas\\ntransmission networks from natural and man-made disasters. To address the\\noperational challenges arising from these disruptions, we consider the task of\\ndetermining a feasible steady-state operating point for a damaged gas pipeline\\nnetwork while ensuring the maximal delivery of load. We formulate the\\nmixed-integer nonconvex maximal load delivery (MLD) problem, which proves\\ndifficult to solve on large-scale networks.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Natural Gas Maximal Load Delivery for Multi-contingency Analysis',\n",
       "  'text': 'To address this challenge, we\\npresent a mixed-integer convex relaxation of the MLD problem and use it to\\ndetermine bounds on the transport capacity of a gas pipeline system. To\\ndemonstrate the effectiveness of the relaxation, the exact and relaxed\\nformulations are compared across a large number of randomized damage scenarios\\non nine natural gas pipeline network models ranging in size from 11 to 4197\\njunctions. A proof of concept application, which assumes network damage from a\\nset of synthetically generated earthquakes, is also presented to demonstrate\\nthe utility of the proposed optimization-based capacity evaluation in the\\ncontext of risk assessment for natural disasters.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Natural Gas Maximal Load Delivery for Multi-contingency Analysis',\n",
       "  'text': 'For all but the largest\\nnetwork, the relaxation-based method is found to be suitable for use in\\nevaluating the impacts of multi-contingency network disruptions, often\\nconverging to the optimal solution of the relaxed formulation in less than ten\\nseconds.',\n",
       "  'meta': {'_split_id': 2}},\n",
       " {'name': 'Probability distribution of drawdowns in risky investments',\n",
       "  'text': \"We study the risk criterion for investments based on the drawdown from the\\nmaximal value of the capital in the past. Depending on investor's risk\\nattitude, thus his risk exposure, we find that the distribution of these\\ndrawdowns follows a general power law. In particular, if the risk exposure is\\nKelly-optimal, the exponent of this power law has the borderline value of 2,\\ni.e. the average drawdown is just about to diverge\",\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Credit Risk Contributions to Value-at-Risk and Expected Shortfall',\n",
       "  'text': 'This paper presents analytical solutions to the problem of how to calculate\\nsensible VaR (Value-at-Risk) and ES (Expected Shortfall) contributions in the\\nCreditRisk+ methodology. Via the ES contributions, ES itself can be exactly\\ncomputed in finitely many steps. The methods are illustrated by numerical\\nexamples.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Long range Ising model for credit risk modeling in homogeneous\\n  portfolios',\n",
       "  'text': 'Within the framework of maximum entropy principle we show that the\\nfinite-size long-range Ising model is the adequate model for the description of\\nhomogeneous credit portfolios and the computation of credit risk when default\\ncorrelations between the borrowers are included. The exact analysis of the\\nmodel suggest that when the correlation increases a first-order-like transition\\nmay occur inducing a sudden risk increase. Such a feature is not reproduced by\\nthe standard models used in credit risk modeling.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Value-at-Risk and expected shortfall for linear portfolios with\\n  elliptically distributed risk factors',\n",
       "  'text': 'In this paper, we generalize the parametric delta-VaR method from portfolios\\nwith normally distributed risk factors to portfolios with elliptically\\ndistributed ones. We treat both the expected shortfall and the Value-at-Risk of\\nsuch portfolios. Special attention is given to the particular case of a\\nmultivariate t-distribution.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Calculation of aggregate loss distributions',\n",
       "  'text': 'Estimation of the operational risk capital under the Loss Distribution\\nApproach requires evaluation of aggregate (compound) loss distributions which\\nis one of the classic problems in risk theory. Closed-form solutions are not\\navailable for the distributions typically used in operational risk. However\\nwith modern computer processing power, these distributions can be calculated\\nvirtually exactly using numerical methods. This paper reviews numerical\\nalgorithms that can be successfully used to calculate the aggregate loss\\ndistributions. In particular Monte Carlo, Panjer recursion and Fourier\\ntransformation methods are presented and compared. Also, several closed-form\\napproximations based on moment matching and asymptotic result for heavy-tailed\\ndistributions are reviewed.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Set-valued risk measures for conical market models',\n",
       "  'text': 'Set-valued risk measures on $L^p_d$ with $0 \\\\leq p \\\\leq \\\\infty$ for conical\\nmarket models are defined, primal and dual representation results are given. The collection of initial endowments which allow to super-hedge a multivariate\\nclaim are shown to form the values of a set-valued sublinear (coherent) risk\\nmeasure. Scalar risk measures with multiple eligible assets also turn out to be\\na special case within the set-valued framework.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Living on the multi-dimensional edge: seeking hidden risks using regular\\n  variation',\n",
       "  'text': 'Multivariate regular variation plays a role assessing tail risk in diverse\\napplications such as finance, telecommunications, insurance and environmental\\nscience. The classical theory, being based on an asymptotic model, sometimes\\nleads to inaccurate and useless estimates of probabilities of joint tail\\nregions. This problem can be partly ameliorated by using hidden regular\\nvariation [Resnick, 2002, Mitra and Resnick, 2010]. We offer a more flexible\\ndefinition of hidden regular variation that provides improved risk estimates\\nfor a larger class of risk tail regions.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Closed form solutions of measures of systemic risk',\n",
       "  'text': 'This paper derives -- considering a Gaussian setting -- closed form solutions\\nof the statistics that Adrian and Brunnermeier and Acharya et al. have\\nsuggested as measures of systemic risk to be attached to individual banks. The\\nstatistics equal the product of statistic specific Beta-coefficients with the\\nmean corrected Value at Risk. Hence, the measures of systemic risks are closely\\nrelated to well known concepts of financial economics. Another benefit of the\\nanalysis is that it is revealed how the concepts are related to each other.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Closed form solutions of measures of systemic risk',\n",
       "  'text': 'Also, it may be relatively easy to convince the regulators to consider a closed\\nform solution, especially so if the statistics involved are well known and can\\neasily be communicated to the financial community.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'A Fourier Approach to the Computation of CV@R and Optimized Certainty\\n  Equivalents',\n",
       "  'text': 'We consider the class of risk measures associated with optimized certainty\\nequivalents. This class includes several popular examples, such as CV@R and\\nmonotone mean-variance. Numerical schemes are developed for the computation of\\nthese risk measures using Fourier transform methods. This leads, in particular,\\nto a very competitive method for the calculation of CV@R which is comparable in\\ncomputational time to the calculation of V@R. We also develop methods for the\\nefficient computation of risk contributions.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Risk Measure Estimation On Fiegarch Processes',\n",
       "  'text': 'We consider the Fractionally Integrated Exponential Generalized\\nAutoregressive Conditional Heteroskedasticity process, denoted by\\nFIEGARCH(p,d,q), introduced by Bollerslev and Mikkelsen (1996). We present a\\nsimulated study regarding the estimation of the risk measure $VaR_p$ on\\nFIEGARCH processes. We consider the distribution function of the portfolio\\nlog-returns (univariate case) and the multivariate distribution function of the\\nrisk-factor changes (multivariate case). We also compare the performance of the\\nrisk measures $VaR_p$, $ES_p$ and MaxLoss for a portfolio composed by stocks of\\nfour Brazilian companies.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Restructuring the \"one-way CSA\" counterparty risk in a CDO',\n",
       "  'text': 'We show how to restructure the counterparty risk faced by the originator of a\\nsecuritization or covered bond arising from an interest rate hedging swap\\nassisted by a \"one-way\" collateral agreement. This risk emerges when the swap\\nis negotiated between the special purpose vehicle and a third party that covers\\nitself through a back-to-back swap with the originator. We show that the\\ncounterparty risk of the originator may be removed by adding a chain of\\nback-to-back credit derivatives between the three parties (originator,\\ncounterparty and vehicle).',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Weak continuity of risk functionals with applications to stochastic\\n  programming',\n",
       "  'text': 'Measuring and managing risk has become crucial in modern decision making\\nunder stochastic uncertainty. In two-stage stochastic programming, mean risk\\nmodels are essentially defined by a parametric recourse problem and a\\nquantification of risk. From the perspective of qualitative robustness theory,\\nwe discuss sufficient conditions for continuity of the resulting objective\\nfunctions with respect to perturbation of the underlying probability measure. Our approach covers a fairly comprehensive class of both stochastic-programming\\nrelated risk measures and relevant recourse models. Not only this unifies\\nprevious approaches but also extends known stability results for two-stage\\nstochastic programs to models with mixed-integer quadratic recourse and\\nmixed-integer convex recourse, respectively.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'On a Transform Method for the Efficient Computation of Conditional VaR\\n  (and VaR) with Application to Loss Models with Jumps and Stochastic\\n  Volatility',\n",
       "  'text': \"In this paper we consider Fourier transform techniques to efficiently compute\\nthe Value-at-Risk and the Conditional Value-at-Risk of an arbitrary loss random\\nvariable, characterized by having a computable generalized characteristic\\nfunction. We exploit the property of these risk measures of being the solution\\nof an elementary optimization problem of convex type in one dimension for which\\nFast and Fractional Fourier transform can be implemented. An application to\\nunivariate loss models driven by L\\\\'{e}vy or stochastic volatility risk factors\\ndynamic is finally reported.\",\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Integral representations of risk functions for basket derivatives',\n",
       "  'text': 'The risk minimizing problem\\n$\\\\mathbf{E}[l((H-X_T^{x,\\\\pi})^{+})]\\\\overset{\\\\pi}{\\\\longrightarrow}\\\\min$ in the\\nmultidimensional Black-Scholes framework is studied. Specific formulas for the\\nminimal risk function and the cost reduction function for basket derivatives\\nare shown. Explicit integral representations for the risk functions for\\n$l(x)=x$ and $l(x)=x^p$, with $p>1$ for digital, quantos, outperformance and\\nspread options are derived.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Modelling cross-border systemic risk in the European banking sector: a\\n  copula approach',\n",
       "  'text': 'We propose a new methodology based on the Marshall-Olkin (MO) copula to model\\ncross-border systemic risk. The proposed framework estimates the impact of the\\nsystematic and idiosyncratic components on systemic risk. Initially, we propose\\na maximum-likelihood method to estimate the parameter of the MO copula. In\\norder to use the data on non-distressed banks for these estimates, we consider\\ntimes to bank failures as censored samples. Hence, we propose an estimation\\nprocedure for the MO copula on censored data. The empirical evidence from\\nEuropean banks shows that the proposed censored model avoid possible\\nunderestimation of the contagion risk.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'An Overview of the Security Concerns in Enterprise Cloud Computing',\n",
       "  'text': 'Deploying cloud computing in an enterprise infrastructure bring significant\\nsecurity concerns. Successful implementation of cloud computing in an\\nenterprise requires proper planning and understanding of emerging risks,\\nthreats, vulnerabilities, and possible countermeasures. We believe enterprise\\nshould analyze the company/organization security risks, threats, and available\\ncountermeasures before adopting this technology. In this paper, we have\\ndiscussed security risks and concerns in cloud computing and enlightened steps\\nthat an enterprise can take to reduce security risks and protect their\\nresources. We have also explained cloud computing strengths/benefits,\\nweaknesses, and applicable areas in information risk management.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Approximation of Some Multivariate Risk Measures for Gaussian Risks',\n",
       "  'text': 'Gaussian random vectors exhibit the loss of dimension phenomena, which relate\\nto their joint survival tail behaviour. Besides, the fact that the components\\nof such vectors are light-tailed complicates the approximations of various\\nmultivariate risk measures significantly. In this contribution we derive\\nprecise approximations of marginal mean excess, marginal expected shortfall and\\nmultivariate conditional tail expectation of Gaussian random vectors and\\nhighlight links with conditional limit theorems. Our study indicates that\\nsimilar results hold for elliptical and Gaussian like multivariate risks.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'On the Basel Liquidity Formula for Elliptical Distributions',\n",
       "  'text': 'A justification of the Basel liquidity formula for risk capital in the\\ntrading book is given under the assumption that market risk-factor changes form\\na Gaussian white noise process over 10-day time steps and changes to P&L are\\nlinear in the risk-factor changes. A generalization of the formula is derived\\nunder the more general assumption that risk-factor changes are multivariate\\nelliptical. It is shown that the Basel formula tends to be conservative when\\nthe elliptical distributions are from the heavier-tailed generalized hyperbolic\\nfamily. As a by-product of the analysis a Fourier approach to calculating\\nexpected shortfall for general symmetric loss distributions is developed.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'A Loan Portfolio Model Subject to Random Liabilities and Systemic Jump\\n  Risk',\n",
       "  'text': 'We extend the Vasi\\\\v{c}ek loan portfolio model to a setting where liabilities\\nfluctuate randomly and asset values may be subject to systemic jump risk. We\\nderive the probability distribution of the percentage loss of a uniform\\nportfolio and analyze its properties. We find that the impact of liability risk\\nis ambiguous and depends on the correlation between the continuous aggregate\\nfactor and the asset-liability ratio as well as on the default intensity. We\\nalso find that systemic jump risk has a significant impact on the upper\\npercentiles of the loss distribution and, therefore, on both the VaR-measure as\\nwell as on the expected shortfall.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Expected Shortfall is jointly elicitable with Value at Risk -\\n  Implications for backtesting',\n",
       "  'text': 'In this note, we comment on the relevance of elicitability for backtesting\\nrisk measure estimates. In particular, we propose the use of Diebold-Mariano\\ntests, and show how they can be implemented for Expected Shortfall (ES), based\\non the recent result of Fissler and Ziegel (2015) that ES is jointly elicitable\\nwith Value at Risk.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'A State-Dependent Dual Risk Model',\n",
       "  'text': 'In a dual risk model, the premiums are considered as the costs and the claims\\nare regarded as the profits. The surplus can be interpreted as the wealth of a\\nventure capital, whose profits depend on research and development. In most of\\nthe existing literature of dual risk models, the profits follow the compound\\nPoisson model and the cost is constant. In this paper, we develop a\\nstate-dependent dual risk model, in which the arrival rate of the profits and\\nthe costs depend on the current state of the wealth process. Ruin probabilities\\nare obtained in closed-forms. Further properties and results will also be\\ndiscussed.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Optimal Investment in a Dual Risk Model',\n",
       "  'text': 'Dual risk models are popular for modeling a venture capital or high tech\\ncompany, for which the running cost is deterministic and the profits arrive\\nstochastically over time. Most of the existing literature on dual risk models\\nconcentrated on the optimal dividend strategies. In this paper, we propose to\\nstudy the optimal investment strategy on research and development for the dual\\nrisk models to minimize the ruin probability of the underlying company. We will\\nalso study the optimization problem when in addition the investment in a risky\\nasset is allowed.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'The strong Fatou property of risk measures',\n",
       "  'text': 'In this paper, we explore several Fatou-type properties of risk measures. The\\npaper continues to reveal that the strong Fatou property, which was introduced\\nin [17], seems to be most suitable to ensure nice dual representations of risk\\nmeasures. Our main result asserts that every quasiconvex law-invariant\\nfunctional on a rearrangement invariant space $\\\\mathcal{X}$ with the strong\\nFatou property is $\\\\sigma(\\\\mathcal{X},L^\\\\infty)$ lower semicontinuous and that\\nthe converse is true on a wide range of rearrangement invariant spaces. We also\\nstudy inf-convolutions of law-invariant or surplus-invariant risk measures that\\npreserve the (strong) Fatou property.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Multivariate risk measures in the non-convex setting',\n",
       "  'text': 'The family of admissible positions in a transaction costs model is a random\\nclosed set, which is convex in case of proportional transaction costs. However,\\nthe convexity fails, e.g. in case of fixed transaction costs or when only a\\nfinite number of transfers are possible. The paper presents an approach to\\nmeasure risks of such positions based on the idea of considering all selections\\nof the portfolio and checking if one of them is acceptable. Properties and\\nbasic examples of risk measures of non-convex portfolios are presented.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Model risk in mean-variance portfolio selection: an analytic solution to\\n  the worst-case approach',\n",
       "  'text': 'In this paper we consider the worst-case model risk approach described in\\nGlasserman and Xu (2014). Portfolio selection with model risk can be a\\nchallenging operational research problem. In particular, it presents an\\nadditional optimisation compared to the classical one. We find the analytical\\nsolution for the optimal mean-variance portfolio selection in the worst-case\\nscenario approach. In the minimum-variance case, we prove that the analytical\\nsolution is significantly different from the one found numerically by\\nGlasserman and Xu (2014) and that model risk reduces to an estimation risk. A\\ndetailed numerical example is provided.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Macroscopic theorem of the portfolio optimization problem with a\\n  risk-free asset',\n",
       "  'text': \"The investment risk minimization problem with budget and return constraints\\nhas been the subject of research using replica analysis but there are\\nshortcomings in the extant literature. With respect to Tobin's separation\\ntheorem and the capital asset pricing model, it is necessary to investigate the\\nimplications of a risk-free asset and examine its influence on the optimal\\nportfolio. Accordingly, in this work, we explore the investment risk\\nminimization problem in the presence of a risk-free asset with budget and\\nreturn constraints. Moreover, we discuss opportunity loss, the Pythagorean\\ntheorem of the Sharpe ratio, and Tobin's separation theorem.\",\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Dual representations for systemic risk measures based on acceptance sets',\n",
       "  'text': 'We establish dual representations for systemic risk measures based on\\nacceptance sets in a general setting. We deal with systemic risk measures of\\nboth \"first allocate, then aggregate\" and \"first aggregate, then allocate\"\\ntype. In both cases, we provide a detailed analysis of the corresponding\\nsystemic acceptance sets and their support functions. The same approach\\ndelivers a simple and self-contained proof of the dual representation of\\nutility-based risk measures for univariate positions.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Avoiding zero probability events when computing Value at Risk\\n  contributions: a Malliavin calculus approach',\n",
       "  'text': \"This paper is concerned with the process of risk allocation for a generic\\nmultivariate model when the risk measure is chosen as the Value-at-Risk (VaR). Making use of Malliavin calculus, we recast the traditional Euler contributions\\nfrom an expectation conditional to an event of zero probability to a ratio of\\nconditional expectations, where both the numerator and the denominator's\\nconditioning events have positive probability. For several different models we\\nshow empirically that the estimator using this novel representation has no\\nperceivable bias and variance smaller than a standard estimator used in\\npractice.\",\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Risk Measures Estimation Under Wasserstein Barycenter',\n",
       "  'text': 'Randomness in financial markets requires modern and robust multivariate\\nmodels of risk measures. This paper proposes a new approach for modeling\\nmultivariate risk measures under Wasserstein barycenters of probability\\nmeasures supported on location-scatter families. Simple and advanced copulas\\nmultivariate Value at Risk models are compared with the derived technique. The\\nperformance of the model is also checked in market indices of United States\\ngenerated by the financial crisis due to COVID-19. The introduced model behaves\\nsatisfactory in both common and volatile periods of asset prices, providing\\nrealistic VaR forecast in this era of social distancing.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Systemic Risk in Financial Networks: A Survey',\n",
       "  'text': \"We provide an overview of the relationship between financial networks and\\nsystemic risk. We present a taxonomy of different types of systemic risk,\\ndifferentiating between direct externalities between financial organizations\\n(e.g., defaults, correlated portfolios and firesales), and perceptions and\\nfeedback effects (e.g., bank runs, credit freezes). We also discuss optimal\\nregulation and bailouts, measurements of systemic risk and financial\\ncentrality, choices by banks' regarding their portfolios and partnerships, and\\nthe changing nature of financial networks.\",\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Black-box model risk in finance',\n",
       "  'text': 'Machine learning models are increasingly used in a wide variety of financial\\nsettings. The difficulty of understanding the inner workings of these systems,\\ncombined with their wide applicability, has the potential to lead to\\nsignificant new risks for users; these risks need to be understood and\\nquantified. In this sub-chapter, we will focus on a well studied application of\\nmachine learning techniques, to pricing and hedging of financial options. Our\\naim will be to highlight the various sources of risk that the introduction of\\nmachine learning emphasises or de-emphasises, and the possible risk mitigation\\nand management strategies that are available.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Risk measures with non-Gaussian fluctuations',\n",
       "  'text': 'Reliable calculations of financial risk require that the fat-tailed nature of\\nprices changes is included in risk measures. To this end, a non-Gaussian\\napproach to financial risk management is presented, modeling the power-law\\ntails of the returns distribution in terms of a Student-$t$ (or Tsallis)\\ndistribution. Non-Gaussian closed-form solutions for Value-at-Risk and Expected Shortfall\\nare obtained and standard formulae known in the literature under the normality\\nassumption are recovered as a special case. The implications of the approach\\nfor risk management are demonstrated through an empirical analysis of financial\\ntime series from the Italian stock market.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Risk measures with non-Gaussian fluctuations',\n",
       "  'text': 'Detailed comparison with the results\\nof the widely used procedures of quantitative finance, such as parametric\\nnormal approach, RiskMetrics methodology and historical simulation, as well as\\nwith previous findings in the literature, are shown and commented. Particular\\nattention is paid to quantify the size of the errors affecting the risk\\nmeasures obtained according to different methodologies, by employing a\\nbootstrap technique.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'A Generalized Fourier Transform Approach to Risk Measures',\n",
       "  'text': 'We introduce the formalism of generalized Fourier transforms in the context\\nof risk management. We develop a general framework to efficiently compute the\\nmost popular risk measures, Value-at-Risk and Expected Shortfall (also known as\\nConditional Value-at-Risk). The only ingredient required by our approach is the\\nknowledge of the characteristic function describing the financial data in use. This allows to extend risk analysis to those non-Gaussian models defined in the\\nFourier space, such as Levy noise driven processes and stochastic volatility\\nmodels.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'A Generalized Fourier Transform Approach to Risk Measures',\n",
       "  'text': 'We test our analytical results on data sets coming from various\\nfinancial indexes, finding that our predictions outperform those provided by\\nthe standard Log-Normal dynamics and are in remarkable agreement with those of\\nthe benchmark historical approach.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Risk Concentration and Diversification: Second-Order Properties',\n",
       "  'text': \"The quantification of diversification benefits due to risk aggregation plays\\na prominent role in the (regulatory) capital management of large firms within\\nthe financial industry. However, the complexity of today's risk landscape makes\\na quantifiable reduction of risk concentration a challenging task. In the\\npresent paper we discuss some of the issues that may arise. The theory of\\nsecond-order regular variation and second-order subexponentiality provides the\\nideal methodological framework to derive second-order approximations for the\\nrisk concentration and the diversification benefit.\",\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Suitability of Capital Allocations for Performance Measurement',\n",
       "  'text': 'Capital allocation principles are used in various contexts in which a risk\\ncapital or a cost of an aggregate position has to be allocated among its\\nconstituent parts. We study capital allocation principles in a performance\\nmeasurement framework. We introduce the notation of suitability of allocations\\nfor performance measurement and show under different assumptions on the\\ninvolved reward and risk measures that there exist suitable allocation methods. The existence of certain suitable allocation principles generally is given\\nunder rather strict assumptions on the underlying risk measure.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Suitability of Capital Allocations for Performance Measurement',\n",
       "  'text': 'Therefore we\\nshow, with a reformulated definition of suitability and in a slightly modified\\nsetting, that there is a known suitable allocation principle that does not\\nrequire any properties of the underlying risk measure. Additionally we extend a\\nprevious characterization result from the literature from a mean-risk to a\\nreward-risk setting. Formulations of this theory are also possible in a game\\ntheoretic setting.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Comments on the BCBS proposal for a New Standardized Approach for\\n  Operational Risk',\n",
       "  'text': 'On March 4th 2016 the Basel Committee on Banking Supervision published a\\nconsultative document where a new methodology, called the Standardized\\nMeasurement Approach (SMA), is introduced for computing Operational Risk\\nregulatory capital for banks.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Comments on the BCBS proposal for a New Standardized Approach for\\n  Operational Risk',\n",
       "  'text': 'In this note, the behavior of the SMA is studied\\nunder a variety of hypothetical and realistic conditions, showing that the\\nsimplicity of the new approach is very costly on other aspects: we find that\\nthe SMA does not respond appropriately to changes in the risk profile of a\\nbank, nor is it capable of differentiating among the range of possible risk\\nprofiles across banks; that SMA capital results generally appear to be more\\nvariable across banks than the previous AMA option of fitting the loss data;\\nthat the SMA can result in banks over- or under-insuring against operational\\nrisks relative to previous AMA standards.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Comments on the BCBS proposal for a New Standardized Approach for\\n  Operational Risk',\n",
       "  'text': 'Finally, we argue that the SMA is not\\nonly retrograde in terms of its capability to measure risk, but perhaps more\\nimportantly, it fails to create any link between management actions and capital\\nrequirement.',\n",
       "  'meta': {'_split_id': 2}},\n",
       " {'name': 'A Composite Risk Measure Framework for Decision Making under Uncertainty',\n",
       "  'text': 'In this paper, we present a unified framework for decision making under\\nuncertainty. Our framework is based on the composite of two risk measures,\\nwhere the inner risk measure accounts for the risk of decision given the exact\\ndistribution of uncertain model parameters, and the outer risk measure\\nquantifies the risk that occurs when estimating the parameters of distribution. We show that the model is tractable under mild conditions. The framework is a\\ngeneralization of several existing models, including stochastic programming,\\nrobust optimization, distributionally robust optimization, etc.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'A Composite Risk Measure Framework for Decision Making under Uncertainty',\n",
       "  'text': 'Using this\\nframework, we study a few new models which imply probabilistic guarantees for\\nsolutions and yield less conservative results comparing to traditional models. Numerical experiments are performed on portfolio selection problems to\\ndemonstrate the strength of our models.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Heterotic Risk Models',\n",
       "  'text': 'We give a complete algorithm and source code for constructing what we refer\\nto as heterotic risk models (for equities), which combine: i) granularity of an\\nindustry classification; ii) diagonality of the principal component factor\\ncovariance matrix for any sub-cluster of stocks; and iii) dramatic reduction of\\nthe factor covariance matrix size in the Russian-doll risk model construction. This appears to prove a powerful approach for constructing out-of-sample stable\\nshort-lookback risk models. Thus, for intraday mean-reversion alphas based on\\novernight returns, Sharpe ratio optimization using our heterotic risk models\\nsizably improves the performance characteristics compared to weighted\\nregressions based on principal components or industry classification.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Heterotic Risk Models',\n",
       "  'text': 'We also\\ngive source code for: a) building statistical risk models; and ii) Sharpe ratio\\noptimization with homogeneous linear constraints and position bounds.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'A Novel Approach to Quantification of Model Risk for Practitioners',\n",
       "  'text': \"Models continue to increase their already broad use across industry as well\\nas their sophistication. Worldwide regulation oblige financial institutions to\\nmanage and address model risk with the same severity as any other type of risk,\\nwhich besides defines model risk as the potential for adverse consequences from\\ndecisions based on incorrect and misused model outputs and reports. Model risk\\nquantification is essential not only in meeting these requirements but for\\ninstitution's basic internal operative. It is however a complex task as any\\ncomprehensive quantification methodology should at least consider the data used\\nfor building the model, its mathematical foundations, the IT infrastructure,\\noverall performance and (most importantly) usage.\",\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'A Novel Approach to Quantification of Model Risk for Practitioners',\n",
       "  'text': 'Besides, the current amount\\nof models and different mathematical modelling techniques is overwhelming. Our proposal is to define quantification of model risk as a calculation of\\nthe norm of some appropriate function that belongs to a Banach space, defined\\nover a weighted Riemannian manifold endowed with the Fisher--Rao metric. The\\naim of the present contribution is twofold: Introduce a sufficiently general\\nand sound mathematical framework to cover the aforementioned points and\\nillustrate how a practitioner may identify the relevant abstract concepts and\\nput them to work.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Computational aspects of robust optimized certainty equivalents and\\n  option pricing',\n",
       "  'text': 'Accounting for model uncertainty in risk management and option pricing leads\\nto infinite dimensional optimization problems which are both analytically and\\nnumerically intractable. In this article we study when this hurdle can be\\novercome for the so-called optimized certainty equivalent risk measure (OCE) --\\nincluding the average value-at-risk as a special case. First we focus on the\\ncase where the uncertainty is modeled by a nonlinear expectation penalizing\\ndistributions that are \"far\" in terms of optimal-transport distance\\n(Wasserstein distance for instance) from a given baseline distribution.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Computational aspects of robust optimized certainty equivalents and\\n  option pricing',\n",
       "  'text': 'It\\nturns out that the computation of the robust OCE reduces to a finite\\ndimensional problem, which in some cases can even be solved explicitly. This\\nprinciple also applies to the shortfall risk measure as well as for the pricing\\nof European options. Further, we derive convex dual representations of the\\nrobust OCE for measurable claims without any assumptions on the set of\\ndistributions. Finally, we give conditions on the latter set under which the\\nrobust average value-at-risk is a tail risk measure.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'A new multilayer network construction via Tensor learning',\n",
       "  'text': 'Multilayer networks proved to be suitable in extracting and providing\\ndependency information of different complex systems. The construction of these\\nnetworks is difficult and is mostly done with a static approach, neglecting\\ntime delayed interdependences. Tensors are objects that naturally represent\\nmultilayer networks and in this paper, we propose a new methodology based on\\nTucker tensor autoregression in order to build a multilayer network directly\\nfrom data. This methodology captures within and between connections across\\nlayers and makes use of a filtering procedure to extract relevant information\\nand improve visualization. We show the application of this methodology to\\ndifferent stationary fractionally differenced financial data.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'A new multilayer network construction via Tensor learning',\n",
       "  'text': 'We argue that our\\nresult is useful to understand the dependencies across three different aspects\\nof financial risk, namely market risk, liquidity risk, and volatility risk. Indeed, we show how the resulting visualization is a useful tool for risk\\nmanagers depicting dependency asymmetries between different risk factors and\\naccounting for delayed cross dependencies. The constructed multilayer network\\nshows a strong interconnection between the volumes and prices layers across all\\nthe stocks considered while a lower number of interconnections between the\\nuncertainty measures is identified.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Regional Flood Risk Projections under Climate Change',\n",
       "  'text': 'Flood-related risks to people and property are expected to increase in the\\nfuture due to environmental and demographic changes. It is important to\\nquantify and effectively communicate flood hazards and exposure to inform the\\ndesign and implementation of flood risk management strategies. Here we develop\\nan integrated modeling framework to assess projected changes in regional\\nriverine flood inundation risks. The framework samples climate model outputs to\\nforce a hydrologic model and generate streamflow projections. Together with a\\nstatistical and hydraulic model, we use the projected streamflow to map the\\nuncertainty of flood inundation projections for extreme flood events.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Regional Flood Risk Projections under Climate Change',\n",
       "  'text': 'We\\nimplement the framework for rivers across the state of Pennsylvania, United\\nStates. Our projections suggest that flood hazards and exposure across\\nPennsylvania are overall increasing with future climate change. Specific\\nregions, including the main stem Susquehanna River, lower portion of the\\nAllegheny basin and central portion of Delaware River basin, demonstrate higher\\nflood inundation risks. In our analysis, the climate uncertainty dominates the\\noverall uncertainty surrounding the flood inundation projection chain. The\\ncombined hydrologic and hydraulic uncertainties can account for as much as 37%\\nof the total uncertainty. We discuss how this framework can provide regional\\nand dynamic flood-risk assessments and help to inform the design of\\nrisk-management strategies.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Research on Survivability Strategies of Virtual Network',\n",
       "  'text': 'Virtualization facilitates heterogeneous cloud applications to share the same\\nphysical infrastructure with admirable flexibility, while resource efficiency\\nand survivability are critical concerns for virtual network embedding (VNE). As\\nmore and more internet applications migrate to the cloud, the resource\\nefficiency and the survivability of VNs, such as single link failure or\\nlarge-scale disaster survivability, have become crucial issues. Separating the\\nVNE problem into node and link mapping sub-problems without coordination might\\ncause a high embedding cost. This dissertation presents two independent\\napproaches to solve the aforementioned challenges. First, we study two-stage\\ncoordinated survivable VNE (SVNE) problem and propose an adaptive path\\nsplitting based SVNE (APSS) scheme.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Research on Survivability Strategies of Virtual Network',\n",
       "  'text': 'We first develop a concise anchor node\\nstrategy to restrict the solution space of the candidate substrate nodes, which\\ncoordinates node mapping with link mapping to limit the distance spans of the\\nvirtual links. Then, we employ an adaptive path splitting policy to provide\\nfull protection against single-link failures with partial backup resource, and\\ndesign an agile frequency slot windows choosing mechanism to mitigate the\\nspectrum fragmentation for link resource efficiency. Simulation results\\ndemonstrate that the proposed APSS scheme can achieve satisfactory performance\\nin terms of spectrum utilization and blocking ratio.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Research on Survivability Strategies of Virtual Network',\n",
       "  'text': 'Second, we propose a\\nsynchronous evacuation strategy for VNs with dual virtual machines (VMs) inside\\na disaster risk zone (DRZ), which suffer higher risks than the VNs with single. The evacuation strategy exploits post-copy technique to sustain the online\\nservice alive and enhances synchronous VM migrations to shorten the dual-VM\\nevacuation time. Numerical results show that the proposed strategy can\\noutperform the best-effort scheme in terms of average and total evacuation\\ntimes of dual-VMs.',\n",
       "  'meta': {'_split_id': 2}},\n",
       " {'name': 'Convex Risk Measures: Lebesgue Property on one Period and Multi Period\\n  Risk Measures and Application in Capital Allocation Problem',\n",
       "  'text': 'In this work we study the Lebesgue property for convex risk measures on the\\nspace of bounded c\\\\`adl\\\\`ag random processes ($\\\\mathcal{R}^\\\\infty$). Lebesgue\\nproperty has been defined for one period convex risk measures in \\\\cite{Jo} and\\nearlier had been studied in \\\\cite{De} for coherent risk measures. We introduce\\nand study the Lebesgue property for convex risk measures in the multi period\\nframework. We give presentation of all convex risk measures with Lebesgue\\nproperty on bounded c\\\\`adl\\\\`ag processes. To do that we need to have a complete\\ndescription of compact sets of $\\\\mathcal{A}^1$. The main mathematical\\ncontribution of this paper is the characterization of the compact sets of\\n$\\\\mathcal{A}^p$ (including $\\\\mathcal{A}^1$).',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Convex Risk Measures: Lebesgue Property on one Period and Multi Period\\n  Risk Measures and Application in Capital Allocation Problem',\n",
       "  'text': 'At the final part of this paper,\\nwe will solve the Capital Allocation Problem when we work with coherent risk\\nmeasures.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Monitoring dates of maximal risk',\n",
       "  'text': 'Monitoring means to observe a system for any changes which may occur over\\ntime, using a monitor or measuring device of some sort. In this paper we\\nformulate a problem of monitoring dates of maximal risk of a financial\\nposition. Thus, the systems we are going to observe arise from situations in\\nfinance. The measuring device we are going to use is a time-consistent measure\\nof risk. In the first part of the paper we discuss the numerical representation of\\nconditional convex risk measures which are defined in a space Lp(F,R) and take\\nvalues in L1(G,R). This will allow us to consider time-consistent convex risk\\nmeasures in L1(R).',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Monitoring dates of maximal risk',\n",
       "  'text': 'In the second part of the paper we use a time-consistent convex risk measure\\nin order to define an abstract problem of monitoring stopping times of maximal\\nrisk. The penalty function involved in the robust representation changes\\nqualitatively the time when maximal risk is for the first time identified. A\\nphenomenon which we discuss from the point of view of robust statistics.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Time Varying Risk Aversion: An Application to Energy Hedging',\n",
       "  'text': 'Risk aversion is a key element of utility maximizing hedge strategies;\\nhowever, it has typically been assigned an arbitrary value in the literature. This paper instead applies a GARCH-in-Mean (GARCH-M) model to estimate a\\ntime-varying measure of risk aversion that is based on the observed risk\\npreferences of energy hedging market participants. The resulting estimates are\\napplied to derive explicit risk aversion based optimal hedge strategies for\\nboth short and long hedgers. Out-of-sample results are also presented based on\\na unique approach that allows us to forecast risk aversion, thereby estimating\\nhedge strategies that address the potential future needs of energy hedgers.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Time Varying Risk Aversion: An Application to Energy Hedging',\n",
       "  'text': 'We\\nfind that the risk aversion based hedges differ significantly from simpler OLS\\nhedges. When implemented in-sample, risk aversion hedges for short hedgers\\noutperform the OLS hedge ratio in a utility based comparison.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Coherence and elicitability',\n",
       "  'text': 'The risk of a financial position is usually summarized by a risk measure. As\\nthis risk measure has to be estimated from historical data, it is important to\\nbe able to verify and compare competing estimation procedures. In statistical\\ndecision theory, risk measures for which such verification and comparison is\\npossible, are called elicitable. It is known that quantile based risk measures\\nsuch as value at risk are elicitable. In this paper we show that law-invariant\\nspectral risk measures such as expected shortfall are not elicitable unless\\nthey reduce to minus the expected value. Hence, it is unclear how to perform\\nforecast verification or comparison.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Coherence and elicitability',\n",
       "  'text': 'However, the class of elicitable\\nlaw-invariant coherent risk measures does not reduce to minus the expected\\nvalue. We show that it consists of certain expectiles.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'The impact of systemic risk on the diversification benefits of a risk\\n  portfolio',\n",
       "  'text': 'Risk diversification is the basis of insurance and investment. It is thus\\ncrucial to study the effects that could limit it. One of them is the existence\\nof systemic risk that affects all the policies at the same time. We introduce\\nhere a probabilistic approach to examine the consequences of its presence on\\nthe risk loading of the premium of a portfolio of insurance policies. This\\napproach could be easily generalized for investment risk. We see that, even\\nwith a small probability of occurrence, systemic risk can reduce dramatically\\nthe diversification benefits.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'The impact of systemic risk on the diversification benefits of a risk\\n  portfolio',\n",
       "  'text': 'It is clearly revealed via a non-diversifiable\\nterm that appears in the analytical expression of the variance of our models. We propose two ways of introducing it and discuss their advantages and\\nlimitations. By using both VaR and TVaR to compute the loading, we see that\\nonly the latter captures the full effect of systemic risk when its probability\\nto occur is low',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Should the advanced measurement approach be replaced with the\\n  standardized measurement approach for operational risk?',\n",
       "  'text': 'Recently, Basel Committee for Banking Supervision proposed to replace all\\napproaches, including Advanced Measurement Approach (AMA), for operational risk\\ncapital with a simple formula referred to as the Standardised Measurement\\nApproach (SMA). This paper discusses and studies the weaknesses and pitfalls of\\nSMA such as instability, risk insensitivity, super-additivity and the implicit\\nrelationship between SMA capital model and systemic risk in the banking sector. We also discuss the issues with closely related operational risk\\nCapital-at-Risk (OpCar) Basel Committee proposed model which is the precursor\\nto the SMA.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Should the advanced measurement approach be replaced with the\\n  standardized measurement approach for operational risk?',\n",
       "  'text': 'In conclusion, we advocate to maintain the AMA internal model\\nframework and suggest as an alternative a number of standardization\\nrecommendations that could be considered to unify internal modelling of\\noperational risk. The findings and views presented in this paper have been\\ndiscussed with and supported by many OpRisk practitioners and academics in\\nAustralia, Europe, UK and USA, and recently at OpRisk Europe 2016 conference in\\nLondon.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Parameter uncertainty and reserve risk under Solvency II',\n",
       "  'text': \"In this article we consider the parameter risk in the context of internal\\nmodelling of the reserve risk under Solvency II. We discuss two opposed perspectives on parameter uncertainty and point out\\nthat standard methods of classical reserving focusing on the estimation error\\nof claims reserves are in general not appropriate to model the impact of\\nparameter uncertainty upon the actual risk of economic losses from the\\nundertakings's perspective.\",\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Parameter uncertainty and reserve risk under Solvency II',\n",
       "  'text': 'Referring to the requirements of Solvency II we assess methods to model\\nparameter uncertainty for the reserve risk by comparing the probability of\\nsolvency actually attained when modelling the solvency risk capital requirement\\nbased on the respective method to the required confidence level. Using the\\nsimple example of a normal model we show that the bootstrapping approach is not\\nappropriate to model parameter uncertainty according to this criterion. We then\\npresent an adaptation of the approach proposed in \\\\cite {froehlich2014}. Experimental results demonstrate that this new method yields a risk capital\\nmodel for the reserve risk achieving the required confidence level in good\\napproximation.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Network versus portfolio structure in financial systems',\n",
       "  'text': 'The question of how to stabilize financial systems has attracted considerable\\nattention since the global financial crisis of 2007-2009. Recently, Beale et\\nal. (\"Individual versus systemic risk and the regulator\\'s dilemma\", Proc Natl\\nAcad Sci USA 108: 12647-12652, 2011) demonstrated that higher portfolio\\ndiversity among banks would reduce systemic risk by decreasing the risk of\\nsimultaneous defaults at the expense of a higher likelihood of individual\\ndefaults. In practice, however, a bank default has an externality in that it\\nundermines other banks\\' balance sheets. This paper explores how each of these\\ndifferent sources of risk, simultaneity risk and externality, contributes to\\nsystemic risk.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Network versus portfolio structure in financial systems',\n",
       "  'text': 'The results show that the allocation of external assets that\\nminimizes systemic risk varies with the topology of the financial network as\\nlong as asset returns have negative correlations. In the model, a well-known\\ncentrality measure, PageRank, reflects an appropriately defined \"infectiveness\"\\nof a bank. An important result is that the most infective bank need not always\\nbe the safest bank. Under certain circumstances, the most infective node should\\nact as a firewall to prevent large-scale collective defaults. The introduction\\nof a counteractive portfolio structure will significantly reduce systemic risk.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Measuring risk with multiple eligible assets',\n",
       "  'text': 'The risk of financial positions is measured by the minimum amount of capital\\nto raise and invest in eligible portfolios of traded assets in order to meet a\\nprescribed acceptability constraint. We investigate nondegeneracy, finiteness\\nand continuity properties of these risk measures with respect to multiple\\neligible assets. Our finiteness and continuity results highlight the interplay\\nbetween the acceptance set and the class of eligible portfolios. We present a\\nsimple, alternative approach to the dual representation of convex risk measures\\nby directly applying to the acceptance set the external characterization of\\nclosed, convex sets.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Measuring risk with multiple eligible assets',\n",
       "  'text': 'We prove that risk measures are nondegenerate if and only\\nif the pricing functional admits a positive extension which is a supporting\\nfunctional for the underlying acceptance set, and provide a characterization of\\nwhen such extensions exist. Finally, we discuss applications to set-valued risk\\nmeasures, superhedging with shortfall risk, and optimal risk sharing.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Risk concentration under second order regular variation',\n",
       "  'text': 'Measures of risk concentration and their asymptotic behavior for portfolios\\nwith heavy-tailed risk factors is of interest in risk management. Second order\\nregular variation is a structural assumption often imposed on such risk factors\\nto study their convergence rates. In this paper, we provide the asymptotic rate\\nof convergence of the measure of risk concentration for a portfolio of\\nheavy-tailed risk factors, when the portfolio admits the so-called second order\\nregular variation property. Moreover, we explore the relationship between\\nmultivariate second order regular variation for a vector (e.g., risk factors)\\nand the second order regular variation property for the sum of its components\\n(e.g., the portfolio of risk factors).',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Risk concentration under second order regular variation',\n",
       "  'text': 'Results are illustrated with a variety\\nof examples.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'RiskRank: Measuring interconnected risk',\n",
       "  'text': 'This paper proposes RiskRank as a joint measure of cyclical and\\ncross-sectional systemic risk. RiskRank is a general-purpose aggregation\\noperator that concurrently accounts for risk levels for individual entities and\\ntheir interconnectedness. The measure relies on the decomposition of systemic\\nrisk into sub-components that are in turn assessed using a set of risk measures\\nand their relationships. For this purpose, motivated by the development of the\\nChoquet integral, we employ the RiskRank function to aggregate risk measures,\\nallowing for the integration of the interrelation of different factors in the\\naggregation process.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'RiskRank: Measuring interconnected risk',\n",
       "  'text': 'The use of RiskRank is illustrated through a real-world\\ncase in a European setting, in which we show that it performs well in\\nout-of-sample analysis. In the example, we provide an estimation of systemic\\nrisk from country-level risk and cross-border linkages.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'On Fairness of Systemic Risk Measures',\n",
       "  'text': 'In our previous paper, \"A Unified Approach to Systemic Risk Measures via\\nAcceptance Set\" (\\\\textit{Mathematical Finance, 2018}), we have introduced a\\ngeneral class of systemic risk measures that allow for random allocations to\\nindividual banks before aggregation of their risks. In the present paper, we\\nprove the dual representation of a particular subclass of such systemic risk\\nmeasures and the existence and uniqueness of the optimal allocation related to\\nthem. We also introduce an associated utility maximization problem which has\\nthe same optimal solution as the systemic risk measure.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'On Fairness of Systemic Risk Measures',\n",
       "  'text': 'In addition, the\\noptimizer in the dual formulation provides a \\\\textit{risk allocation} which is\\nfair from the point of view of the individual financial institutions. The case\\nwith exponential utilities which allows for explicit computation is treated in\\ndetails.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Shortfall Deviation Risk: An alternative to risk measurement',\n",
       "  'text': 'We present the Shortfall Deviation Risk (SDR), a risk measure that represents\\nthe expected loss that occurs with certain probability penalized by the\\ndispersion of results that are worse than such an expectation. SDR combines\\nExpected Shortfall (ES) and Shortfall Deviation (SD), which we also introduce,\\ncontemplating two fundamental pillars of the risk concept, the probability of\\nadverse events and the variability of an expectation, and considers extreme\\nresults. We demonstrate that SD is a generalized deviation measure, whereas SDR\\nis a coherent risk measure.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Shortfall Deviation Risk: An alternative to risk measurement',\n",
       "  'text': 'We achieve the dual representation of SDR, and we\\ndiscuss issues such as its representation by a weighted ES, acceptance sets,\\nconvexity, continuity and the relationship with stochastic dominance. Illustrations with real and simulated data allow us to conclude that SDR offers\\ngreater protection in risk measurement compared with VaR and ES, especially in\\ntimes of significant turbulence in riskier scenarios.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Optimal risk allocation in a market with non-convex preferences',\n",
       "  'text': 'The aims of this study are twofold. First, we consider an optimal risk\\nallocation problem with non-convex preferences. By establishing an infimal\\nrepresentation for distortion risk measures, we give some necessary and\\nsufficient conditions for the existence of optimal and asymptotic optimal\\nallocations. We will show that, similar to a market with convex preferences, in\\na non-convex framework with distortion risk measures the boundedness of the\\noptimal risk allocation problem depends only on the preferences. Second, we\\nconsider the same optimal allocation problem by adding a further assumption\\nthat allocations are co-monotone.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Optimal risk allocation in a market with non-convex preferences',\n",
       "  'text': 'We characterize the co-monotone optimal risk\\nallocations within which we prove the \"marginal risk allocations\" take only the\\nvalues zero or one. Remarkably, we can separate the role of the market\\npreferences and the total risk in our representation.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'New class of distortion risk measures and their tail asymptotics with\\n  emphasis on VaR',\n",
       "  'text': 'Distortion risk measures are extensively used in finance and insurance\\napplications because of their appealing properties. We present three methods to\\nconstruct new class of distortion functions and measures. The approach involves\\nthe composting methods, the mixing methods and the approach that based on the\\ntheory of copula. Subadditivity is an important property when aggregating risks\\nin order to preserve the benefits of diversification. However, Value at risk\\n(VaR), as the most well-known example of distortion risk measure is not always\\nglobally subadditive, except of elliptically distributed risks. In this paper,\\ninstead of study subadditivity we investigate the tail subadditivity for VaR\\nand other distortion risk measures.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'New class of distortion risk measures and their tail asymptotics with\\n  emphasis on VaR',\n",
       "  'text': 'In particular, we demonstrate that VaR is\\ntail subadditive for the case where the support of risk is bounded. Various\\nexamples are also presented to illustrate the results.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'On the properties of the Lambda value at risk: robustness, elicitability\\n  and consistency',\n",
       "  'text': 'Recently, financial industry and regulators have enhanced the debate on the\\ngood properties of a risk measure. A fundamental issue is the evaluation of the\\nquality of a risk estimation. On the one hand, a backtesting procedure is\\ndesirable for assessing the accuracy of such an estimation and this can be\\nnaturally achieved by elicitable risk measures. For the same objective, an\\nalternative approach has been introduced by Davis (2016) through the so-called\\nconsistency property. On the other hand, a risk estimation should be less\\nsensitive with respect to small changes in the available data set and exhibit\\nqualitative robustness.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'On the properties of the Lambda value at risk: robustness, elicitability\\n  and consistency',\n",
       "  'text': 'A new risk measure, the Lambda value at risk (Lambda\\nVaR), has been recently proposed by Frittelli et al. (2014), as a\\ngeneralization of VaR with the ability to discriminate the risk among P&L\\ndistributions with different tail behaviour. In this article, we show that\\nLambda VaR also satisfies the properties of robustness, elicitability and\\nconsistency under some conditions.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Optimal Risk Allocation in Reinsurance Networks',\n",
       "  'text': \"In this paper we consider reinsurance or risk sharing from a macroeconomic\\npoint of view. Our aim is to find socially optimal reinsurance treaties. In our\\nsetting we assume that there are $n$ insurance companies each bearing a certain\\nrisk and one representative reinsurer. The optimization problem is to minimize\\nthe sum of all capital requirements of the insurers where we assume that all\\ninsurance companies use a form of Range-Value-at-Risk. We show that in case all\\ninsurers use Value-at-Risk and the reinsurer's premium principle satisfies\\nmonotonicity, then layer reinsurance treaties are socially optimal. For this\\nresult we do not need any dependence structure between the risks.\",\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Optimal Risk Allocation in Reinsurance Networks',\n",
       "  'text': 'In the\\ngeneral setting with Range-Value-at-Risk we obtain again the optimality of\\nlayer reinsurance treaties under further assumptions, in particular under the\\nassumption that the individual risks are positively dependent through the\\nstochastic ordering. At the end, we discuss the difference between socially\\noptimal reinsurance treaties and individually optimal ones by looking at a\\nnumber of special cases.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Quantification of systemic risk from overlapping portfolios in the\\n  financial system',\n",
       "  'text': 'Financial markets are exposed to systemic risk, the risk that a substantial\\nfraction of the system ceases to function and collapses. Systemic risk can\\npropagate through different mechanisms and channels of contagion. One important\\nform of financial contagion arises from indirect interconnections between\\nfinancial institutions mediated by financial markets. This indirect\\ninterconnection occurs when financial institutions invest in common assets and\\nis referred to as overlapping portfolios. In this work we quantify systemic\\nrisk from indirect interconnections between financial institutions.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Quantification of systemic risk from overlapping portfolios in the\\n  financial system',\n",
       "  'text': 'Having\\ncomplete information of security holdings of major Mexican financial\\nintermediaries and the ability to uniquely identify securities in their\\nportfolios, allows us to represent the Mexican financial system as a bipartite\\nnetwork of securities and financial institutions. This makes it possible to\\nquantify systemic risk arising from overlapping portfolios. We show that\\nfocusing only on direct exposures underestimates total systemic risk levels by\\nup to 50%. By representing the financial system as a multi-layer network of\\ndirect exposures (default contagion) and indirect exposures (overlapping\\nportfolios) we estimate the mutual influence of different channels of\\ncontagion.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Quantification of systemic risk from overlapping portfolios in the\\n  financial system',\n",
       "  'text': 'The method presented here is the first objective data-driven\\nquantification of systemic risk on national scales that includes overlapping\\nportfolios.',\n",
       "  'meta': {'_split_id': 2}},\n",
       " {'name': 'Extremal dependence and spatial risk measures for insured losses due to\\n  extreme winds',\n",
       "  'text': 'A meticulous assessment of the risk of impacts associated with extreme wind\\nevents is of great necessity for populations, civil authorities as well as the\\ninsurance industry. Using the concept of spatial risk measure and related set\\nof axioms introduced by Koch (2017, 2019), we quantify the risk of losses due\\nto extreme wind speeds. The insured cost due to wind events is proportional to\\nthe wind speed at a power ranging typically between 2 and 12. Hence we first\\nperform a detailed study of the correlation structure of powers of the\\nBrown-Resnick max-stable random fields and look at the influence of the power.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Extremal dependence and spatial risk measures for insured losses due to\\n  extreme winds',\n",
       "  'text': 'Then, using the latter results, we thoroughly investigate spatial risk measures\\nassociated with variance and induced by powers of max-stable random fields. In\\naddition, we show that spatial risk measures associated with several classical\\nrisk measures and induced by such cost fields satisfy (at least part of) the\\npreviously mentioned axioms under conditions which are generally satisfied for\\nthe risk of damaging extreme wind speeds. In particular, we specify the rates\\nof spatial diversification in different cases, which is valuable for the\\ninsurance industry.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'A framework for simulating systemic risk and its application to the\\n  South African banking sector',\n",
       "  'text': 'We present a network-based framework for simulating systemic risk that\\nconsiders shock propagation in banking systems. In particular, the framework\\nallows the modeller to reflect a top-down framework where a shock to one bank\\nin the system affects the solvency and liquidity position of other banks,\\nthrough systemic market risks and consequential liquidity strains. We\\nillustrate the framework with an application using South African bank balance\\nsheet data. Spikes in simulated assessments of systemic risk agree closely with\\nspikes in documented subjective assessments of this risk. This indicates that\\nnetwork models can be useful for monitoring systemic risk levels.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'A framework for simulating systemic risk and its application to the\\n  South African banking sector',\n",
       "  'text': 'The model\\nresults are sensitive to liquidity risk and market sentiment and therefore the\\nrelated parameters are important considerations when using a network approach\\nto systemic risk modelling.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Controlling systemic risk - network structures that minimize it and node\\n  properties to calculate it',\n",
       "  'text': 'Evaluation of systemic risk in networks of financial institutions in general\\nrequires information of inter-institution financial exposures. In the framework\\nof Debt Rank algorithm, we introduce an approximate method of systemic risk\\nevaluation which requires only node properties, such as total assets and\\nliabilities, as inputs. We demonstrate that this approximation captures a large\\nportion of systemic risk measured by Debt Rank. Furthermore, using Monte Carlo\\nsimulations, we investigate network structures that can amplify systemic risk. Indeed, while no topology in general sense is {\\\\em a priori} more stable if the\\nmarket is liquid [1], a larger complexity is detrimental for the overall\\nstability [2].',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Controlling systemic risk - network structures that minimize it and node\\n  properties to calculate it',\n",
       "  'text': 'Here we find that the measure of scalar assortativity correlates\\nwell with level of systemic risk. In particular, network structures with high\\nsystemic risk are scalar assortative, meaning that risky banks are mostly\\nexposed to other risky banks. Network structures with low systemic risk are\\nscalar disassortative, with interactions of risky banks with stable banks.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'The Impact of the Choice of Risk and Dispersion Measure on\\n  Procyclicality',\n",
       "  'text': 'Procyclicality of historical risk measure estimation means that one tends to\\nover-estimate future risk when present realized volatility is high and vice\\nversa under-estimate future risk when the realized volatility is low. Out of it\\ndifferent questions arise, relevant for applications and theory: What are the\\nfactors which affect the degree of procyclicality? More specifically, how does\\nthe choice of risk measure affect this? How does this behaviour vary with the\\nchoice of realized volatility estimator? How do different underlying model\\nassumptions influence the pro-cyclical effect?',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'The Impact of the Choice of Risk and Dispersion Measure on\\n  Procyclicality',\n",
       "  'text': 'In this paper we consider three\\ndifferent well-known risk measures (Value-at-Risk, Expected Shortfall,\\nExpectile), the r-th absolute centred sample moment, for any integer $r>0$, as\\nrealized volatility estimator (this includes the sample variance and the sample\\nmean absolute deviation around the sample mean) and two models (either an iid\\nmodel or an augmented GARCH($p$,$q$) model). We show that the strength of\\nprocyclicality depends on these three factors, the choice of risk measure, the\\nrealized volatility estimator and the model considered. But, no matter the\\nchoices, the procyclicality will always be present.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Dependent Conditional Value-at-Risk for Aggregate Risk Models',\n",
       "  'text': 'Risk measure forecast and model have been developed in order to not only\\nprovide better forecast but also preserve its (empirical) property especially\\ncoherent property. Whilst the widely used risk measure of Value-at-Risk (VaR)\\nhas shown its performance and benefit in many applications, it is in fact not a\\ncoherent risk measure. Conditional VaR (CoVaR), defined as mean of losses\\nbeyond VaR, is one of alternative risk measures that satisfies coherent\\nproperty. There has been several extensions of CoVaR such as Modified CoVaR\\n(MCoVaR) and Copula CoVaR (CCoVaR).',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Dependent Conditional Value-at-Risk for Aggregate Risk Models',\n",
       "  'text': 'In this paper, we propose another risk\\nmeasure, called Dependent CoVaR (DCoVaR), for a target loss that depends on\\nanother random loss, including model parameter treated as random loss. It is\\nfound that our DCoVaR outperforms than both MCoVaR and CCoVaR. Numerical\\nsimulation is carried out to illustrate the proposed DCoVaR. In addition, we do\\nan empirical study of financial returns data to compute the DCoVaR forecast for\\nheteroscedastic process.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Two-stage Stochastic Programming under Multivariate Risk Constraints\\n  with an Application to Humanitarian Relief Network Design',\n",
       "  'text': 'In this study, we consider two classes of multicriteria two-stage stochastic\\nprograms in finite probability spaces with multivariate risk constraints. The\\nfirst-stage problem features a multivariate stochastic benchmarking constraint\\nbased on a vector-valued random variable representing multiple and possibly\\nconflicting stochastic performance measures associated with the second-stage\\ndecisions. In particular, the aim is to ensure that the associated random\\noutcome vector of interest is preferable to a specified benchmark with respect\\nto the multivariate polyhedral conditional value-at-risk (CVaR) or a\\nmultivariate stochastic order relation. In this case, the classical\\ndecomposition methods cannot be used directly due to the complicating\\nmultivariate stochastic benchmarking constraints.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Two-stage Stochastic Programming under Multivariate Risk Constraints\\n  with an Application to Humanitarian Relief Network Design',\n",
       "  'text': 'We propose an exact unified\\ndecomposition framework for solving these two classes of optimization problems\\nand show its finite convergence. We apply the proposed approach to a stochastic\\nnetwork design problem in a pre-disaster humanitarian logistics context and\\nconduct a computational study concerning the threat of hurricanes in the\\nSoutheastern part of the United States. Our numerical results on these\\nlarge-scale problems show that our proposed algorithm is computationally\\nscalable.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Bi-objective facility location in the presence of uncertainty',\n",
       "  'text': 'Multiple and usually conflicting objectives subject to data uncertainty are\\nmain features in many real-world problems. Consequently, in practice,\\ndecision-makers need to understand the trade-off between the objectives,\\nconsidering different levels of uncertainty in order to choose a suitable\\nsolution. In this paper, we consider a two-stage bi-objective\\nlocation-allocation model to design a last-mile network in disaster relief\\nwhere one of the objectives is subject to demand uncertainty. We analyze\\nscenario-based two-stage risk-neutral stochastic programming, adaptive\\n(two-stage) robust optimization, and a two-stage risk-averse stochastic\\napproach using conditional value-at-risk (CVaR).',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Bi-objective facility location in the presence of uncertainty',\n",
       "  'text': 'To cope with the bi-objective\\nnature of the problem, we embed these concepts into two criterion space search\\nframeworks, the $\\\\epsilon$-constraint method and the balanced box method, to\\ndetermine the Pareto frontier. We propose a decomposition-based algorithm to\\ndeal with the computationally challenging representation of the two-stage CVaR\\nmodel. Additionally, a matheuristic technique is developed to obtain\\nhigh-quality approximations of the Pareto frontier for large-size instances. Finally, we evaluate and compare the performance of the applied approaches\\nbased on real-world data from a Thies drought case, Senegal.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Risk Management and Return Prediction',\n",
       "  'text': 'With the good development in the financial industry, the market starts to\\ncatch people\\'s eyes, not only by the diversified investing choices ranging from\\nbonds and stocks to futures and options but also by the general \"high-risk,\\nhigh-reward\" mindset prompting people to put money in the financial market. People are interested in reducing risk at a given level of return since there\\nis no way of having both high returns and low risk.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Risk Management and Return Prediction',\n",
       "  'text': 'Many researchers have been\\nstudying this issue, and the most pioneering one is Harry Markowitz\\'s Modern\\nPortfolio Theory developed in 1952, which is the cornerstone of investment\\nportfolio management and aims at \"maximum the return at the given risk\". In\\ncontrast to that, fifty years later, E. Robert Fernholz\\'s Stochastic Portfolio\\nTheory, as opposed to the normative assumption served as the basis of earlier\\nmodern portfolio theory, is consistent with the observable characteristics of\\nactual portfolios and markets.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Risk Management and Return Prediction',\n",
       "  'text': \"In this paper, after introducing some basic\\ntheories of Markowitz's MPT and Fernholz's SPT, then we step across to the\\napplication side, trying to figure out under four basic models based on\\nMarkowitz Efficient Frontier, including Markowitz Model, Constant Correlation\\nModel, Single Index Model, and Multi-Factor Model, which portfolios will be\\nselected and how do these portfolios perform in the real world. Here we also\\ninvolve universal Portfolio Algorithmby Thomas M. Cover to select portfolios as\\na comparison. Besides, each portfolio value at Risk, Expected Shortfall, and\\ncorresponding bootstrap confidence interval for risk management will be\\nevaluated.\",\n",
       "  'meta': {'_split_id': 2}},\n",
       " {'name': 'Risk Management and Return Prediction',\n",
       "  'text': 'Finally, by utilizing factor analysis and time series models, we\\ncould predict the future performance of our four models.',\n",
       "  'meta': {'_split_id': 3}},\n",
       " {'name': 'The Structural Modelling of Operational Risk via Bayesian inference:\\n  Combining Loss Data with Expert Opinions',\n",
       "  'text': \"To meet the Basel II regulatory requirements for the Advanced Measurement\\nApproaches, the bank's internal model must include the use of internal data,\\nrelevant external data, scenario analysis and factors reflecting the business\\nenvironment and internal control systems. Quantification of operational risk\\ncannot be based only on historical data but should involve scenario analysis. Historical internal operational risk loss data have limited ability to predict\\nfuture behaviour moreover, banks do not have enough internal data to estimate\\nlow frequency high impact events adequately. Historical external data are\\ndifficult to use due to different volumes and other factors.\",\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'The Structural Modelling of Operational Risk via Bayesian inference:\\n  Combining Loss Data with Expert Opinions',\n",
       "  'text': 'In addition,\\ninternal and external data have a survival bias, since typically one does not\\nhave data of all collapsed companies. The idea of scenario analysis is to\\nestimate frequency and severity of risk events via expert opinions taking into\\naccount bank environment factors with reference to events that have occurred\\n(or may have occurred) in other banks. Scenario analysis is forward looking and\\ncan reflect changes in the banking environment. It is important to not only\\nquantify the operational risk capital but also provide incentives to business\\nunits to improve their risk management policies, which can be accomplished\\nthrough scenario analysis.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'The Structural Modelling of Operational Risk via Bayesian inference:\\n  Combining Loss Data with Expert Opinions',\n",
       "  'text': 'By itself, scenario analysis is very subjective but\\ncombined with loss data it is a powerful tool to estimate operational risk\\nlosses. Bayesian inference is a statistical technique well suited for combining\\nexpert opinions and historical data. In this paper, we present examples of the\\nBayesian inference methods for operational risk quantification.',\n",
       "  'meta': {'_split_id': 2}},\n",
       " {'name': 'Portfolio management under risk contraints - Lectures given at\\n  MITACS-PIMS-UBC Summer School in Risk Management and Risk Sharing',\n",
       "  'text': 'The aim of these lectures at MITACS-PIMS-UBC Summer School in Risk Man-\\nagement and Risk Sharing is to discuss risk controlled approaches for the\\npricing and hedging of financial risks. We will start with the classical dual\\napproach for financial markets, which al- lows to rewrite super-hedging\\nproblems in terms of optimal control problems in standard form. Based on this,\\nwe shall then consider hedging and pricing prob- lems under utility or risk\\nminimization criteria. This approach will turn out to be powerful whenever\\nlinear (or essentially linear) problems are considered, but not adapted to more\\ngeneral settings with non-linear dynamics (e.g.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Portfolio management under risk contraints - Lectures given at\\n  MITACS-PIMS-UBC Summer School in Risk Management and Risk Sharing',\n",
       "  'text': 'large investor models, high\\nfrequency trading with market impact features, mixed finance/insurance issues). In the second part of this lecture, we will develop on a new approach for risk\\ncontrol problems based on a stochastic target formulation. We will see how\\nflexible this approach is and how it allows to characterize very easily super-\\nhedging prices in term of suitable Hamilton-Jacobi-Bellman type partial differ-\\nential equations (PDEs). We will then see how quantile hedging and expected\\nloss pricing problems can be embeded into this framework, for a very large\\nclass of financial models.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Portfolio management under risk contraints - Lectures given at\\n  MITACS-PIMS-UBC Summer School in Risk Management and Risk Sharing',\n",
       "  'text': 'We shall finally consider a simple example of\\noptimal book liquidation in which the control is a continuous non-decreasing\\nprocess, as an illustration of possible practical developments in optimal\\ntrading under risk constraint.These lectures are organized in small chapters,\\neach of them being focused on a particular aspect.',\n",
       "  'meta': {'_split_id': 2}},\n",
       " {'name': 'Infection Risk Score: Identifying the risk of infection propagation\\n  based on human contact',\n",
       "  'text': 'A wide range of approaches have been applied to manage the spread of global\\npandemic events such as COVID-19, which have met with varying degrees of\\nsuccess. Given the large-scale social and economic impact coupled with the\\nincreasing time span of the pandemic, it is important to not only manage the\\nspread of the disease but also put extra efforts on measures that expedite\\nresumption of social and economic life. It is therefore important to identify\\nsituations that carry high risk, and act early whenever such situations are\\nidentified.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Infection Risk Score: Identifying the risk of infection propagation\\n  based on human contact',\n",
       "  'text': 'While a large number of mobile applications have been developed,\\nthey are aimed at obtaining information that can be used for contact tracing,\\nbut not at estimating the risk of social situations. In this paper, we\\nintroduce an infection risk score that provides an estimate of the infection\\nrisk arising from human contacts. Using a real-world human contact dataset, we\\nshow that the proposed risk score can provide a realistic estimate of the level\\nof risk in the population. We also describe how the proposed infection risk\\nscore can be implemented on smartphones.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Infection Risk Score: Identifying the risk of infection propagation\\n  based on human contact',\n",
       "  'text': 'Finally, we identify representative\\nuse cases that can leverage the risk score to minimize infection propagation.',\n",
       "  'meta': {'_split_id': 2}},\n",
       " {'name': 'Opening discussion on banking sector risk exposures and vulnerabilities\\n  from virtual currencies: An operational risk perspective',\n",
       "  'text': 'We develop the first basic Operational Risk perspective on key risk\\nmanagement issues associated with the development of new forms of electronic\\ncurrency in the real economy. In particular, we focus on understanding the\\ndevelopment of new risks types and the evolution of current risk types as new\\ncomponents of financial institutions arise to cater for an increasing demand\\nfor electronic money, micro-payment systems, Virtual money and cryptographic\\n(Crypto) currencies. In particular, this paper proposes a framework of risk\\nidentification and assessment applied to Virtual and Crypto currencies from a\\nbanking regulation perspective.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Opening discussion on banking sector risk exposures and vulnerabilities\\n  from virtual currencies: An operational risk perspective',\n",
       "  'text': 'In doing so, it addresses the topical issues of\\nunderstanding important key Operational Risk vulnerabilities and exposure risk\\ndrivers under the framework of the Basel II/III banking regulation,\\nspecifically associated with Virtual and Crypto currencies. This is critical to\\nconsider should such alternative currencies continue to grow in utilisation to\\nthe point that they enter into the banking sector, through commercial banks and\\nfinancial institutions who are beginning to contemplate their recognition in\\nterms of deposits, transactions and exchangeability for fiat currencies.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Opening discussion on banking sector risk exposures and vulnerabilities\\n  from virtual currencies: An operational risk perspective',\n",
       "  'text': 'We highlight how some of the features of Virtual and Crypto currencies are\\nimportant drivers of Operational Risk, posing both management and regulatory\\nchallenges that must start to be considered and addressed both by regulators,\\ncentral banks and security exchanges. In this paper we focus purely on the\\nOperational Risk perspective of banks operating in an environment where such\\nelectronic Virtual currencies are available. Some aspects of this discussion\\nare directly relevant now, whilst others can be understood as discussions to\\nraise awareness of issues in Operational Risk that will arise as Virtual\\ncurrency start to interact more widely in the real economy.',\n",
       "  'meta': {'_split_id': 2}},\n",
       " {'name': 'Resilient Disaster Recovery Logistics of Distribution Systems:\\n  Co-Optimize Service Restoration with Repair Crew and Mobile Power Source\\n  Dispatch',\n",
       "  'text': 'Repair crews (RCs) and mobile power sources (MPSs) are critical resources for\\ndistribution system (DS) outage management after a natural disaster. However,\\ntheir logistics is not well investigated. We propose a resilient scheme for\\ndisaster recovery logistics to co-optimize DS restoration with dispatch of RCs\\nand MPSs. A novel co-optimization model is formulated to route RCs and MPSs in\\nthe transportation network, schedule them in the DS, and reconfigure the DS for\\nmicrogrid formation coordinately, etc. The model incorporates different\\ntimescales of DS restoration and RC/MPS dispatch, the coupling of\\ntransportation and power networks, etc.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Resilient Disaster Recovery Logistics of Distribution Systems:\\n  Co-Optimize Service Restoration with Repair Crew and Mobile Power Source\\n  Dispatch',\n",
       "  'text': 'To ensure radiality of the DS with\\nvariable physical structure and MPS allocation, we also model topology\\nconstraints based on the concept of spanning forest. The model is convexified\\nequivalently and linearized into a mixed-integer linear programming. To reduce\\nits computation time, preprocessing methods are proposed to pre-assign a\\nminimal set of repair tasks to depots and reduce the number of candidate nodes\\nfor MPS connection. Resilient recovery strategies thus are generated to enhance\\nservice restoration, especially by dynamic formation of microgrids that are\\npowered by MPSs and topologized by repair actions of RCs and network\\nreconfiguration of the DS. Case studies demonstrate the proposed methodology.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'A system for the 2019 Sentiment, Emotion and Cognitive State Task of\\n  DARPAs LORELEI project',\n",
       "  'text': 'During the course of a Humanitarian Assistance-Disaster Relief (HADR) crisis,\\nthat can happen anywhere in the world, real-time information is often posted\\nonline by the people in need of help which, in turn, can be used by different\\nstakeholders involved with management of the crisis. Automated processing of\\nsuch posts can considerably improve the effectiveness of such efforts; for\\nexample, understanding the aggregated emotion from affected populations in\\nspecific areas may help inform decision-makers on how to best allocate\\nresources for an effective disaster response. However, these efforts may be\\nseverely limited by the availability of resources for the local language.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'A system for the 2019 Sentiment, Emotion and Cognitive State Task of\\n  DARPAs LORELEI project',\n",
       "  'text': 'The\\nongoing DARPA project Low Resource Languages for Emergent Incidents (LORELEI)\\naims to further language processing technologies for low resource languages in\\nthe context of such a humanitarian crisis. In this work, we describe our\\nsubmission for the 2019 Sentiment, Emotion and Cognitive state (SEC) pilot task\\nof the LORELEI project. We describe a collection of sentiment analysis systems\\nincluded in our submission along with the features extracted. Our fielded\\nsystems obtained the best results in both English and Spanish language\\nevaluations of the SEC pilot task.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'I-AID: Identifying Actionable Information from Disaster-related Tweets',\n",
       "  'text': 'Social media data plays a significant role in modern disaster management by\\nproviding valuable data about affected people, donations, help requests, and\\nadvice. Recent studies highlight the need to filter information on social media\\ninto fine-grained content categories. However, identifying useful information\\nfrom massive amounts of social media posts during a crisis is a challenging\\ntask. Automatically categorizing the information (e.g., reports on affected\\nindividuals, donations, and volunteers) contained in these posts is vital for\\ntheir efficient handling and consumption by the communities affected and\\norganizations concerned.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'I-AID: Identifying Actionable Information from Disaster-related Tweets',\n",
       "  'text': 'In this paper, we propose a system, dubbed I-AID, to\\nautomatically filter tweets with critical or actionable information from the\\nenormous volume of social media data. Our system combines state-of-the-art\\napproaches to process and represents textual data in order to capture its\\nunderlying semantics. In particular, we use 1) Bidirectional Encoder\\nRepresentations from Transformers (commonly known as, BERT) to learn a\\ncontextualized vector representation of a tweet, and 2) a graph-based\\narchitecture to compute semantic correlations between the entities and hashtags\\nin tweets and their corresponding labels. We conducted our experiments on a\\nreal-world dataset of disaster-related tweets.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'I-AID: Identifying Actionable Information from Disaster-related Tweets',\n",
       "  'text': 'Our experimental results\\nindicate that our model outperforms state-of-the-art approaches baselines in\\nterms of F1-score by +11%.',\n",
       "  'meta': {'_split_id': 2}},\n",
       " {'name': 'Post-Hurricane Damage Assessment Using Satellite Imagery and Geolocation\\n  Features',\n",
       "  'text': 'Gaining timely and reliable situation awareness after hazard events such as a\\nhurricane is crucial to emergency managers and first responders. One effective\\nway to achieve that goal is through damage assessment. Recently, disaster\\nresearchers have been utilizing imagery captured through satellites or drones\\nto quantify the number of flooded/damaged buildings. In this paper, we propose\\na mixed data approach, which leverages publicly available satellite imagery and\\ngeolocation features of the affected area to identify damaged buildings after a\\nhurricane. The method demonstrated significant improvement from performing a\\nsimilar task using only imagery features, based on a case study of Hurricane\\nHarvey affecting Greater Houston area in 2017.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Post-Hurricane Damage Assessment Using Satellite Imagery and Geolocation\\n  Features',\n",
       "  'text': 'This result opens door to a wide\\nrange of possibilities to unify the advancement in computer vision algorithms\\nsuch as convolutional neural networks and traditional methods in damage\\nassessment, for example, using flood depth or bare-earth topology. In this\\nwork, a creative choice of the geolocation features was made to provide extra\\ninformation to the imagery features, but it is up to the users to decide which\\nother features can be included to model the physical behavior of the events,\\ndepending on their domain knowledge and the type of disaster. The dataset\\ncurated in this work is made openly available (DOI: 10.17603/ds2-3cca-f398).',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Effect of different patient peak arrivals on an Emergency Department via\\n  discrete event simulation',\n",
       "  'text': 'Emergency Departments (EDs) overcrowding is a well recognized worldwide\\nphenomenon. The consequences range from long waiting times for visits and\\ntreatment of patients up to life-threatening health conditions. The\\ninternational community is devoting greater and greater efforts to analyze this\\nphenomenon aiming at reducing waiting times, improving the quality of the\\nservice. Within this framework, we propose a Discrete Event Simulation (DES)\\nmodel to study the patient flows through a medium-size ED located in a region\\nof Central Italy recently hit by a severe earthquake.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Effect of different patient peak arrivals on an Emergency Department via\\n  discrete event simulation',\n",
       "  'text': 'In particular, our aim is\\nto simulate unusual ED conditions, corresponding to critical events (like a\\nnatural disaster) that cause a sudden spike in the number of patient arrivals. The availability of detailed data concerning the ED processes enabled to build\\nan accurate DES model and to perform extensive scenario analyses. The model\\nprovides a valid decision support system for the ED managers also in defining\\nspecific emergency plans to be activated in case of mass casualty disasters.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Modeling Institutional Credit Risk with Financial News',\n",
       "  'text': \"Credit risk management, the practice of mitigating losses by understanding\\nthe adequacy of a borrower's capital and loan loss reserves, has long been\\nimperative to any financial institution's long-term sustainability and growth. MassMutual is no exception. The company is keen on effectively monitoring\\ndowngrade risk, or the risk associated with the event when credit rating of a\\ncompany deteriorates. Current work in downgrade risk modeling depends on\\nmultiple variations of quantitative measures provided by third-party rating\\nagencies and risk management consultancy companies.\",\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Modeling Institutional Credit Risk with Financial News',\n",
       "  'text': 'As these structured\\nnumerical data become increasingly commoditized among institutional investors,\\nthere has been a wide push into using alternative sources of data, such as\\nfinancial news, earnings call transcripts, or social media content, to possibly\\ngain a competitive edge in the industry. The volume of qualitative information\\nor unstructured text data has exploded in the past decades and is now available\\nfor due diligence to supplement quantitative measures of credit risk. This\\npaper proposes a predictive downgrade model using solely news data represented\\nby neural network embeddings. The model standalone achieves an Area Under the\\nReceiver Operating Characteristic Curve (AUC) of more than 80 percent.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Modeling Institutional Credit Risk with Financial News',\n",
       "  'text': 'The\\noutput probability from this news model, as an additional feature, improves the\\nperformance of our benchmark model using only quantitative measures by more\\nthan 5 percent in terms of both AUC and recall rate. A qualitative evaluation\\nalso indicates that news articles related to our predicted downgrade events are\\nspecially relevant and high-quality in our business context.',\n",
       "  'meta': {'_split_id': 2}},\n",
       " {'name': 'On the Fractal Geometry of the Balance Sheet and the Fractal Index of\\n  Insolvency Risk',\n",
       "  'text': 'This paper reviews the economic and theoretical foundations of insolvency\\nrisk measurement and capital adequacy rules. The proposed new measure of\\ninsolvency risk is constructed by disentangling assets, debt and equity at the\\nmicro-prudential firm level. This new risk index is the Firm Insolvency Risk\\nIndex (FIRI) which is symmetrical, proportional and scale invariant. We\\ndemonstrate that the balance sheet can be shown to evolve with a fractal\\npattern. As such we construct a fractal index that can measure the risk of\\nassets.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'On the Fractal Geometry of the Balance Sheet and the Fractal Index of\\n  Insolvency Risk',\n",
       "  'text': 'This index can differentiate between the similarity and dissimilarity\\nin asset risk, and it will also possess the properties of being self-similar\\nand invariant to firm characteristics that make up its asset composition hence\\ninvariant to all types of risk derived from assets. Self-similarity and scale\\ninvariance across the cross section allows direct comparison of degrees of risk\\nin assets. This is by comparing the risk dissimilarity of assets. Being\\nnaturally bounded to its highest upper bound, (0,2], the fractal index is able\\nto serve like a risk thermometer.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'On the Fractal Geometry of the Balance Sheet and the Fractal Index of\\n  Insolvency Risk',\n",
       "  'text': 'We assign geometric probabilities of\\ninsolvency P (equity is equal or less than 0 conditional on debt being greater\\nthan 0).',\n",
       "  'meta': {'_split_id': 2}},\n",
       " {'name': 'Quantitative Statistical Robustness for Tail-Dependent Law Invariant\\n  Risk Measures',\n",
       "  'text': 'When estimating the risk of a financial position with empirical data or Monte\\nCarlo simulations via a tail-dependent law invariant risk measure such as the\\nConditional Value-at-Risk (CVaR), it is important to ensure the robustness of\\nthe statistical estimator particularly when the data contain noise. Kr\\x7fatscher\\net al. [1] propose a new framework to examine the qualitative robustness of\\nestimators for tail-dependent law invariant risk measures on Orlicz spaces,\\nwhich is a step further from earlier work for studying the robustness of risk\\nmeasurement procedures by Cont et al. [2].',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Quantitative Statistical Robustness for Tail-Dependent Law Invariant\\n  Risk Measures',\n",
       "  'text': 'In this paper, we follow the stream\\nof research to propose a quantitative approach for verifying the statistical\\nrobustness of tail-dependent law invariant risk measures. A distinct feature of\\nour approach is that we use the Fortet-Mourier metric to quantify the variation\\nof the true underlying probability measure in the analysis of the discrepancy\\nbetween the laws of the plug-in estimators of law invariant risk measure based\\non the true data and perturbed data, which enables us to derive an explicit\\nerror bound for the discrepancy when the risk functional is Lipschitz\\ncontinuous with respect to a class of admissible laws.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Quantitative Statistical Robustness for Tail-Dependent Law Invariant\\n  Risk Measures',\n",
       "  'text': 'Moreover, the newly\\nintroduced notion of Lipschitz continuity allows us to examine the degree of\\nrobustness for tail-dependent risk measures. Finally, we apply our quantitative\\napproach to some well-known risk measures to illustrate our theory.',\n",
       "  'meta': {'_split_id': 2}},\n",
       " {'name': 'Resilience for Landslide Geohazards and Promoting Strategies in the\\n  Three Gorges Reservoir Area',\n",
       "  'text': 'Recently, resilience is increasingly used as a concept for understanding\\nnatural disaster systems. Landslide is one of the most frequent geohazards in\\nthe Three Gorges Reservoir Area (TGRA).However, it is difficult to measure\\nlocal disaster resilience, because of special geographical location in the TGRA\\nand the special disaster landslide. Current approaches to disaster resilience\\nevaluation are usually limited either by the qualitative method or properties\\nof different disaster. Therefore, practical evaluating methods for the disaster\\nresilience are needed. In this study, we developed an indicator system to\\nevaluate landslides disaster resilience in the TGRE at the county level.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Resilience for Landslide Geohazards and Promoting Strategies in the\\n  Three Gorges Reservoir Area',\n",
       "  'text': 'It\\nincludes two properties of inherent geological stress and external social\\nresponse, which are summarized into physical stress and social forces. The\\nevaluated disaster resilience can be simulated for promoting strategies with\\nfuzzy cognitive map (FCM).',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Scaling conditional tail probability and quantile estimators',\n",
       "  'text': 'We present a novel procedure for scaling relatively high frequency tail\\nprobability and quantile estimates for the conditional distribution of returns.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Optimal Hedging for Fund & Insurance Managers with Partially Observable\\n  Investment Flows',\n",
       "  'text': 'All the financial practitioners are working in incomplete markets full of\\nunhedgeable risk-factors. Making the situation worse, they are only equipped\\nwith the imperfect information on the relevant processes. In addition to the\\nmarket risk, fund and insurance managers have to be prepared for sudden and\\npossibly contagious changes in the investment flows from their clients so that\\nthey can avoid the over- as well as under-hedging. In this work, the prices of\\nsecurities, the occurrences of insured events and (possibly a network of) the\\ninvestment flows are used to infer their drifts and intensities by a stochastic\\nfiltering technique.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Optimal Hedging for Fund & Insurance Managers with Partially Observable\\n  Investment Flows',\n",
       "  'text': 'We utilize the inferred information to provide the optimal\\nhedging strategy based on the mean-variance (or quadratic) risk criterion. A\\nBSDE approach allows a systematic derivation of the optimal strategy, which is\\nshown to be implementable by a set of simple ODEs and the standard Monte Carlo\\nsimulation. The presented framework may also be useful for manufactures and\\nenergy firms to install an efficient overlay of dynamic hedging by financial\\nderivatives to minimize the costs.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'European banking supervision, the role of stress test. Some brief\\n  considerations',\n",
       "  'text': 'A quick review of European financial stability institutions and the role of\\nstress tests in the current juridical system.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Menger 1934 revisited',\n",
       "  'text': \"Karl Menger's 1934 paper on the St. Petersburg paradox contains mathematical\\nerrors that invalidate his conclusion that unbounded utility functions,\\nspecifically Bernoulli's logarithmic utility, fail to resolve modified versions\\nof the St. Petersburg paradox.\",\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Default Contagion with Domino Effect , A First Passage Time Approach',\n",
       "  'text': 'The present paper introduces a structural framework to model dependent\\ndefaults, with a particular interest in their contagion.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Shortfall Minimization for Game Options in Discrete Time',\n",
       "  'text': 'We prove existence of a self-financing strategy which minimizes shortfall for\\ngame options in discrete time',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Multivariate Modeling of Natural Gas Spot Trading Hubs Incorporating\\n  Futures Market Realized Volatility',\n",
       "  'text': 'Financial markets for Liquified Natural Gas (LNG) are an important and\\nrapidly-growing segment of commodities markets. Like other commodities markets,\\nthere is an inherent spatial structure to LNG markets, with different price\\ndynamics for different points of delivery hubs. Certain hubs support highly\\nliquid markets, allowing efficient and robust price discovery, while others are\\nhighly illiquid, limiting the effectiveness of standard risk management\\ntechniques. We propose a joint modeling strategy, which uses high-frequency\\ninformation from thickly-traded hubs to improve volatility estimation and risk\\nmanagement at thinly traded hubs.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Multivariate Modeling of Natural Gas Spot Trading Hubs Incorporating\\n  Futures Market Realized Volatility',\n",
       "  'text': 'The resulting model has superior in- and\\nout-of-sample predictive performance, particularly for several commonly used\\nrisk management metrics, demonstrating that joint modeling is indeed possible\\nand useful. To improve estimation, a Bayesian estimation strategy is employed\\nand data-driven weakly informative priors are suggested. Our model is robust to\\nsparse data and can be effectively used in any market with similar irregular\\npatterns of data availability.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'The Market Measure of Carbon Risk and its Impact on the Minimum Variance\\n  Portfolio',\n",
       "  'text': 'Like ESG investing, climate change is an important concern for asset managers\\nand owners, and a new challenge for portfolio construction. Until now,\\ninvestors have mainly measured carbon risk using fundamental approaches, such\\nas with carbon intensity metrics. Nevertheless, it has not been proven that\\nasset prices are directly impacted by these fundamental-based measures. In this\\npaper, we focus on another approach, which consists in measuring the\\nsensitivity of stock prices with respect to a carbon risk factor.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'The Market Measure of Carbon Risk and its Impact on the Minimum Variance\\n  Portfolio',\n",
       "  'text': 'In our\\nopinion, carbon betas are market-based measures that are complementary to\\ncarbon intensities or fundamental-based measures when managing investment\\nportfolios, because carbon betas may be viewed as an extension or\\nforward-looking measure of the current carbon footprint. In particular, we show\\nhow this new metric can be used to build minimum variance strategies and how\\nthey impact their portfolio construction.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Insurance Business and Sustainable Development',\n",
       "  'text': 'In this study, we will discuss recent developments in risk management of the\\nglobal financial and insurance business with respect to sustainable\\ndevelopment. So far climate change aspects have been the dominant aspect in\\nmanaging sustainability risks and opportunities, accompanied by the development\\nof several legislative initiatives triggered by supervisory authorities. However, a sole concentration on these aspects misses out other important\\neconomic and social facets of sustainable development goals formulated by the\\nUN. Such aspects have very recently come into the focus of the European\\nCommittee concerning the Solvency II project for the European insurance\\nindustry.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Insurance Business and Sustainable Development',\n",
       "  'text': 'Clearly the new legislative expectations can be better handled by\\nlarger insurance companies and holdings than by small- and medium-sized mutual\\ninsurance companies which are numerous in central Europe, due to their historic\\ndevelopment starting in the late medieval ages and early modern times. We\\ntherefore also concentrate on strategies within the risk management of such\\nsmall- and medium-sized enterprises that can be achieved without much effort,\\nin particular those that are not directly related to climate change.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Russian-Doll Risk Models',\n",
       "  'text': 'We give a simple explicit algorithm for building multi-factor risk models. It\\ndramatically reduces the number of or altogether eliminates the risk factors\\nfor which the factor covariance matrix needs to be computed. This is achieved\\nvia a nested \"Russian-doll\" embedding: the factor covariance matrix itself is\\nmodeled via a factor model, whose factor covariance matrix in turn is modeled\\nvia a factor model, and so on. We discuss in detail how to implement this\\nalgorithm in the case of (binary) industry classification based risk factors\\n(e.g., \"sector -> industry -> sub-industry\"), and also in the presence of\\n(non-binary) style factors.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Russian-Doll Risk Models',\n",
       "  'text': 'Our algorithm is particularly useful when long\\nhistorical lookbacks are unavailable or undesirable, e.g., in short-horizon\\nquant trading.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Multi-curve HJM modelling for risk management',\n",
       "  'text': 'We present a HJM approach to the projection of multiple yield curves\\ndeveloped to capture the volatility content of historical term structures for\\nrisk management purposes. Since we observe the empirical data at daily\\nfrequency and only for a finite number of time-to-maturity buckets, we propose\\na modelling framework which is inherently discrete. In particular, we show how\\nto approximate the HJM continuous time description of the multi-curve dynamics\\nby a Vector Autoregressive process of order one. The resulting dynamics lends\\nitself to a feasible estimation of the model volatility-correlation structure\\nand market risk-premia.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Multi-curve HJM modelling for risk management',\n",
       "  'text': 'Then, resorting to the Principal Component Analysis we\\nfurther simplify the dynamics reducing the number of covariance components. Applying the constant volatility version of our model on a sample of curves\\nfrom the Euro area, we demonstrate its forecasting ability through an\\nout-of-sample test.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Alarm System for Insurance Companies: A Strategy for Capital Allocation',\n",
       "  'text': 'One possible way of risk management for an insurance company is to develop an\\nearly and appropriate alarm system before the possible ruin. The ruin is\\ndefined through the status of the aggregate risk process, which in turn is\\ndetermined by premium accumulation as well as claim settlement outgo for the\\ninsurance company. The main purpose of this work is to design an effective\\nalarm system, i.e. to define alarm times and to recommend augmentation of\\ncapital of suitable magnitude at those points to prevent or reduce the chance\\nof ruin.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Alarm System for Insurance Companies: A Strategy for Capital Allocation',\n",
       "  'text': 'To draw a fair measure of effectiveness of alarm system, comparison is\\ndrawn between an alarm system, with capital being added at the sound of every\\nalarm, and the corresponding system without any alarm, but an equivalently\\nhigher initial capital. Analytical results are obtained in general setup and\\nthis is backed up by simulated performances with various types of loss severity\\ndistributions. This provides a strategy for suitably spreading out the capital\\nand yet addressing survivability concerns at satisfactory level.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Managing Systematic Mortality Risk in Life Annuities: An Application of\\n  Longevity Derivatives',\n",
       "  'text': 'This paper assesses the hedge effectiveness of an index-based longevity swap\\nand a longevity cap. Although swaps are a natural instrument for hedging\\nlongevity risk, derivatives with non-linear pay-offs, such as longevity caps,\\nalso provide downside protection. A tractable stochastic mortality model with\\nage dependent drift and volatility is developed and analytical formulae for\\nprices of these longevity derivatives are derived. Hedge effectiveness is\\nconsidered for a hypothetical life annuity portfolio.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Managing Systematic Mortality Risk in Life Annuities: An Application of\\n  Longevity Derivatives',\n",
       "  'text': 'The hedging of the life\\nannuity portfolio is comprehensively assessed for a range of assumptions for\\nthe market price of longevity risk, the term to maturity of the hedging\\ninstruments, as well as the size of the underlying annuity portfolio. The model\\nis calibrated using Australian mortality data. The results provide a\\ncomprehensive analysis of longevity hedging, highlighting the risk management\\nbenefits and costs of linear and nonlinear payoff structures.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Optimal FX Hedge Tenor with Liquidity Risk',\n",
       "  'text': 'We develop an optimal currency hedging strategy for fund managers who own\\nforeign assets to choose the hedge tenors that maximize their FX carry returns\\nwithin a liquidity risk constraint. The strategy assumes that the offshore\\nassets are fully hedged with FX forwards. The chosen liquidity risk metric is\\nCash Flow at Risk (CFaR). The strategy involves time-dispersing the total\\nnominal hedge value into future time buckets to maximize (minimize) the\\nexpected FX carry benefit (cost), given the constraint that the CFaRs in all\\nthe future time buckets do not breach a predetermined liquidity budget.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Optimal FX Hedge Tenor with Liquidity Risk',\n",
       "  'text': 'We\\ndemonstrate the methodology via an illustrative example where shorter-dated\\nforwards are assumed to deliver higher carry trade returns (motivated by the\\nhistorical experience where AUD is the domestic currency and USD is the foreign\\ncurrency). We also introduce a tenor-ranking method which is useful when this\\nassumption fails. We show by Monte Carlo simulation and by backtesting that our\\nhedging strategy successfully operates within the liquidity budget. We provide\\npractical insights on when and why fund managers should choose short-dated or\\nlong-dated tenors.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Assessing Supply Chain Cyber Risks',\n",
       "  'text': 'Risk assessment is a major challenge for supply chain managers, as it\\npotentially affects business factors such as service costs, supplier\\ncompetition and customer expectations. The increasing interconnectivity between\\norganisations has put into focus methods for supply chain cyber risk\\nmanagement. We introduce a general approach to support such activity taking\\ninto account various techniques of attacking an organisation and its suppliers,\\nas well as the impacts of such attacks. Since data is lacking in many respects,\\nwe use structured expert judgment methods to facilitate its implementation. We\\ncouple a family of forecasting models to enrich risk monitoring.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Assessing Supply Chain Cyber Risks',\n",
       "  'text': 'The approach\\nmay be used to set up risk alarms, negotiate service level agreements, rank\\nsuppliers and identify insurance needs, among other management possibilities.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Stochastic reserving with a stacked model based on a hybridized\\n  Artificial Neural Network',\n",
       "  'text': 'Currently, legal requirements demand that insurance companies increase their\\nemphasis on monitoring the risks linked to the underwriting and asset\\nmanagement activities. Regarding underwriting risks, the main uncertainties\\nthat insurers must manage are related to the premium sufficiency to cover\\nfuture claims and the adequacy of the current reserves to pay outstanding\\nclaims. Both risks are calibrated using stochastic models due to their nature. This paper introduces a reserving model based on a set of machine learning\\ntechniques such as Gradient Boosting, Random Forest and Artificial Neural\\nNetworks. These algorithms and other widely used reserving models are stacked\\nto predict the shape of the runoff.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Stochastic reserving with a stacked model based on a hybridized\\n  Artificial Neural Network',\n",
       "  'text': 'To compute the deviation around a former\\nprediction, a log-normal approach is combined with the suggested model. The\\nempirical results demonstrate that the proposed methodology can be used to\\nimprove the performance of the traditional reserving techniques based on\\nBayesian statistics and a Chain Ladder, leading to a more accurate assessment\\nof the reserving risk.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Pandemic risk management: resources contingency planning and allocation',\n",
       "  'text': 'Repeated history of pandemics, such as SARS, H1N1, Ebola, Zika, and COVID-19,\\nhas shown that pandemic risk is inevitable. Extraordinary shortages of medical\\nresources have been observed in many parts of the world. Some attributing\\nfactors include the lack of sufficient stockpiles and the lack of coordinated\\nefforts to deploy existing resources to the location of greatest needs. The\\npaper investigates contingency planning and resources allocation from a risk\\nmanagement perspective, as opposed to the prevailing supply chain perspective. The key idea is that the competition of limited critical resources is not only\\npresent in different geographical locations but also at different stages of a\\npandemic.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Pandemic risk management: resources contingency planning and allocation',\n",
       "  'text': 'This paper draws on an analogy between risk aggregation and capital\\nallocation in finance and pandemic resources planning and allocation for\\nhealthcare systems. The main contribution is to introduce new strategies for\\noptimal stockpiling and allocation balancing spatio-temporal competitions of\\nmedical supply and demand.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Applying the CobiT Control Framework to Spreadsheet Developments',\n",
       "  'text': 'One of the problems reported by researchers and auditors in the field of\\nspreadsheet risks is that of getting and keeping managements attention to the\\nproblem. Since 1996, the Information Systems Audit & Control Foundation and the\\nIT Governance Institute have published CobiT which brings mainstream IT control\\nissues into the corporate governance arena. This paper illustrates how\\nspreadsheet risk and control issues can be mapped onto the CobiT framework and\\nthus brought to managers attention in a familiar format.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Varying the VaR for Unconditional and Conditional Environments',\n",
       "  'text': 'Accurate forecasting of risk is the key to successful risk management\\ntechniques. Using the largest stock index futures from twelve European bourses,\\nthis paper presents VaR measures based on their unconditional and conditional\\ndistributions for single and multi-period settings. These measures underpinned\\nby extreme value theory are statistically robust explicitly allowing for\\nfat-tailed densities. Conditional tail estimates are obtained by adjusting the\\nunconditional extreme value procedure with GARCH filtered returns. The\\nconditional modelling results in iid returns allowing for the use of a simple\\nand efficient multi-period extreme value scaling law. The paper examines the\\nproperties of these distinct conditional and unconditional trading models.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Varying the VaR for Unconditional and Conditional Environments',\n",
       "  'text': 'The\\npaper finds that the biases inherent in unconditional single and multi-period\\nestimates assuming normality extend to the conditional setting.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Computing Quantiles in Regime-Switching Jump-Diffusions with Application\\n  to Optimal Risk Management: a Fourier Transform Approach',\n",
       "  'text': 'In this paper we consider the problem of calculating the quantiles of a risky\\nposition, the dynamic of which is described as a continuous time\\nregime-switching jump-diffusion, by using Fourier Transform methods. Furthermore, we study a classical option-based portfolio strategy which\\nminimizes the Value-at-Risk of the hedged position and show the impact of jumps\\nand switching regimes on the optimal strategy in a numerical example. However,\\nthe analysis of this hedging strategy, as well as the computational technique\\nfor its implementation, is fairly general, i.e. it can be applied to any\\ndynamical model for which Fourier transform methods are viable.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'A Top-down Model for Cash CLO',\n",
       "  'text': 'We propose a top-down model for cash CLO. This model can consistently price\\ncash CLO tranches both within the same deal and across different deals. Meaningful risk measures for cash CLO tranches can also be defined and\\ncomputed. This method is self-consistent, easy to implement and computationally\\nefficient. It has the potential to bring the much needed pricing transparency\\nto the cash CLO markets; and it could also greatly improve the risk management\\nof cash instruments.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Estimation and prediction of credit risk based on rating transition\\n  systems',\n",
       "  'text': 'Risk management is an important practice in the banking industry. In this\\npaper we develop a new methodology to estimate and predict the probability of\\ndefault (PD) based on the rating transition matrices, which relates the rating\\ntransition matrices to the macroeconomic variables. Our method can overcome the\\nshortcomings of the framework of Belkin et al. (1998), and is especially useful\\nin predicting the PD and doing stress testing. Simulation is conducted at the\\nend, which shows that our method can provide more accurate estimate than that\\nobtained by the method of Belkin et al. (1998).',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Combining Alphas via Bounded Regression',\n",
       "  'text': 'We give an explicit algorithm and source code for combining alpha streams via\\nbounded regression. In practical applications typically there is insufficient\\nhistory to compute a sample covariance matrix (SCM) for a large number of\\nalphas. To compute alpha allocation weights, one then resorts to (weighted)\\nregression over SCM principal components. Regression often produces alpha\\nweights with insufficient diversification and/or skewed distribution against,\\ne.g., turnover. This can be rectified by imposing bounds on alpha weights\\nwithin the regression procedure. Bounded regression can also be applied to\\nstock and other asset portfolio construction. We discuss illustrative examples.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Minimal Investment Risk with Cost and Return Constraints: A Replica\\n  Analysis',\n",
       "  'text': 'Previous studies into the budget constraint of portfolio optimization\\nproblems based on statistical mechanical informatics have not considered that\\nthe purchase cost per unit of each asset is distinct. Moreover, the fact that\\nthe optimal investment allocation differs depending on the size of investable\\nfunds has also been neglected. In this paper, we approach the problem of\\ninvestment risk minimization using replica analysis. This problem imposes cost\\nand return constraints. We also derive the macroscopic theory indicated by the\\noptimal solution and confirm the validity of our proposed method through\\nnumerical experiments.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Discrete time portfolio optimisation managing value at risk under heavy\\n  tail return distribution',\n",
       "  'text': 'We consider an investor, whose portfolio consists of a single risky asset and\\na risk free asset, who wants to maximize his expected utility of the portfolio\\nsubject to the Value at Risk assuming a heavy tail distribution of the stock\\nprices return. We use Markov Decision Process and dynamic programming principle\\nto get the optimal strategies and the value function which maximize the\\nexpected utility for parametric as well as non parametric distributions. Due to\\nlack of explicit solution in the non parametric case, we use numerical\\nintegration for optimization',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'An Integrated Conceptual Model for Information System Security Risk\\n  Management and Enterprise Architecture Management based on TOGAF, ArchiMate,\\n  IAF and DoDAF',\n",
       "  'text': 'Risk management is today a major steering tool for any organization wanting\\nto deal with Information System (IS) security. However, IS Security Risk\\nManagement (ISSRM) remains difficult to establish and maintain, mainly in a\\ncontext of multi-regulations with complex and inter-connected IS. We claim that\\na connection with Enterprise Architecture Management (EAM) contributes to deal\\nwith these issues. A first step towards a better integration of both domains is\\nto define an integrated EAM-ISSRM conceptual model.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'An Integrated Conceptual Model for Information System Security Risk\\n  Management and Enterprise Architecture Management based on TOGAF, ArchiMate,\\n  IAF and DoDAF',\n",
       "  'text': 'Among the steps of the\\nresearch method followed to define such an integrated EAM-ISSRM conceptual,\\nthis technical report presents the whole outputs (through alignment tables) of\\nthe conceptual alignment between concepts used to model EA (based on ArchiMate,\\nTOGAF, IAF and DoDAF) and concepts of the ISSRM domain model.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Cybersecurity Cost of Quality: Managing the Costs of Cybersecurity Risk\\n  Management',\n",
       "  'text': 'There is no standard yet for measuring and controlling the costs associated\\nwith implementing cybersecurity programs. To advance research and practice\\ntowards this end, we develop a mapping using the well-known concept of quality\\ncosts and the Framework Core within the Cybersecurity Framework produced by the\\nNational Institute of Standards and Technology (NIST) in response to the\\nCybersecurity Enhancement Act of 2014. This mapping can be easily adopted by\\norganizations that are already using the NIST CSF for cybersecurity risk\\nmanagement to plan, manage, and continually improve cybersecurity operations.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Cybersecurity Cost of Quality: Managing the Costs of Cybersecurity Risk\\n  Management',\n",
       "  'text': 'If an organization is not using the NIST CSF, this mapping may still be useful\\nfor linking elements in accounting systems that are associated with\\ncybersecurity operations and risk management to a quality cost model.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Psychological model of the investor and manager behavior in risk',\n",
       "  'text': 'All people have to make risky decisions in everyday life. And we do not know\\nhow true they are. But is it possible to mathematically assess the correctness\\nof our choice? This article discusses the model of decision making under risk\\non the example of project management. This is a game with two players, one of\\nwhich is Investor, and the other is the Project Manager. Each player makes a\\nrisky decision for himself, based on his past experience. With the help of a\\nmathematical model, the players form a level of confidence, depending on who\\nthe player accepts the strategy or does not accept.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Psychological model of the investor and manager behavior in risk',\n",
       "  'text': 'The project manager\\nassesses the costs and compares them with the level of confidence. An investor\\nevaluates past results. Also visit the case where the strategy of the player\\naccepts the part.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'The StressVaR: A New Risk Concept for Superior Fund Allocation',\n",
       "  'text': 'In this paper we introduce a novel approach to risk estimation based on\\nnonlinear factor models - the \"StressVaR\" (SVaR). Developed to evaluate the\\nrisk of hedge funds, the SVaR appears to be applicable to a wide range of\\ninvestments. Its principle is to use the fairly short and sparse history of the\\nhedge fund returns to identify relevant risk factors among a very broad set of\\npossible risk sources. This risk profile is obtained by calibrating a\\ncollection of nonlinear single-factor models as opposed to a single\\nmulti-factor model.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'The StressVaR: A New Risk Concept for Superior Fund Allocation',\n",
       "  'text': 'We then use the risk profile and the very long and rich\\nhistory of the factors to asses the possible impact of known past crises on the\\nfunds, unveiling their hidden risks and so called \"black swans\". In backtests using data of 1060 hedge funds we demonstrate that the SVaR has\\nbetter or comparable properties than several common VaR measures - shows less\\nVaR exceptions and, perhaps even more importantly, in case of an exception, by\\nsmaller amounts. The ultimate test of the StressVaR however, is in its usage as a fund\\nallocating tool.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'The StressVaR: A New Risk Concept for Superior Fund Allocation',\n",
       "  'text': 'By simulating a realistic investment in a portfolio of hedge\\nfunds, we show that the portfolio constructed using the StressVaR on average\\noutperforms both the market and the portfolios constructed using common VaR\\nmeasures. For the period from Feb. 2003 to June 2009, the StressVaR constructed\\nportfolio outperforms the market by about 6% annually, and on average the\\ncompeting VaR measures by around 3%. The performance numbers from Aug. 2007 to\\nJune 2009 are even more impressive. The SVaR portfolio outperforms the market\\nby 20%, and the best competing measure by 4%.',\n",
       "  'meta': {'_split_id': 2}},\n",
       " {'name': 'A Directional Multivariate Value at Risk',\n",
       "  'text': 'In economics, insurance and finance, value at risk (VaR) is a widely used\\nmeasure of the risk of loss on a specific portfolio of financial assets. For a\\ngiven portfolio, time horizon, and probability $\\\\alpha$, the $100\\\\alpha\\\\%$ VaR\\nis defined as a threshold loss value, such that the probability that the loss\\non the portfolio over the given time horizon exceeds this value is $\\\\alpha$. That is to say, it is a quantile of the distribution of the losses, which has\\nboth good analytic properties and easy interpretation as a risk measure.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'A Directional Multivariate Value at Risk',\n",
       "  'text': 'However, its extension to the multivariate framework is not unique because a\\nunique definition of multivariate quantile does not exist. In the current\\nliterature, the multivariate quantiles are related to a specific partial order\\nconsidered in $\\\\mathbb{R}^{n}$, or to a property of the univariate quantile\\nthat is desirable to be extended to $\\\\mathbb{R}^{n}$. In this work, we\\nintroduce a multivariate value at risk as a vector-valued directional risk\\nmeasure, based on a directional multivariate quantile, which has recently been\\nintroduced in the literature. The directional approach allows the manager to\\nconsider external information or risk preferences in her/his analysis.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'A Directional Multivariate Value at Risk',\n",
       "  'text': 'We have\\nderived some properties of the risk measure and we have compared the univariate\\n\\\\textit{VaR} over the marginals with the components of the directional\\nmultivariate VaR. We have also analyzed the relationship between some families\\nof copulas, for which it is possible to obtain closed forms of the multivariate\\nVaR that we propose. Finally, comparisons with other alternative multivariate\\nVaR given in the literature, are provided in terms of robustness.',\n",
       "  'meta': {'_split_id': 2}},\n",
       " {'name': 'A Bayesian approach to the evaluation of risk-based microbiological\\n  criteria for \\\\uppercaseCampylobacter in broiler meat',\n",
       "  'text': 'Shifting from traditional hazard-based food safety management toward\\nrisk-based management requires statistical methods for evaluating intermediate\\ntargets in food production, such as microbiological criteria (MC), in terms of\\ntheir effects on human risk of illness. A fully risk-based evaluation of MC\\ninvolves several uncertainties that are related to both the underlying\\nQuantitative Microbiological Risk Assessment (QMRA) model and the\\nproduction-specific sample data on the prevalence and concentrations of\\nmicrobes in production batches. We used Bayesian modeling for statistical\\ninference and evidence synthesis of two sample data sets.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'A Bayesian approach to the evaluation of risk-based microbiological\\n  criteria for \\\\uppercaseCampylobacter in broiler meat',\n",
       "  'text': 'Thus, parameter\\nuncertainty was represented by a joint posterior distribution, which we then\\nused to predict the risk and to evaluate the criteria for acceptance of\\nproduction batches. We also applied the Bayesian model to compare alternative\\ncriteria, accounting for the statistical uncertainty of parameters, conditional\\non the data sets. Comparison of the posterior mean relative risk,\\n$E(\\\\mathit{RR}|\\\\mathrm{data})=E(P(\\\\mathrm{illness}|\\\\mathrm{criterion is\\nmet})/P(\\\\mathrm{illness})|\\\\mathrm{data})$, and relative posterior risk,\\n$\\\\mathit{RPR}=P(\\\\mathrm{illness}|\\\\mathrm{data, criterion is\\nmet})/P(\\\\mathrm{illness}|\\\\mathrm{data})$, showed very similar results, but\\ncomputing is more efficient for RPR. Based on the sample data, together with\\nthe QMRA model, one could achieve a relative risk of 0.4 by insisting that the\\ndefault criterion be fulfilled for acceptance of each batch.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Risk and Utility in Portfolio Optimization',\n",
       "  'text': 'Modern portfolio theory(MPT) addresses the problem of determining the optimum\\nallocation of investment resources among a set of candidate assets. In the\\noriginal mean-variance approach of Markowitz, volatility is taken as a proxy\\nfor risk, conflating uncertainty with risk. There have been many subsequent\\nattempts to alleviate that weakness which, typically, combine utility and risk. We present here a modification of MPT based on the inclusion of separate risk\\nand utility criteria. We define risk as the probability of failure to meet a\\npre-established investment goal. We define utility as the expectation of a\\nutility function with positive and decreasing marginal value as a function of\\nyield.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Risk and Utility in Portfolio Optimization',\n",
       "  'text': 'The emphasis throughout is on long investment horizons for which\\nrisk-free assets do not exist. Analytic results are presented for a Gaussian\\nprobability distribution. Risk-utility relations are explored via empirical\\nstock-price data, and an illustrative portfolio is optimized using the\\nempirical data.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Forecasting Portfolio Risk in Normal and Stressed Markets',\n",
       "  'text': 'The instability of historical risk factor correlations renders their use in\\nestimating portfolio risk extremely questionable. In periods of market stress\\ncorrelations of risk factors have a tendency to quickly go well beyond\\nestimated values. For instance, in times of severe market stress, one would\\nexpect with certainty to see the correlation of yield levels and credit spreads\\ngo to -1, even though historical estimates will miss this region of\\ncorrelation. This event might lead to realized portfolio risk profile\\nsubstantially different from what was initially estimated. The purpose of this\\npaper is to explore the correlation driven effects on fixed income portfolio\\nrisks.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Forecasting Portfolio Risk in Normal and Stressed Markets',\n",
       "  'text': 'To achieve this, we propose a methodology to estimate portfolio risks in\\nboth normal and stressed times using confidence weighted forecast correlations.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Dynamic operational risk: modeling dependence and combining different\\n  sources of information',\n",
       "  'text': 'In this paper, we model dependence between operational risks by allowing risk\\nprofiles to evolve stochastically in time and to be dependent. This allows for\\na flexible correlation structure where the dependence between frequencies of\\ndifferent risk categories and between severities of different risk categories\\nas well as within risk categories can be modeled. The model is estimated using\\nBayesian inference methodology, allowing for combination of internal data,\\nexternal data and expert opinion in the estimation procedure. We use a\\nspecialized Markov chain Monte Carlo simulation methodology known as Slice\\nsampling to obtain samples from the resulting posterior distribution and\\nestimate the model parameters.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Convex risk measures for good deal bounds',\n",
       "  'text': 'We study convex risk measures describing the upper and lower bounds of a good\\ndeal bound, which is a subinterval of a no-arbitrage pricing bound. We call\\nsuch a convex risk measure a good deal valuation and give a set of equivalent\\nconditions for its existence in terms of market. A good deal valuation is\\ncharacterized by several equivalent properties and in particular, we see that a\\nconvex risk measure is a good deal valuation only if it is given as a risk\\nindifference price. An application to shortfall risk measure is given.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Convex risk measures for good deal bounds',\n",
       "  'text': 'In\\naddition, we show that the no-free-lunch (NFL) condition is equivalent to the\\nexistence of a relevant convex risk measure which is a good deal valuation. The\\nrelevance turns out to be a condition for a good deal valuation to be\\nreasonable. Further we investigate conditions under which any good deal\\nvaluation is relevant.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Looking for grass-root sources of systemic risk: the case of\\n  \"cheques-as-collateral\" network',\n",
       "  'text': 'The global financial system has become highly connected and complex. Has been\\nproven in practice that existing models, measures and reports of financial risk\\nfail to capture some important systemic dimensions. Only lately, advisory\\nboards have been established in high level and regulations are directly\\ntargeted to systemic risk. In the same direction, a growing number of\\nresearchers employ network analysis to model systemic risk in financial\\nnetworks. Current approaches are concentrated on interbank payment network\\nflows in national and international level. This work builds on existing\\napproaches to account for systemic risk assessment in micro level.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Looking for grass-root sources of systemic risk: the case of\\n  \"cheques-as-collateral\" network',\n",
       "  'text': 'Particularly, we introduce the analysis of intra-bank financial risk\\ninterconnections, by examining the real case of \"cheques-as-collateral\" network\\nfor a major Greek bank. Our model offers useful information about the negative\\nspillovers of disruption to a financial entity in a bank\\'s lending network and\\ncould complement existing credit scoring models that account only for\\nidiosyncratic customer\\'s financial profile. Most importantly, the proposed\\nmethodology can be employed in many segments of the entire financial system,\\nproviding a useful tool in the hands of regulatory authorities in assessing\\nmore accurate estimates of systemic risk.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Comparative and qualitative robustness for law-invariant risk measures',\n",
       "  'text': \"When estimating the risk of a P&L from historical data or Monte Carlo\\nsimulation, the robustness of the estimate is important. We argue here that\\nHampel's classical notion of qualitative robustness is not suitable for risk\\nmeasurement and we propose and analyze a refined notion of robustness that\\napplies to tail-dependent law-invariant convex risk measures on Orlicz space. This concept of robustness captures the tradeoff between robustness and\\nsensitivity and can be quantified by an index of qualitative robustness. By\\nmeans of this index, we can compare various risk measures, such as distortion\\nrisk measures, in regard to their degree of robustness.\",\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Comparative and qualitative robustness for law-invariant risk measures',\n",
       "  'text': 'Our analysis also\\nyields results that are of independent interest such as continuity properties\\nand consistency of estimators for risk measures, or a Skorohod representation\\ntheorem for {\\\\psi}-weak convergence.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Aggregation operators for the measurement of systemic risk',\n",
       "  'text': 'The policy objective of safeguarding financial stability has stimulated a\\nwave of research on systemic risk analytics, yet it still faces challenges in\\nmeasurability. This paper models systemic risk by tapping into expert knowledge\\nof financial supervisors. We decompose systemic risk into a number of\\ninterconnected segments, for which the level of vulnerability is measured. The\\nsystem is modeled in the form of a Fuzzy Cognitive Map (FCM), in which nodes\\nrepresent vulnerability in segments and links their interconnectedness. A main\\nproblem tackled in this paper is the aggregation of values in different\\ninterrelated nodes of the network to obtain an estimate systemic risk.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Aggregation operators for the measurement of systemic risk',\n",
       "  'text': 'To this\\nend, the Choquet integral is employed for aggregating expert evaluations of\\nmeasures, as it allows for the integration of interrelations among factors in\\nthe aggregation process. The approach is illustrated through two applications\\nin a European setting. First, we provide an estimation of systemic risk with a\\nof pan-European set-up. Second, we estimate country-level risks, allowing for a\\nmore granular decomposition. This sets a starting point for the use of the\\nrich, oftentimes tacit, knowledge in policy organizations.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Factors influencing risk acceptance of Cloud Computing services in the\\n  UK Government',\n",
       "  'text': 'Cloud Computing services are increasingly being made available by the UK\\nGovernment through the Government digital marketplace to reduce costs and\\nimprove IT efficiency; however, little is known about factors influencing the\\ndecision-making process to adopt cloud services within the UK Government. This\\nresearch aims to develop a theoretical framework to understand risk perception\\nand risk acceptance of cloud computing services. Study subjects (N=24) were\\nrecruited from three UK Government organizations to attend a semi- structured\\ninterview. Transcribed texts were analyzed using the approach termed\\ninterpretive phenomenological analysis.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Factors influencing risk acceptance of Cloud Computing services in the\\n  UK Government',\n",
       "  'text': 'Results showed that the most important\\nfactors influencing risk acceptance of cloud services are: perceived benefits\\nand opportunities, organization risk culture and perceived risks. We focused on\\nperceived risks and perceived security concerns. Based on these results, we\\nsuggest a number of implications for risk managers, policy makers and cloud\\nservice providers.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Warehousing Credit (CVA) Risk, Capital (KVA) and Tax (TVA) Consequences',\n",
       "  'text': 'Credit risk may be warehoused by choice, or because of limited hedging\\npossibilities. Credit risk warehousing increases capital requirements and\\nleaves open risk. Open risk must be priced in the physical measure, rather than\\nthe risk neutral measure, and implies profits and losses. Furthermore the rate\\nof return on capital that shareholders require must be paid from profits. Profits are taxable and losses provide tax credits. Here we extend the\\nsemi-replication approach of Burgard and Kjaer (2013) and the capital formalism\\n(KVA) of Green, Kenyon, and Dennis (2014) to cover credit risk warehousing and\\ntax, formalized as double-semi-replication and TVA (Tax Valuation Adjustment)\\nto enable quantification.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Dual representations for systemic risk measures',\n",
       "  'text': 'The financial crisis showed the importance of measuring, allocating and\\nregulating systemic risk. Recently, the systemic risk measures that can be\\ndecomposed into an aggregation function and a scalar measure of risk, received\\na lot of attention. In this framework, capital allocations are added after\\naggregation and can represent bailout costs. More recently, a framework has\\nbeen introduced, where institutions are supplied with capital allocations\\nbefore aggregation. This yields an interpretation that is particularly useful\\nfor regulatory purposes. In each framework, the set of all feasible capital\\nallocations leads to a multivariate risk measure.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Dual representations for systemic risk measures',\n",
       "  'text': 'In this paper, we present\\ndual representations for scalar systemic risk measures as well as for the\\ncorresponding multivariate risk measures concerning capital allocations. Our\\nresults cover both frameworks: aggregating after allocating and allocating\\nafter aggregation. As examples, we consider the aggregation mechanisms of the\\nEisenberg-Noe model as well as those of the resource allocation and network\\nflow models.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'From Smile Asymptotics to Market Risk Measures',\n",
       "  'text': \"The left tail of the implied volatility skew, coming from quotes on\\nout-of-the-money put options, can be thought to reflect the market's assessment\\nof the risk of a huge drop in stock prices. We analyze how this market\\ninformation can be integrated into the theoretical framework of convex monetary\\nmeasures of risk. In particular, we make use of indifference pricing by dynamic\\nconvex risk measures, which are given as solutions of backward stochastic\\ndifferential equations (BSDEs), to establish a link between these two\\napproaches to risk measurement.\",\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'From Smile Asymptotics to Market Risk Measures',\n",
       "  'text': 'We derive a characterization of the implied\\nvolatility in terms of the solution of a nonlinear PDE and provide a small\\ntime-to-maturity expansion and numerical solutions. This procedure allows to\\nchoose convex risk measures in a conveniently parametrized class, distorted\\nentropic dynamic risk measures, which we introduce here, such that the\\nasymptotic volatility skew under indifference pricing can be matched with the\\nmarket skew. We demonstrate this in a calibration exercise to market implied\\nvolatility data.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Optimal Portfolio Problem Using Entropic Value at Risk: When the\\n  Underlying Distribution is Non-Elliptical',\n",
       "  'text': \"This paper is devoted to study the optimal portfolio problem. Harry\\nMarkowitz's Ph.D. thesis prepared the ground for the mathematical theory of\\nfinance. In modern portfolio theory, we typically find asset returns that are\\nmodeled by a random variable with an elliptical distribution and the notion of\\nportfolio risk is described by an appropriate risk measure. In this paper, we\\npropose new stochastic models for the asset returns that are based on Jumps-\\nDiffusion (J-D) distributions. This family of distributions are more compatible\\nwith stylized features of asset returns.\",\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Optimal Portfolio Problem Using Entropic Value at Risk: When the\\n  Underlying Distribution is Non-Elliptical',\n",
       "  'text': 'On the other hand, in the past\\ndecades, we find attempts in the literature to use well-known risk measures,\\nsuch as Value at Risk and Expected Shortfall, in this context. Unfortunately,\\none drawback with these previous approaches is that no explicit formulas are\\navailable and numerical approximations are used to solve the optimization\\nproblem. In this paper, we propose to use a new coherent risk measure,\\nso-called, Entropic Value at Risk(EVaR), in the optimization problem. For\\ncertain models, including a jump-diffusion distribution, this risk measure\\nyields an explicit formula for the objective function so that the optimization\\nproblem can be solved without resorting to numerical approximations.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Conditional Value-at-Risk Constraint and Loss Aversion Utility Functions',\n",
       "  'text': \"We provide an economic interpretation of the practice consisting in\\nincorporating risk measures as constraints in a classic expected return\\nmaximization problem. For what we call the infimum of expectations class of\\nrisk measures, we show that if the decision maker (DM) maximizes the\\nexpectation of a random return under constraint that the risk measure is\\nbounded above, he then behaves as a ``generalized expected utility maximizer''\\nin the following sense.\",\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Conditional Value-at-Risk Constraint and Loss Aversion Utility Functions',\n",
       "  'text': \"The DM exhibits ambiguity with respect to a family of\\nutility functions defined on a larger set of decisions than the original one;\\nhe adopts pessimism and performs first a minimization of expected utility over\\nthis family, then performs a maximization over a new decisions set. This\\neconomic behaviour is called ``Maxmin under risk'' and studied by Maccheroni\\n(2002). This economic interpretation allows us to exhibit a loss aversion\\nfactor when the risk measure is the Conditional Value-at-Risk.\",\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Time-consistency of risk measures with GARCH volatilities and their\\n  estimation',\n",
       "  'text': 'In this paper we study time-consistent risk measures for returns that are\\ngiven by a GARCH(1,1) model. We present a construction of risk measures based\\non their static counterparts that overcomes the lack of time-consistency. We\\nthen study in detail our construction for the risk measures Value-at-Risk (VaR)\\nand Average Value-at-Risk (AVaR). While in the VaR case we can derive an\\nanalytical formula for its time-consistent counterpart, in the AVaR case we\\nderive lower and upper bounds to its time-consistent version. Furthermore, we\\nincorporate techniques from Extreme Value Theory (EVT) to allow for a more\\ntail-geared statistical analysis of the corresponding risk measures.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Time-consistency of risk measures with GARCH volatilities and their\\n  estimation',\n",
       "  'text': 'We\\nconclude with an application of our results to a data set of stock prices.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Gap Risk KVA and Repo Pricing: An Economic Capital Approach in the\\n  Black-Scholes-Merton Framework',\n",
       "  'text': \"Although not a formal pricing consideration, gap risk or hedging errors are\\nthe norm of derivatives businesses. Starting with the gap risk during a margin\\nperiod of risk of a repurchase agreement (repo), this article extends the\\nBlack-Scholes-Merton option pricing framework by introducing a reserve capital\\napproach to the hedging error's irreducible variability. An extended partial\\ndifferential equation is derived with two new terms for expected gap loss and\\neconomic capital charge, leading to the gap risk economic value adjustment and\\ncapital valuation adjustment (KVA) respectively. Practical repo pricing\\nformulae is obtained showing that the break-even repo rate decomposes into cost\\nof fund and economic capital charge in KVA.\",\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Gap Risk KVA and Repo Pricing: An Economic Capital Approach in the\\n  Black-Scholes-Merton Framework',\n",
       "  'text': \"At zero haircut, a one-year term\\nrepo on main equities could command a capital charge as large as 50 basis\\npoints for a 'BBB' rated borrower.\",\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Portfolio Selection with Multiple Spectral Risk Constraints',\n",
       "  'text': 'We propose an iterative gradient-based algorithm to efficiently solve the\\nportfolio selection problem with multiple spectral risk constraints. Since the\\nconditional value at risk (CVaR) is a special case of the spectral risk\\nmeasure, our algorithm solves portfolio selection problems with multiple CVaR\\nconstraints. In each step, the algorithm solves very simple separable convex\\nquadratic programs; hence, we show that the spectral risk constrained portfolio\\nselection problem can be solved using the technology developed for solving\\nmean-variance problems.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Portfolio Selection with Multiple Spectral Risk Constraints',\n",
       "  'text': 'The algorithm extends to the case where the objective\\nis a weighted sum of the mean return and either a weighted combination or the\\nmaximum of a set of spectral risk measures. We report numerical results that\\nshow that our proposed algorithm is very efficient; it is at least one order of\\nmagnitude faster than the state-of-the-art general purpose solver for all\\npractical instances. One can leverage this efficiency to be robust against\\nmodel risk by including constraints with respect to several different risk\\nmodels.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Risk-Constrained Kelly Gambling',\n",
       "  'text': 'We consider the classic Kelly gambling problem with general distribution of\\noutcomes, and an additional risk constraint that limits the probability of a\\ndrawdown of wealth to a given undesirable level. We develop a bound on the\\ndrawdown probability; using this bound instead of the original risk constraint\\nyields a convex optimization problem that guarantees the drawdown risk\\nconstraint holds. Numerical experiments show that our bound on drawdown\\nprobability is reasonably close to the actual drawdown risk, as computed by\\nMonte Carlo simulation.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Risk-Constrained Kelly Gambling',\n",
       "  'text': 'Our method is parametrized by a single parameter that\\nhas a natural interpretation as a risk-aversion parameter, allowing us to\\nsystematically trade off asymptotic growth rate and drawdown risk. Simulations\\nshow that this method yields bets that out perform fractional-Kelly bets for\\nthe same drawdown risk level or growth rate. Finally, we show that a natural\\nquadratic approximation of our convex problem is closely connected to the\\nclassical mean-variance Markowitz portfolio selection problem.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'A theory for combinations of risk measures',\n",
       "  'text': 'We study combinations of risk measures under no restrictive assumption on the\\nset of alternatives. We develop and discuss results regarding the preservation\\nof properties and acceptance sets for the combinations of risk measures. One of\\nthe main results is the representation for resulting risk measures from the\\nproperties of both alternative functionals and combination functions. To that,\\nwe build on the development of a representation for arbitrary mixture of convex\\nrisk measures. In this case, we obtain a penalty that recalls the notion of\\ninf-convolution under theoretical measure integration. As an application, we\\naddress the context of probability-based risk measurements for functionals on\\nthe set of distribution functions.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'A theory for combinations of risk measures',\n",
       "  'text': 'We develop results related to this specific\\ncontext. We also explore features of individual interest generated by our\\nframework, such as the preservation of continuity properties, the\\nrepresentation of worst-case risk measures, stochastic dominance and\\nelicitability. We also address model uncertainty measurement under our\\nframework and propose a new class of measures for this task.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Quantifying the Model Risk Inherent in the Calibration and Recalibration\\n  of Option Pricing Models',\n",
       "  'text': 'We focus on two particular aspects of model risk: the inability of a chosen\\nmodel to fit observed market prices at a given point in time (calibration\\nerror) and the model risk due to recalibration of model parameters (in\\ncontradiction to the model assumptions). In this context, we follow the\\napproach of Glasserman and Xu (2014) and use relative entropy as a pre-metric\\nin order to quantify these two sources of model risk in a common framework, and\\nconsider the trade-offs between them when choosing a model and the frequency\\nwith which to recalibrate to the market.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Quantifying the Model Risk Inherent in the Calibration and Recalibration\\n  of Option Pricing Models',\n",
       "  'text': 'We illustrate this approach applied to\\nthe models of Black and Scholes (1973) and Heston (1993), using option data for\\nApple (AAPL) and Google (GOOG). We find that recalibrating a model more\\nfrequently simply shifts model risk from one type to another, without any\\nsubstantial reduction of aggregate model risk. Furthermore, moving to a more\\ncomplicated stochastic model is seen to be counterproductive if one requires a\\nhigh degree of robustness, for example as quantified by a 99 percent quantile\\nof aggregate model risk.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Stochastic control of optimized certainty equivalents',\n",
       "  'text': 'Optimized certainty equivalents (OCEs) is a family of risk measures widely\\nused by both practitioners and academics. This is mostly due to its\\ntractability and the fact that it encompasses important examples, including\\nentropic risk measures and average value at risk. In this work we consider stochastic optimal control problems where the\\nobjective criterion is given by an OCE risk measure, or put in other words, a\\nrisk minimization problem for controlled diffusions. A major difficulty arises\\nsince OCEs are often time inconsistent. Nevertheless, via an enlargement of\\nstate space we achieve a substitute of sorts for time consistency in fair\\ngenerality.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Stochastic control of optimized certainty equivalents',\n",
       "  'text': 'This allows us to derive a dynamic programming principle and thus\\nrecover central results of (risk-neutral) stochastic control theory. In\\nparticular, we show that the value of our risk minimization problem can be\\ncharacterized via the viscosity solution of a Hamilton--Jacobi--Bellman--Issacs\\nequation. We further establish the uniqueness of the latter under suitable\\ntechnical conditions.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Tile test for back-testing risk evaluation',\n",
       "  'text': 'A new test for measuring the accuracy of financial market risk estimations is\\nintroduced. It is based on the probability integral transform (PIT) of the ex\\npost realized returns using the ex ante probability distributions underlying\\nthe risk estimation. If the forecast is correct, the result of the PIT, that we\\ncalled probtile, should be an iid random variable with a uniform distribution. The new test measures the variance of the number of probtiles in a tiling over\\nthe whole sample. Using different tilings allow to check the dynamic and the\\ndistributional aspect of risk methodologies.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Tile test for back-testing risk evaluation',\n",
       "  'text': 'The new test is very powerful, and\\nnew benchmarks need to be introduced to take into account subtle mean reversion\\neffects induced by some risk estimations. The test is applied on 2 data sets\\nfor risk horizons of 1 and 10 days. The results show unambiguously the\\nimportance of capturing correctly the dynamic of the financial market, and\\nexclude some broadly used risk methodologies.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Tail-risk protection: Machine Learning meets modern Econometrics',\n",
       "  'text': 'Tail risk protection is in the focus of the financial industry and requires\\nsolid mathematical and statistical tools, especially when a trading strategy is\\nderived. Recent hype driven by machine learning (ML) mechanisms has raised the\\nnecessity to display and understand the functionality of ML tools. In this\\npaper, we present a dynamic tail risk protection strategy that targets a\\nmaximum predefined level of risk measured by Value-At-Risk while controlling\\nfor participation in bull market regimes. We propose different weak\\nclassifiers, parametric and non-parametric, that estimate the exceedance\\nprobability of the risk level from which we derive trading signals in order to\\nhedge tail events.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Tail-risk protection: Machine Learning meets modern Econometrics',\n",
       "  'text': 'We then compare the different approaches both with\\nstatistical and trading strategy performance, finally we propose an ensemble\\nclassifier that produces a meta tail risk protection strategy improving both\\ngeneralization and trading performance.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Insurance valuation: A two-step generalised regression approach',\n",
       "  'text': 'Current approaches to fair valuation in insurance often follow a two-step\\napproach, combining quadratic hedging with application of a risk measure on the\\nresidual liability, to obtain a cost-of-capital margin. In such approaches, the\\npreferences represented by the regulatory risk measure are not reflected in the\\nhedging process. We address this issue by an alternative two-step hedging\\nprocedure, based on generalised regression arguments, which leads to portfolios\\nthat are neutral with respect to a risk measure, such as Value-at-Risk or the\\nexpectile. First, a portfolio of traded assets aimed at replicating the\\nliability is determined by local quadratic hedging.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Insurance valuation: A two-step generalised regression approach',\n",
       "  'text': 'Second, the residual\\nliability is hedged using an alternative objective function. The risk margin is\\nthen defined as the cost of the capital required to hedge the residual\\nliability. In the case quantile regression is used in the second step, yearly\\nsolvency constraints are naturally satisfied; furthermore, the portfolio is a\\nrisk minimiser among all hedging portfolios that satisfy such constraints. We\\npresent a neural network algorithm for the valuation and hedging of insurance\\nliabilities based on a backward iterations scheme. The algorithm is fairly\\ngeneral and easily applicable, as it only requires simulated paths of risk\\ndrivers.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Measuring and hedging financial risks in dynamical world',\n",
       "  'text': 'Financial markets have developed a lot of strategies to control risks induced\\nby market fluctuations. Mathematics has emerged as the leading discipline to\\naddress fundamental questions in finance as asset pricing model and hedging\\nstrategies. History began with the paradigm of zero-risk introduced by Black &\\nScholes stating that any random amount to be paid in the future may be\\nreplicated by a dynamical portfolio. In practice, the lack of information leads\\nto ill-posed problems when model calibrating. The real world is more complex\\nand new pricing and hedging methodologies have been necessary.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Measuring and hedging financial risks in dynamical world',\n",
       "  'text': 'This challenging\\nquestion has generated a deep and intensive academic research in the 20 last\\nyears, based on super-replication (perfect or with respect to confidence level)\\nand optimization. In the interplay between theory and practice, Monte Carlo\\nmethods have been revisited, new risk measures have been back-tested. These\\ntypical examples give some insights on how may be used mathematics in financial\\nrisk management.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Optimal Dividend and reinsurance strategy of a Property Insurance\\n  Company under Catastrophe Risk',\n",
       "  'text': 'We consider an optimal control problem of a property insurance company with\\nproportional reinsurance strategy. The insurance business brings in catastrophe\\nrisk, such as earthquake and flood. The catastrophe risk could be partly\\nreduced by reinsurance. The management of the company controls the reinsurance\\nrate and dividend payments process to maximize the expected present value of\\nthe dividends before bankruptcy. This is the first time to consider the\\ncatastrophe risk in property insurance model, which is more realistic. We\\nestablish the solution of the problem by the mixed singular-regular control of\\njump diffusions.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Optimal Dividend and reinsurance strategy of a Property Insurance\\n  Company under Catastrophe Risk',\n",
       "  'text': 'We first derive the optimal retention ratio, the optimal\\ndividend payments level, the optimal return function and the optimal control\\nstrategy of the property insurance company, then the impacts of the catastrophe\\nrisk and key model parameters on the optimal return function and the optimal\\ncontrol strategy of the company are discussed.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Leverage-induced systemic risk under Basle II and other credit risk\\n  policies',\n",
       "  'text': 'We use a simple agent based model of value investors in financial markets to\\ntest three credit regulation policies. The first is the unregulated case, which\\nonly imposes limits on maximum leverage. The second is Basle II and the third\\nis a hypothetical alternative in which banks perfectly hedge all of their\\nleverage-induced risk with options. When compared to the unregulated case both\\nBasle II and the perfect hedge policy reduce the risk of default when leverage\\nis low but increase it when leverage is high.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Leverage-induced systemic risk under Basle II and other credit risk\\n  policies',\n",
       "  'text': 'This is because both regulation\\npolicies increase the amount of synchronized buying and selling needed to\\nachieve deleveraging, which can destabilize the market. None of these policies\\nare optimal for everyone: Risk neutral investors prefer the unregulated case\\nwith low maximum leverage, banks prefer the perfect hedge policy, and fund\\nmanagers prefer the unregulated case with high maximum leverage. No one prefers\\nBasle II.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Managing counterparty credit risk via BSDEs',\n",
       "  'text': \"We discuss a general dynamic replication approach to counterparty credit risk\\nmodeling. This leads to a fundamental jump-process backward stochastic\\ndifferential equation (BSDE) for the credit risk adjusted portfolio value. We\\nthen reduce the fundamental BSDE to a continuous BSDE. Depending on the close\\nout value convention, the reduced fundamental BSDE's solution can be\\nrepresented explicitly or through an accurate approximate expression.\",\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Managing counterparty credit risk via BSDEs',\n",
       "  'text': 'Furthermore, we discuss practical aspects of the approach, important for the\\nits industry applications: (i) efficient numerical methodology for solving a\\nBSDE driven by a moderate number of Brownian motions, and (ii) factor reduction\\nmethodology that allows one to approximately replace a portfolio driven by a\\nlarge number of risk factors with a portfolio driven by a moderate number of\\nrisk factors.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Optimal Dynamic Portfolio with Mean-CVaR Criterion',\n",
       "  'text': 'Value-at-Risk (VaR) and Conditional Value-at-Risk (CVaR) are popular risk\\nmeasures from academic, industrial and regulatory perspectives. The problem of\\nminimizing CVaR is theoretically known to be of Neyman-Pearson type binary\\nsolution. We add a constraint on expected return to investigate the Mean-CVaR\\nportfolio selection problem in a dynamic setting: the investor is faced with a\\nMarkowitz type of risk reward problem at final horizon where variance as a\\nmeasure of risk is replaced by CVaR. Based on the complete market assumption,\\nwe give an analytical solution in general.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " {'name': 'Optimal Dynamic Portfolio with Mean-CVaR Criterion',\n",
       "  'text': 'The novelty of our solution is that\\nit is no longer Neyman-Pearson type where the final optimal portfolio takes\\nonly two values. Instead, in the case where the portfolio value is required to\\nbe bounded from above, the optimal solution takes three values; while in the\\ncase where there is no upper bound, the optimal investment portfolio does not\\nexist, though a three-level portfolio still provides a sub-optimal solution.',\n",
       "  'meta': {'_split_id': 1}},\n",
       " {'name': 'Biased Risk Parity with Fractal Model of Risk',\n",
       "  'text': 'For the past two decades investors have observed long memory and highly\\ncorrelated behavior of asset classes that does not fit into the framework of\\nModern Portfolio Theory. Custom correlation and standard deviation estimators\\nconsider normal distribution of returns and market efficiency hypothesis. It\\nforced investors to search more universal instruments of tail risk protection. One of the possible solutions is a naive risk parity strategy, which avoids\\nestimation of expected returns and correlations. The authors develop the idea\\nfurther and propose a fractal distribution of returns as a core. This class of\\ndistributions is more general as it does not imply strict limitations on risk\\nevolution.',\n",
       "  'meta': {'_split_id': 0}},\n",
       " ...]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/07/2020 18:47:49 - INFO - elasticsearch -   POST http://localhost:9200/_bulk?refresh=wait_for [status:200 request:1.706s]\n",
      "10/07/2020 18:47:50 - INFO - elasticsearch -   POST http://localhost:9200/_bulk?refresh=wait_for [status:200 request:1.008s]\n"
     ]
    }
   ],
   "source": [
    "document_store.write_documents(descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.retriever.sparse import ElasticsearchRetriever\n",
    "retriever = ElasticsearchRetriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/07/2020 18:48:30 - INFO - farm.utils -   device: cpu n_gpu: 0, distributed training: False, automatic mixed precision training: None\n",
      "10/07/2020 18:48:30 - INFO - farm.infer -   Could not find `deepset/roberta-base-squad2` locally. Try to download from model hub ...\n",
      "10/07/2020 18:48:34 - WARNING - farm.modeling.language_model -   Could not automatically detect from language model name what language it is. \n",
      "\t We guess it's an *ENGLISH* model ... \n",
      "\t If not: Init the language model by supplying the 'language' param.\n",
      "10/07/2020 18:48:40 - WARNING - farm.modeling.prediction_head -   Some unused parameters are passed to the QuestionAnsweringHead. Might not be a problem. Params: {\"loss_ignore_index\": -1}\n",
      "10/07/2020 18:48:46 - INFO - farm.utils -   device: cpu n_gpu: 0, distributed training: False, automatic mixed precision training: None\n",
      "10/07/2020 18:48:46 - INFO - farm.infer -   Got ya 3 parallel workers to do inference ...\n",
      "10/07/2020 18:48:46 - INFO - farm.infer -    0    0    0 \n",
      "10/07/2020 18:48:46 - INFO - farm.infer -   /w\\  /w\\  /w\\\n",
      "10/07/2020 18:48:46 - INFO - farm.infer -   /'\\  / \\  /'\\\n",
      "10/07/2020 18:48:46 - INFO - farm.infer -       \n",
      "10/07/2020 18:48:46 - INFO - haystack.reader.farm -   Saving reader model to models/roberta-temp\n"
     ]
    }
   ],
   "source": [
    "'''load baseline roberat model from FARM(huggingface also possible):'''\n",
    "\n",
    "'''uncomment if model is not stored on disk'''\n",
    "#reader = FARMReader(model_name_or_path=\"deepset/roberta-base-squad2\", use_gpu=False)\n",
    "#reader.save(\"models/roberta-temp\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FARMReader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-065f25764a16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#load Roberta reader from disk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFARMReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"models/roberta-temp\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_seq_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_stride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# for choosing right pre-trained model:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# https://haystack.deepset.ai/en/docs/readermd#Choosing-the-Right-Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'FARMReader' is not defined"
     ]
    }
   ],
   "source": [
    "'''load Roberta reader from disk'''\n",
    "\n",
    "reader = FARMReader(model_name_or_path=\"models/roberta-temp\", use_gpu=False, max_seq_len=500, doc_stride=50)\n",
    "# for choosing right pre-trained model:\n",
    "# https://haystack.deepset.ai/en/docs/readermd#Choosing-the-Right-Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put reader and retriever together in pipeline:\n",
    "finder = Finder(reader, retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/07/2020 18:49:47 - INFO - elasticsearch -   POST http://localhost:9200/document/_search [status:200 request:0.295s]\n",
      "10/07/2020 18:49:47 - INFO - haystack.retriever.sparse -   Got 10 candidates from retriever\n",
      "10/07/2020 18:49:47 - INFO - haystack.finder -   Reader is looking for detailed answer in 17281 chars ...\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.11 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.30 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:01<00:00,  1.93s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.04 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.45 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.91 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:02<00:00,  2.31s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.21 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:01<00:00,  1.70s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.24 Batches/s]\n"
     ]
    }
   ],
   "source": [
    "prediction = finder.get_answers(question=\"What system shall be implemented in Serbia?\", top_k_retriever=10, top_k_reader=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'answers': [   {   'answer': 'a monitoring, reporting, and verification '\n",
      "                                 '(MRV) system',\n",
      "                       'context': 'reement. \\n'\n",
      "                                  '\\n'\n",
      "                                  'The project will finalize and launch a '\n",
      "                                  'monitoring, reporting, and verification '\n",
      "                                  '(MRV) system that will provide more '\n",
      "                                  'accurate information and',\n",
      "                       'document_id': 'a3056f62-5c73-4046-a46c-ad4d16c2595e',\n",
      "                       'meta': {'PIMS_ID': 6211},\n",
      "                       'offset_end': 102,\n",
      "                       'offset_end_in_doc': 570,\n",
      "                       'offset_start': 48,\n",
      "                       'offset_start_in_doc': 516,\n",
      "                       'probability': 0.7726760525124913,\n",
      "                       'score': 9.787870407104492},\n",
      "                   {   'answer': 'a uniform national wildlife PA system',\n",
      "                       'context': 'ablishment and management, the project aims '\n",
      "                                  'to establish a uniform national wildlife PA '\n",
      "                                  'system in Peninsular Malaysia and to '\n",
      "                                  'establish a performance-b',\n",
      "                       'document_id': 'cee4152e-af8a-4644-8a22-d1f3f900fcb9',\n",
      "                       'meta': {'PIMS_ID': 3967},\n",
      "                       'offset_end': 94,\n",
      "                       'offset_end_in_doc': 1872,\n",
      "                       'offset_start': 57,\n",
      "                       'offset_start_in_doc': 1835,\n",
      "                       'probability': 0.7623624780920136,\n",
      "                       'score': 9.32540512084961},\n",
      "                   {   'answer': 'municipal Energy Management Systems',\n",
      "                       'context': 'jective is to introduce and support the '\n",
      "                                  'implementation of municipal Energy '\n",
      "                                  'Management Systems (EMS), including Energy '\n",
      "                                  'Management Information Systems (',\n",
      "                       'document_id': 'f0ccbdbd-9219-4a93-92db-80b82f5f7a30',\n",
      "                       'meta': {'PIMS_ID': 4588},\n",
      "                       'offset_end': 93,\n",
      "                       'offset_end_in_doc': 107,\n",
      "                       'offset_start': 58,\n",
      "                       'offset_start_in_doc': 72,\n",
      "                       'probability': 0.7236274268999738,\n",
      "                       'score': 7.700214385986328},\n",
      "                   {   'answer': 'a Full-Size Project (FSP) along the coastal '\n",
      "                                 'zone, in six different pilot sites',\n",
      "                       'context': ' Countries Fund (LDCF) to implement a '\n",
      "                                  'Full-Size Project (FSP) along the coastal '\n",
      "                                  'zone, in six different pilot sites (Conakry '\n",
      "                                  'Dee, Lakka, Hamilton, Tomb',\n",
      "                       'document_id': 'b7744907-337a-4cd3-98da-501115e9f6dc',\n",
      "                       'meta': {'PIMS_ID': 5178},\n",
      "                       'offset_end': 114,\n",
      "                       'offset_end_in_doc': 1213,\n",
      "                       'offset_start': 36,\n",
      "                       'offset_start_in_doc': 1135,\n",
      "                       'probability': 0.637876222440903,\n",
      "                       'score': 4.529265403747559},\n",
      "                   {   'answer': 'The project’s approach',\n",
      "                       'context': 'roaches to address the climate risk facing '\n",
      "                                  'coastal communities. The project’s approach '\n",
      "                                  'to be adopted will deliver three '\n",
      "                                  'complimentary outcomes to addr',\n",
      "                       'document_id': 'b7744907-337a-4cd3-98da-501115e9f6dc',\n",
      "                       'meta': {'PIMS_ID': 5178},\n",
      "                       'offset_end': 86,\n",
      "                       'offset_end_in_doc': 2615,\n",
      "                       'offset_start': 64,\n",
      "                       'offset_start_in_doc': 2593,\n",
      "                       'probability': 0.5935355109898552,\n",
      "                       'score': 3.028803825378418}],\n",
      "    'no_ans_gap': 6.785511016845703,\n",
      "    'question': 'What system shall be implemented in Serbia?'}\n"
     ]
    }
   ],
   "source": [
    "print_answers(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Dense Passage Retriever "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/07/2020 18:50:24 - INFO - filelock -   Lock 6070406640 acquired on /Users/jonas/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67c49719784c4b22a9641e921d86d8be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/07/2020 18:50:25 - INFO - filelock -   Lock 6070406640 released on /Users/jonas/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/07/2020 18:50:26 - INFO - filelock -   Lock 6057116864 acquired on /Users/jonas/.cache/torch/transformers/4b05580c0bfb2b640a50c1c6ae3fe9bca923871a29e0182927c086905d6c4c47.7652e92693c670fb8dfd7ec1f9191e3f82673742ff6a86cde9133a4ea6002ced.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4edc6089d0d84b5098b79656bafb4644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=493.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/07/2020 18:50:27 - INFO - filelock -   Lock 6057116864 released on /Users/jonas/.cache/torch/transformers/4b05580c0bfb2b640a50c1c6ae3fe9bca923871a29e0182927c086905d6c4c47.7652e92693c670fb8dfd7ec1f9191e3f82673742ff6a86cde9133a4ea6002ced.lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/07/2020 18:50:27 - INFO - filelock -   Lock 6057589776 acquired on /Users/jonas/.cache/torch/transformers/8fdd0d2838c23f921379f2b0322aecf406cbdaa97ffecc544e3a1d49a7c302bd.6f90756c59007364d7842118056ad653f39f4d340fbe20bcc04037d2a45cb0f7.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c8d17f652034ef682c3d99c0c4ba74c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=437986065.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/07/2020 18:53:10 - INFO - filelock -   Lock 6057589776 released on /Users/jonas/.cache/torch/transformers/8fdd0d2838c23f921379f2b0322aecf406cbdaa97ffecc544e3a1d49a7c302bd.6f90756c59007364d7842118056ad653f39f4d340fbe20bcc04037d2a45cb0f7.lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DPRQuestionEncoder were not initialized from the model checkpoint at facebook/dpr-question_encoder-single-nq-base and are newly initialized: ['question_encoder.bert_model.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "10/07/2020 18:53:16 - INFO - filelock -   Lock 5006083408 acquired on /Users/jonas/.cache/torch/transformers/f6388f32b32eac5dad8f0f9c7009ce69e967c1b65ebae62f805fced8022ea991.9500f04f28d7c0ca5f9c265db7ba5030897a2d752451412827f7dec185b1ee36.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42060a238bb94e8ba024c7edf3a35a0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=492.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/07/2020 18:53:17 - INFO - filelock -   Lock 5006083408 released on /Users/jonas/.cache/torch/transformers/f6388f32b32eac5dad8f0f9c7009ce69e967c1b65ebae62f805fced8022ea991.9500f04f28d7c0ca5f9c265db7ba5030897a2d752451412827f7dec185b1ee36.lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/07/2020 18:53:17 - INFO - filelock -   Lock 5006083072 acquired on /Users/jonas/.cache/torch/transformers/d1c705617c02da7a616f4b5a8cb445a7f78e84bc4f9e26378c89901d97e16d78.232fed629becb590e5b2ac6c6124f9d1561ef7a1d17ad0394232dd46a0835002.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71c394f19e3544b893bafef0eaeda116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=437983985.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/07/2020 18:56:02 - INFO - filelock -   Lock 5006083072 released on /Users/jonas/.cache/torch/transformers/d1c705617c02da7a616f4b5a8cb445a7f78e84bc4f9e26378c89901d97e16d78.232fed629becb590e5b2ac6c6124f9d1561ef7a1d17ad0394232dd46a0835002.lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DPRContextEncoder were not initialized from the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base and are newly initialized: ['ctx_encoder.bert_model.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Dense Passage Retriever:\n",
    "\n",
    "    Utilizes BERT to embed both the document and the query to compute a more contextual similarity score for ranking.\n",
    "    \n",
    "    Embedding of documents is computationally very expensive and is probably unfeasible without proper GPU support.\n",
    "    \n",
    "'''\n",
    "\n",
    "from haystack.retriever.dense import DensePassageRetriever\n",
    "retriever = DensePassageRetriever(document_store=document_store,\n",
    "                                  query_embedding_model=\"facebook/dpr-question_encoder-single-nq-base\",\n",
    "                                  passage_embedding_model=\"facebook/dpr-ctx_encoder-single-nq-base\",\n",
    "                                  use_gpu=True,\n",
    "                                  embed_title=True,\n",
    "                                  max_seq_len=256,\n",
    "                                  batch_size=16,\n",
    "                                  remove_sep_tok_from_untitled_passages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1444.4657039711192\n",
      "sliding window or pre-processing has to enabled for proper results\n"
     ]
    }
   ],
   "source": [
    "#check for average length to see if max_seq_length of BERT model is sufficient:\n",
    "mean_len = description.project_description.str.len().mean()\n",
    "print(mean_len)\n",
    "if mean_len > 512:\n",
    "    print('sliding window or pre-processing has to enabled for proper results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/03/2020 11:57:29 - INFO - elasticsearch -   POST http://localhost:9200/document/_search?scroll=5m&size=1000 [status:200 request:0.525s]\n",
      "10/03/2020 11:57:30 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.137s]\n",
      "10/03/2020 11:57:30 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.121s]\n",
      "10/03/2020 11:57:30 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.084s]\n",
      "10/03/2020 11:57:30 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.113s]\n",
      "10/03/2020 11:57:30 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.104s]\n",
      "10/03/2020 11:57:30 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.091s]\n",
      "10/03/2020 11:57:30 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.079s]\n",
      "10/03/2020 11:57:30 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.086s]\n",
      "10/03/2020 11:57:30 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.068s]\n",
      "10/03/2020 11:57:30 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.070s]\n",
      "10/03/2020 11:57:31 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.078s]\n",
      "10/03/2020 11:57:31 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.068s]\n",
      "10/03/2020 11:57:32 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.074s]\n",
      "10/03/2020 11:57:32 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.080s]\n",
      "10/03/2020 11:57:32 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.081s]\n",
      "10/03/2020 11:57:32 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.072s]\n",
      "10/03/2020 11:57:32 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.066s]\n",
      "10/03/2020 11:57:32 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.081s]\n",
      "10/03/2020 11:57:32 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.079s]\n",
      "10/03/2020 11:57:33 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.071s]\n",
      "10/03/2020 11:57:33 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.063s]\n",
      "10/03/2020 11:57:33 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.061s]\n",
      "10/03/2020 11:57:33 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.081s]\n",
      "10/03/2020 11:57:33 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.080s]\n",
      "10/03/2020 11:57:33 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.067s]\n",
      "10/03/2020 11:57:33 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.083s]\n",
      "10/03/2020 11:57:33 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.082s]\n",
      "10/03/2020 11:57:33 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.059s]\n",
      "10/03/2020 11:57:33 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.064s]\n",
      "10/03/2020 11:57:33 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.061s]\n",
      "10/03/2020 11:57:33 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.052s]\n",
      "10/03/2020 11:57:34 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.070s]\n",
      "10/03/2020 11:57:34 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.062s]\n",
      "10/03/2020 11:57:34 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.067s]\n",
      "10/03/2020 11:57:34 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.060s]\n",
      "10/03/2020 11:57:34 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.061s]\n",
      "10/03/2020 11:57:34 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.092s]\n",
      "10/03/2020 11:57:34 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.088s]\n",
      "10/03/2020 11:57:34 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.076s]\n",
      "10/03/2020 11:57:34 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.080s]\n",
      "10/03/2020 11:57:34 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.107s]\n",
      "10/03/2020 11:57:34 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.072s]\n",
      "10/03/2020 11:57:35 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.080s]\n",
      "10/03/2020 11:57:35 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.074s]\n",
      "10/03/2020 11:57:35 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.083s]\n",
      "10/03/2020 11:57:35 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.095s]\n",
      "10/03/2020 11:57:35 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.091s]\n",
      "10/03/2020 11:57:35 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.075s]\n",
      "10/03/2020 11:57:35 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.056s]\n",
      "10/03/2020 11:57:35 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.076s]\n",
      "10/03/2020 11:57:35 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.067s]\n",
      "10/03/2020 11:57:35 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.063s]\n",
      "10/03/2020 11:57:36 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.059s]\n",
      "10/03/2020 11:57:36 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.083s]\n",
      "10/03/2020 11:57:36 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.097s]\n",
      "10/03/2020 11:57:36 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.075s]\n",
      "10/03/2020 11:57:36 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.061s]\n",
      "10/03/2020 11:57:36 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.055s]\n",
      "10/03/2020 11:57:36 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.067s]\n",
      "10/03/2020 11:57:36 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.078s]\n",
      "10/03/2020 11:57:36 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.082s]\n",
      "10/03/2020 11:57:36 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.056s]\n",
      "10/03/2020 11:57:36 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.050s]\n",
      "10/03/2020 11:57:36 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.076s]\n",
      "10/03/2020 11:57:37 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.076s]\n",
      "10/03/2020 11:57:37 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.071s]\n",
      "10/03/2020 11:57:37 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.058s]\n",
      "10/03/2020 11:57:37 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.055s]\n",
      "10/03/2020 11:57:37 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.062s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/03/2020 11:57:37 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.072s]\n",
      "10/03/2020 11:57:37 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.074s]\n",
      "10/03/2020 11:57:37 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.067s]\n",
      "10/03/2020 11:57:37 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.069s]\n",
      "10/03/2020 11:57:37 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.073s]\n",
      "10/03/2020 11:57:37 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.073s]\n",
      "10/03/2020 11:57:38 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.066s]\n",
      "10/03/2020 11:57:38 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.084s]\n",
      "10/03/2020 11:57:38 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.063s]\n",
      "10/03/2020 11:57:38 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.059s]\n",
      "10/03/2020 11:57:38 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.066s]\n",
      "10/03/2020 11:57:38 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.070s]\n",
      "10/03/2020 11:57:38 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.097s]\n",
      "10/03/2020 11:57:38 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.070s]\n",
      "10/03/2020 11:57:38 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.071s]\n",
      "10/03/2020 11:57:38 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.058s]\n",
      "10/03/2020 11:57:38 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.071s]\n",
      "10/03/2020 11:57:38 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.062s]\n",
      "10/03/2020 11:57:38 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.060s]\n",
      "10/03/2020 11:57:39 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.079s]\n",
      "10/03/2020 11:57:39 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.078s]\n",
      "10/03/2020 11:57:39 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.075s]\n",
      "10/03/2020 11:57:39 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.064s]\n",
      "10/03/2020 11:57:39 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.062s]\n",
      "10/03/2020 11:57:39 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.056s]\n",
      "10/03/2020 11:57:39 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.064s]\n",
      "10/03/2020 11:57:39 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.066s]\n",
      "10/03/2020 11:57:39 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.087s]\n",
      "10/03/2020 11:57:39 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.061s]\n",
      "10/03/2020 11:57:39 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.065s]\n",
      "10/03/2020 11:57:39 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.087s]\n",
      "10/03/2020 11:57:40 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.081s]\n",
      "10/03/2020 11:57:40 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.110s]\n",
      "10/03/2020 11:57:40 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.080s]\n",
      "10/03/2020 11:57:40 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.070s]\n",
      "10/03/2020 11:57:40 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.078s]\n",
      "10/03/2020 11:57:40 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.092s]\n",
      "10/03/2020 11:57:40 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.067s]\n",
      "10/03/2020 11:57:40 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.059s]\n",
      "10/03/2020 11:57:40 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.070s]\n",
      "10/03/2020 11:57:40 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.060s]\n",
      "10/03/2020 11:57:41 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.051s]\n",
      "10/03/2020 11:57:41 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.046s]\n",
      "10/03/2020 11:57:41 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.059s]\n",
      "10/03/2020 11:57:41 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.072s]\n",
      "10/03/2020 11:57:41 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.055s]\n",
      "10/03/2020 11:57:41 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.046s]\n",
      "10/03/2020 11:57:41 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.050s]\n",
      "10/03/2020 11:57:41 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.067s]\n",
      "10/03/2020 11:57:41 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.059s]\n",
      "10/03/2020 11:57:41 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.056s]\n",
      "10/03/2020 11:57:41 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.057s]\n",
      "10/03/2020 11:57:41 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.056s]\n",
      "10/03/2020 11:57:41 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.080s]\n",
      "10/03/2020 11:57:41 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.072s]\n",
      "10/03/2020 11:57:41 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.066s]\n",
      "10/03/2020 11:57:42 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.081s]\n",
      "10/03/2020 11:57:42 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.105s]\n",
      "10/03/2020 11:57:42 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.111s]\n",
      "10/03/2020 11:57:42 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.079s]\n",
      "10/03/2020 11:57:42 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.058s]\n",
      "10/03/2020 11:57:42 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.078s]\n",
      "10/03/2020 11:57:42 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.088s]\n",
      "10/03/2020 11:57:42 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.090s]\n",
      "10/03/2020 11:57:42 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.086s]\n",
      "10/03/2020 11:57:42 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.057s]\n",
      "10/03/2020 11:57:43 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.084s]\n",
      "10/03/2020 11:57:43 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.083s]\n",
      "10/03/2020 11:57:43 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.077s]\n",
      "10/03/2020 11:57:43 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.065s]\n",
      "10/03/2020 11:57:43 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.062s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/03/2020 11:57:43 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.072s]\n",
      "10/03/2020 11:57:43 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.082s]\n",
      "10/03/2020 11:57:43 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.075s]\n",
      "10/03/2020 11:57:43 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.082s]\n",
      "10/03/2020 11:57:43 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.066s]\n",
      "10/03/2020 11:57:44 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.067s]\n",
      "10/03/2020 11:57:44 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.060s]\n",
      "10/03/2020 11:57:44 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.056s]\n",
      "10/03/2020 11:57:44 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.055s]\n",
      "10/03/2020 11:57:44 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.060s]\n",
      "10/03/2020 11:57:44 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.059s]\n",
      "10/03/2020 11:57:44 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.051s]\n",
      "10/03/2020 11:57:44 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.043s]\n",
      "10/03/2020 11:57:44 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.047s]\n",
      "10/03/2020 11:57:44 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.063s]\n",
      "10/03/2020 11:57:44 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.058s]\n",
      "10/03/2020 11:57:44 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.054s]\n",
      "10/03/2020 11:57:44 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.055s]\n",
      "10/03/2020 11:57:44 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.072s]\n",
      "10/03/2020 11:57:44 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.056s]\n",
      "10/03/2020 11:57:45 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.060s]\n",
      "10/03/2020 11:57:45 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.046s]\n",
      "10/03/2020 11:57:45 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.050s]\n",
      "10/03/2020 11:57:45 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.059s]\n",
      "10/03/2020 11:57:45 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.060s]\n",
      "10/03/2020 11:57:45 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.049s]\n",
      "10/03/2020 11:57:45 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.047s]\n",
      "10/03/2020 11:57:45 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.061s]\n",
      "10/03/2020 11:57:45 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.057s]\n",
      "10/03/2020 11:57:45 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.055s]\n",
      "10/03/2020 11:57:45 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.044s]\n",
      "10/03/2020 11:57:45 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.040s]\n",
      "10/03/2020 11:57:45 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.047s]\n",
      "10/03/2020 11:57:46 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.047s]\n",
      "10/03/2020 11:57:46 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.057s]\n",
      "10/03/2020 11:57:46 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.042s]\n",
      "10/03/2020 11:57:46 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.055s]\n",
      "10/03/2020 11:57:46 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.059s]\n",
      "10/03/2020 11:57:46 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.053s]\n",
      "10/03/2020 11:57:46 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.057s]\n",
      "10/03/2020 11:57:46 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.029s]\n",
      "10/03/2020 11:57:46 - INFO - elasticsearch -   POST http://localhost:9200/_search/scroll [status:200 request:0.013s]\n",
      "10/03/2020 11:57:46 - INFO - elasticsearch -   DELETE http://localhost:9200/_search/scroll [status:200 request:0.017s]\n",
      "10/03/2020 11:57:46 - INFO - haystack.document_store.elasticsearch -   Updating embeddings for 181640 docs ...\n",
      "/opt/anaconda3/envs/haystack/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1764: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "../torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero(Tensor input, *, Tensor out)\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(Tensor input, *, bool as_tuple)\n",
      "10/03/2020 11:58:21 - INFO - haystack.retriever.dense -   Embedded 80 / 181640 texts\n",
      "10/03/2020 11:58:55 - INFO - haystack.retriever.dense -   Embedded 160 / 181640 texts\n",
      "10/03/2020 11:59:28 - INFO - haystack.retriever.dense -   Embedded 240 / 181640 texts\n",
      "10/03/2020 12:00:02 - INFO - haystack.retriever.dense -   Embedded 320 / 181640 texts\n",
      "10/03/2020 12:00:35 - INFO - haystack.retriever.dense -   Embedded 400 / 181640 texts\n",
      "10/03/2020 12:01:08 - INFO - haystack.retriever.dense -   Embedded 480 / 181640 texts\n",
      "10/03/2020 12:01:41 - INFO - haystack.retriever.dense -   Embedded 560 / 181640 texts\n",
      "10/03/2020 12:02:14 - INFO - haystack.retriever.dense -   Embedded 640 / 181640 texts\n",
      "10/03/2020 12:02:56 - INFO - haystack.retriever.dense -   Embedded 720 / 181640 texts\n",
      "10/03/2020 12:03:37 - INFO - haystack.retriever.dense -   Embedded 800 / 181640 texts\n",
      "10/03/2020 12:04:18 - INFO - haystack.retriever.dense -   Embedded 880 / 181640 texts\n",
      "10/03/2020 12:04:58 - INFO - haystack.retriever.dense -   Embedded 960 / 181640 texts\n",
      "10/03/2020 12:05:33 - INFO - haystack.retriever.dense -   Embedded 1040 / 181640 texts\n",
      "10/03/2020 12:06:12 - INFO - haystack.retriever.dense -   Embedded 1120 / 181640 texts\n",
      "10/03/2020 12:06:50 - INFO - haystack.retriever.dense -   Embedded 1200 / 181640 texts\n",
      "10/03/2020 12:07:27 - INFO - haystack.retriever.dense -   Embedded 1280 / 181640 texts\n",
      "10/03/2020 12:08:04 - INFO - haystack.retriever.dense -   Embedded 1360 / 181640 texts\n",
      "10/03/2020 12:08:44 - INFO - haystack.retriever.dense -   Embedded 1440 / 181640 texts\n",
      "10/03/2020 12:09:21 - INFO - haystack.retriever.dense -   Embedded 1520 / 181640 texts\n",
      "10/03/2020 12:09:59 - INFO - haystack.retriever.dense -   Embedded 1600 / 181640 texts\n",
      "10/03/2020 12:10:50 - INFO - haystack.retriever.dense -   Embedded 1680 / 181640 texts\n",
      "10/03/2020 12:11:31 - INFO - haystack.retriever.dense -   Embedded 1760 / 181640 texts\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-986995a6977e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#update embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdocument_store\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretriever\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/envs/haystack/lib/python3.8/site-packages/haystack/document_store/elasticsearch.py\u001b[0m in \u001b[0;36mupdate_embeddings\u001b[0;34m(self, retriever, index)\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_all_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Updating embeddings for {len(docs)} docs ...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 481\u001b[0;31m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretriever\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_passages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    482\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/haystack/lib/python3.8/site-packages/haystack/retriever/dense.py\u001b[0m in \u001b[0;36membed_passages\u001b[0;34m(self, docs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0mtitles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"name\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         result = self._generate_batch_predictions(texts=texts, titles=titles,\n\u001b[0m\u001b[1;32m    123\u001b[0m                                                   \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpassage_encoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                                                   \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpassage_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/haystack/lib/python3.8/site-packages/haystack/retriever/dense.py\u001b[0m in \u001b[0;36m_generate_batch_predictions\u001b[0;34m(self, texts, model, tokenizer, titles, batch_size)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mctx_ids_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mctx_attn_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mctx_seg_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m                 \u001b[0;31m# TODO revert back to when updating transformers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 \u001b[0;31m# out = out.pooler_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/haystack/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/haystack/lib/python3.8/site-packages/transformers/modeling_dpr.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    459\u001b[0m             \u001b[0mtoken_type_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m         outputs = self.ctx_encoder(\n\u001b[0m\u001b[1;32m    462\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/haystack/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/haystack/lib/python3.8/site-packages/transformers/modeling_dpr.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mreturn_dict\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     ) -> Union[BaseModelOutputWithPooling, Tuple[Tensor, ...]]:\n\u001b[0;32m--> 166\u001b[0;31m         outputs = self.bert_model(\n\u001b[0m\u001b[1;32m    167\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/haystack/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/haystack/lib/python3.8/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    825\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m         )\n\u001b[0;32m--> 827\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    828\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/haystack/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/haystack/lib/python3.8/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    482\u001b[0m                 )\n\u001b[1;32m    483\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    485\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/haystack/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/haystack/lib/python3.8/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    430\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcross_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# add cross attentions if we output attention weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m         layer_output = apply_chunking_to_forward(\n\u001b[0m\u001b[1;32m    433\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m         )\n",
      "\u001b[0;32m/opt/anaconda3/envs/haystack/lib/python3.8/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m   1595\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1597\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/envs/haystack/lib/python3.8/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/haystack/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/haystack/lib/python3.8/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate_act_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/haystack/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/haystack/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/haystack/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1610\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1611\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1612\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1614\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#update embeddings - do not compile without GPU support. \n",
    "document_store.update_embeddings(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finder = Finder(reader, retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = finder.get_answers(question=\"What is the MRV system supporting in Serbia?\", top_k_retriever=10, top_k_reader=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
